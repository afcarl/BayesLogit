\documentclass[12pt]{article}

\input{commands}

\usepackage{setspace}
\usepackage{natbib}
\usepackage{marginnote}

\newcommand{\Polya}{P\'{o}lya}
\newcommand{\PG}{\text{PG}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Ga}{\text{Ga}}
\newcommand{\NB}{\text{NB}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\eqd}{\,{\buildrel d \over =}\,}
\newcommand{\KS}{\text{KS}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\oomega}{\boldsymbol{\omega}}
\newcommand{\yy}{\boldsymbol{y}}
\newcommand{\vx}{\boldsymbol{x}}
%\newcommand{\betap}{\widetilde{\boldsymbol{\beta}}}
\newcommand{\betap}{{\boldsymbol{B}}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\vf}{\boldsymbol{f}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\veta}{\boldsymbol{\eta}}

\newcommand{\point}{\noindent $\bullet$ }

\newcounter{parnum}
\newcommand{\npoint}{%
  \noindent\refstepcounter{parnum}%
  \makebox[0.5in][c]{\textbf{\arabic{parnum}.}} %
  \marginnote{\small\ttfamily\the\inputlineno}}

%\renewcommand{\npoint}{}

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
% \parindent 0in
% \parskip 12pt

% Set Fancy Style
% \pagestyle{fancy}

% \lhead{Left Header}
% \rhead{Right Header}

\title{\Polya-Gamma Data Augmentation for Dynamic Models}
% 
\author{ %
  Jesse Windle, %
  Carlos M. Carvalho, %
  James G. Scott, %
  Liang Sun %
  \footnote{ %
    Jesse Windle is a Post-Doc in the Department of Statistical Science, Duke
    University, Box 90251, Durham NC 27708-0251 (E-mail:
    jesse.windle@gmail.com).
    % 
    Carlos M. Carvalho is Associate Professor of Statistics, Department of
    Information, Risk, and Operations Management, The University of Texas at Austin,
    2110 Speedway Stop B6500 Austin, TX 78712 (E-mail:
    carlos.carvalho@mccombs.utexas.edu).
    % 
    James G. Scott is Assistant Professor of Statistics, Department of Information,
    Risk, and Operations Management, The University of Texas at Austin,
    2110 Speedway Stop B6500 Austin, TX 78712 (E-mail:
    james.scott@mccombs.utexas.edu).
    % 
    Liang Sun is a graduate student in the Department of Operations Research \&
    Industrial Engineering, The University of Texas at Austin, 204 E. Dean Keeton
    St.  Austin, Texas 78712 (E-mail: sallylia@gmail.com).
    % 
  }
  \\
  \textit{The Univeristy of Texas at Austin} }

\begin{document}

\maketitle

\abstract{Dynamic linear models with Gaussian observations and Gaussian states
  lead to closed-form formulas for posterior simulation.  However, these
  closed-form formulas break down when the response or state evolution ceases to
  be Gaussian.  Dynamic, generalized linear models exemplify a class of models
  for which this is the case, and include, amongst other models, dynamic
  binomial logistic regression and dynamic negative binomial regression.
  Finding and appraising posterior simulation techniques for these models is
  important since modeling temporally correlated categories or counts is useful
  in a variety of disciplines, including ecology, economics, epidemiology,
  medicine, and neuroscience.  In this paper, we present one such technique,
  \Polya-Gamma data augmentation, and compare it against two competing methods.
  We find that the \Polya-Gamma approach works well for dynamic logistic
  regression and for dynamic negative binomial regression when the count sizes
  are small.

  \smallskip \textit{Keywords:} {binomial likelihood, dynamic logistic
    regression, dynamic negative binomial regression} }

% \tableofcontents
% \newpage

\doublespace

\section{Introduction}

\npoint There are a variety of settings in which one wants to model non real-valued
data.  For instance, integral data may be found in ecology or epidemiology when
modeling populations of species or outbreaks of infections and in neuroscience
when modeling brain activity in terms of discrete neural impulses.  Categorical
data is similarly ubiquitous, arising, for example, in medicine to describe a
successful treatment and in economics to denote the default on a loan.  
%
Such data may be temporally correlated as well, in which case setting up an
autoregressive-like structure is helpful, though at the cost of making posterior
inference more difficult.
%
\npoint This paper presents a data augmentation technique that produces
posterior samples for these types of models.  We begin by setting up the
problem, starting with the static case, moving to the dynamic analog, and then
describing the challenge of generating posterior samples.

\npoint When there is a linear relationship between the expectation of the response and
the covariates, and the variance of the response is constant---that is when one
is doing linear regression---the Gauss-Markov theorem ensures that ordinary
least squares (OLS) produces the best linear unbiased estimator, a frequentist
way of saying that the model has a nice solution \citep{christensen-book-2001}.
%
\npoint Further, assuming that the response is normally distributed and adopting a
normal prior leads to a closed-form posterior distribution that is easy to
sample from, which is a Bayesian equivalent of niceness.  When modeling counts
or categories, the aforementioned necessary conditions do not hold and hence OLS
does not lead to the best linear unbiased estimator and convenient posterior
distributions are not guaranteed.

\npoint Generalized linear models (GLMs) partially address these deficiencies.
In particular, parameterizing the expectation and variance of the response in
terms of a linear combination of the predictors, $\psi_t = \vx_t' \bbeta$, leads
to tractable models for counts, categories, and other non-real valued data
\citep{mccullagh-nelder-1989, wedderburn-1974}.  Strictly speaking, one need not
specify the distribution of the response, just its first two moments, but since
we are interested in Bayesian inference we will restrict our attention to
situations in which the likelihood, $f(y_t | \psi_t)$, is specified.  When the
likelihood comes from the exponential family of distributions, tractable means
that one may easily calculate the posterior mode and associated error estimates;
however, most likelihoods lead to posterior distributions which are difficult to
sample, requiring Metropolis-Hastings sampling, Gibbs sampling via data
augmentation, or some other MCMC method.

\npoint Dynamic generalized linear models (DGLMs) permit an evolving
relationship between the response and the covariates via time-varying regression
coefficients, $\{\bbeta_t\}_{t=1}^T$, so that $\psi_t = \vx_t' \bbeta_t$ for
$t=1, \ldots, T$.  One must specify a prior for $\{\bbeta_t\}_{t=1}^T$, which
controls how quickly $\{\bbeta_t\}_{t=1}^T$ may change, to complete the model.
It is common to let the prior be an AR(1) process or a random walk, both with
Gaussian innovations.  In either case, one recovers a state-space model
characterized by the observation distribution $f(y_t | \psi_t)$ for the response
and the Markovian evolution distribution $g(\bbeta_t | \bbeta_{t-1})$ for the
hidden states.  Transitioning from the static to the dynamic model is a small
step conceptually but a big step practically.  In particular, it becomes more
difficult to generate posterior samples.

\npoint To clarify the challenge of simulating $(\{\bbeta_t\}_{t=1}^T |
\{y_t\}_{t=1}^T)$ for dynamic generalized linear models, it is helpful to
consider more and less tractable state-space models.  
%
\npoint State-space models with non-Gaussian, non-linear responses and
non-linear evolution equations are less tractable.  One may efficiently
approximate the filtered distributions of such models, $p(\bbeta_t |
\{y_s\}_{s=1}^t)$, using sequential methods, but the lack of structure makes
sampling from the posterior of $\{\bbeta_t\}_{t=1}^T$ difficult.  
%
\npoint In contrast, dynamic linear models (DLMs) with Gaussian states and
Gaussian observations are more tractable: one may sample the posterior
distribution of the states using the forward filter backward sample (FFBS)
algorithm \citep{carter-kohn-1994, fruhwirth-schnatter-1994}, a procedure that
is linear in the number of observations and thus quite efficient.  However, the
FFBS algorithm breaks down outside of these vary narrow assumptions. 
%
\npoint Dynamic generalized linear models that have Gaussian, linear evolution
equations, but non-Gaussian, non-linear response distributions sit between these
extremes.  Within this class, the FFBS algorithm is not immediately applicable,
but the linear evolution equations give one hope of finding a clever way to
reintroduce it.  Many approaches follow this path, combining techniques used for
generalized linear models with the FFBS algorithm.  However, these amalgamations
are not always as effective as their constituent parts.  In particular,
Metropolis-Hastings based techniques for GLMs do not translate well to the
dynamic setting, as $\{\bbeta_t\}_{t=1}^T$ is of much higher dimension than
$\bbeta$.  
%
\npoint Data augmentation techniques for GLMs that lead to a Gaussian complete
conditional for $\bbeta$ provide a preferable approach since the corresponding
complete conditional for $\{\bbeta_t\}_{t=1}^T$ will coincide with the
likelihood of a DLM and hence the FFBS algorithm.  (See section \ref{sec:pg} for
details.)  But such data augmentation tricks are difficult to find.  Recently,
\cite{polson-etal-2013} introduced a data augmentation trick for GLMs with
binomial likelihoods and that is what we exploit here.

\npoint This paper presents \Polya-Gamma data augmentation for dynamic
regression models with binomial likelihoods and compares it to a pair of other
approaches: conjugate updating and backward sampling with a Metroplis-Hastings
step \citep{migon-etal-2013} and data augmentation using a discrete mixture of
normals approximation \citep{fruhwirth-schnatter-etal-2009, fussl-etal-2013}.
Our method is pleasing because of its parsimony, ease of implementation, and
efficiency.

\npoint Dynamic models of counts or categories, like the ones studied herein,
may arise across a range of disciplines, as noted above.  Hence, having a guide
to the most efficient posterior simulation techniques for these models is
important for practitioners.  To that end, we benchmark two dynamic generalized
linear models and show that the \Polya-Gamma approach is the best choice for
dynamic logistic regression and dynamic negative binomial regression when the
count size is small.

\npoint The outline of the paper is as follows: Section 2 reviews other methods
of posterior simulation for dynamic models; Section 3 shows how one may use the
\Polya-Gamma data augmentation technique in the dynamic setting; Section 4
presents the benchmarking results; Section 5 concludes.

\section{Previous Efforts}
\label{sec:previous-efforts}

\npoint Let us still assume that $y_t$ is the response, generated by some member
of the exponential family according to $f(y_t | \psi_t)$ where $\psi_t = \vx_t'
\bbeta_t$ for $t=1, \ldots, T$, and that the hidden states
$\{\bbeta_t\}_{t=1}^T$ evolve according to an AR(1) or random walk process with
Gaussian disturbances described by the transition density $g(\bbeta_t |
\bbeta_{t-1})$.
%fake
\npoint In the very specific case that $y_t \sim N(\psi_t | V_t)$ there is an
algorithm, the FFBS, that generates posterior samples of $\{\beta_{t}\}_{t=1}^T$
in $\mcO(T)$ time as $T$ varies \citep{carter-kohn-1994,
  fruhwirth-schnatter-1994}.  When the response comes from some other
distribution, like the binomial or negative binomial distributions, the FFBS
breaks down.
%fake
\npoint Thus, one must appeal to approximate distributions, Metropolis-Hastings
sampling, Gibbs sampling, or some other MCMC approach, like particle methods.

\npoint Alternative approaches to posterior simulation date back to at least
\cite{west-etal-1985}, who employed a conjugate updating, backward sampling
(CUBS) strategy to generate approximate posterior draws.  Their strategy is to
use an approximation of the filtered distributions $(\bbeta_t |
\{y_s\}_{s=1}^t)$ when backwards sampling.  The CUBS method is also $\mcO(T)$ as
$T$ varies, but requires solving $T$ non-linear equations, which is
time consuming.

\npoint Metropolis-Hastings based approaches can take approximate draws and make
them exact by introducing an accept/reject step.  The challenge is to devise a
good approximation so that the probability of accepting a proposal is
reasonable.  As the dimension of the quantity one wants to sample increases,
this becomes more difficult.  A potential solution is to break the
high-dimensional quantity into pieces and sample the complete conditional of
these pieces sequentially, that is to do Metropolis-Hastings within Gibbs
sampling.  However, sampling the high-dimensional quantity in blocks tends to
increase the correlation between the successive samples within the Markov chain.
Thus, one must try to strike a balance between blocks that are too large,
leading to poor acceptance rates, and blocks that are too small, leading to
excessive autocorrelation.  One finds an extreme form of the blocking approach
in \cite{carlin-etal-1992}, prior to the advent of the FFBS.

\npoint Given this general strategy, one must still figure out how to pick a
good proposal distribution.  Since $f$ comes from the exponential family, it is
natural to use the Laplace approximation to arrive at a Gaussian proposal, as
this coincides with the iteratively reweighted least squares procedure for
generalized linear models \citep{gamerman-1997}.  One may follow a similar
strategy by first doing a change of coordinates to sample the disturbances
instead of the hidden states and then use a Laplace approximation
\citep{gamerman-1998, shephard-pitt-1997}.  Advocates of this strategy suggest
that the subsequent blocking possesses less intrinsic autocorrelation.
\cite{migon-etal-2013} find that the CUBS approximation of \cite{west-etal-1985}
works well.  We did our own comparison and found that, indeed, the approach of
\cite{migon-etal-2013} is superior to sampling blocks of disturbances.  However,
none of these Metropolis-Hastings are completely satisfactory.  In particular,
\cite{migon-etal-2013} is still time consuming, requiring $T$ non-linear solves
for each MCMC iteration.

\npoint Data augmentation provides a preferable approach.  If one finds a data
augmentation scheme for the static problem, that is when $\psi_t = \vx_t'
\bbeta$, so that the complete conditional of $\bbeta$ is Gaussian, then the
corresponding complete conditional of $\{\bbeta_t\}_{t=1}^T$ for the dynamic
analog will coincide with a DLM so that one may use of the FFBS algorithm.
%fake
\npoint But finding such a scheme is difficult since it requires auxiliary
variables that (1) yield a Gaussian complete conditional for $\bbeta$ and (2)
can be sampled efficiently.  For instance, \cite{mcfadden-1974} provides an
example where (1) is not met while \cite{holmes-held-2006} provide an example
where (2) can be significantly improved.

\npoint Fr\"{u}hwirth-Schnatter and her colleagues have developed fixes for
these shortcomings that rely upon approximating one or several distributions
using discrete mixtures of Gaussians.  Their work has lead to data augmentation
schemes that satisfy (1) and (2), in an approximate though accurate sense, for
binomial and multinomial logistic regression
\citep{fruhwirth-schnatter-fruhwirth-2007, fruhwirth-schnatter-fruhwirth-2010,
  fussl-etal-2013}, Poisson regression \citep{fruhwirth-schnatter-wagner-2006,
  fruhwirth-schnatter-etal-2009}, and negative binomial regression
\citep{fruhwirth-schnatter-etal-2009}.  (In the sequel, for dynamic binomial
logistic regression, we will limit our comparison to \cite{fussl-etal-2013}
since it appears to be the best choice within Fr\"{u}hwirth-Schnatter et al.'s
discrete mixture of normals cornucopia.)
%fake
\npoint While their methods work well, several rely upon precomputing large
tables of weights, means, and variances for the components of a collection of
mixtures that approximate an entire family of distributions.  Further all of the
discrete mixture of normal techniques make use of at least two layers of
auxiliary variables.  One would prefer to avoid many layers of latents since
this may inhibit traversing the posterior landscape and enlarges the memory
footprint when storing the latent states.

\npoint The situation is significantly harder once one abandons the structure we
have assumed to this point.  \cite{geweke-tanizaki-2001} have reviewed a
plethora of approaches within the more general setting of state-space models,
none of which work that well.  \cite{godsill-etal-2004} show how to sample
smoothed states, $(\{\bbeta_t\}_{t=1}^T | \{y_t\}_{t=1}^T)$, using particle
methods; however, this is relatively expensive in comparison to filtering
states.  Recently, \cite{geweke-etal-2013} leveraged the power of GPUs as a way
to significantly speed up sequential Monte Carlo, an interesting avenue not
considered herein.

\section{\Polya-Gamma Data Augmentation}
\label{sec:pg}

We consider DGLMs with binomial likelihoods: \( f(y_t | q_t) = c(y_t) q_t^{a_t}
(1-q_t)^{d_t}, \) where $a_t$ and $d_t$ do not depend upon $q_t$.  Logistic
regression and negative binomial regression are two common models that fit
within this regime.  It is preferable to express this likelihood in terms of the
log odds, $\psi_t = \log \frac{q_t}{1-q_t}$, since this is the scale on which we
linearly model the covariates:
\[
f(y_t | \psi_t) = c(y_t) \, \frac{\exp(\psi_t)^{a_t}}{(1+\exp(\psi_t))^{b_t}},
\]
where $b_t = a_t + d_t$.  Given a collection of observations $\yy =
\{y_t\}_{t=1}^T$, the posterior distribution is
\[
p(\betap | \yy) = c(\yy) \Big[ \prod_{t=1}^T
\frac{\exp({\psi_t})^{a_t}}{(1+\exp({\psi_t}))^{b_t}}
\Big] p(\betap),
\]
where $\psi_t = \vx_t' \bbeta_t$ and $\betap = \{\bbeta_t\}_{t=1}^T$.  (We will use
the generic function $p$ to denote a probability density.)  Following
\cite{polson-etal-2013}, one may introduce a collection of independent
\Polya-Gamma random variates $\oomega = \{\omega_t\}_{t=1}^T$, $\omega_t \sim
\PG(b_t, \psi_t)$ for $t=1, \ldots, T$, to construct the joint distribution
\[
p(\betap, \oomega | \yy) = c(\yy) \Big[ \prod_{t=1}^T
\frac{\exp({\psi_t})^{a_t}}{(1+\exp({\psi_t}))^{b_t}} p(\omega_t | b_t, \psi_t)
\Big] p(\betap) \, .
\]
via the conditional structure $p(\betap, \oomega | \yy) = p(\oomega | \betap,
\yy) p(\betap | \yy)$.  The $\PG{}(b_t, \psi_t)$ density possesses the special
form
\[
p(\omega_t | b_t, \psi_t) = \cosh^{b_t}(\psi_t/2) \exp({- \omega_t \psi_t^2 / 2})
p(\omega_t),
\]
which is useful since the ratio
\begin{equation}
  \label{eqn:pg-cancellation}
  \cosh^{b_t}(\psi_t/2) / (1+\exp({\psi_t}))^{b_t}
  \propto
  \exp({-\psi_t b_t/2}) 
\end{equation}
so that the complete conditional of $\bbeta$ is
% \[
% p(\beta | y, \omega) \propto
% \Big[ \prod_{t=1}^n e^{\kappa_t \psi_t - \frac{\omega_t}{2} \psi_t^2}
% \bbI \{\psi_t = \vx_t' \beta\} \Big] p(\beta)
% \]
% where $\kappa_t = a_t - b_t / 2$.  A single term from the product has
% \[
% \exp \Big( \kappa_t \psi_t - \frac{\omega_t}{2} \psi_t^2 \Big)
% \propto
% \exp \Big(-\frac{1}{2} (\kappa_t / \omega_t - \psi_t)^2 \omega_t \Big),
% \]
% which is identical to the likelihood of an observation $z_t = \kappa_t /
% \omega_t$ drawn from
% \[
% z_t \sim N(\vx_t' \beta, \omega_{i}^{-1}).
% \]
\[
p(\betap | \oomega, \yy) \propto \Big[ \prod_{t=1}^T \exp \Big(-\frac{\omega_t}{2}
\Big(\frac{\kappa_t}{\omega_t} - \psi_t\Big)^2 \Big) \Big] p(\betap), \; \psi_t
= \vx_t' \bbeta_t,
\]
where $\kappa_t = a_t - b_t / 2$.  A single term from the product above is
identical to the likelihood of a pseudo-data point $z_t = \kappa_t / \omega_t$
drawn from $z_t \sim N(\psi_t, 1/\omega_t)$.  Thus, if $p(\betap)$ specifies
that $\betap$ is a Gaussian AR(1) process, then sampling the complete
conditional for $\betap$ is equivalent to sampling $(\betap | \{z_t\}_{t=1}^T)$
from the DLM
\[
\begin{cases}
  z_t = \psi_t + \nu_t, & \nu_t \sim N(0, 1/\omega_t) \\
  \psi_t = \vx_t' \bbeta_t \\
  \bbeta_t = \bmu + \bPhi (\bbeta_{t-1} - \bmu) + \veta_t, & \veta_t \sim N(0, \bW).
\end{cases}
\]

Collecting these two complete conditionals leads to posterior simulation by
Gibbs sampling: draw $(\betap|\oomega, \yy)$ using the FFBS algorithm and draw
$(\oomega | \betap, \yy)$ by taking independent samples of $\omega_t \sim
\PG(b_t, \psi_t)$ for $t=1, \ldots, T$.  \cite{polson-etal-2013} describe how to
sample from $\PG$ distributions and implement this sampler in the \texttt{R}
package \texttt{BayesLogit} \citep{bayeslogit-2013}.  Sampling any
hyperparameters, like the autocorrelation coefficient of the AR(1) process or
the innovation variance, follows using standard conjugate or MCMC techniques.

\begin{example}
  Suppose that one observes binomial outcomes $y_t \sim \text{Binom}(n_t, q_t)$
  for $t=1, \ldots, T$.  Letting $\psi_t$ be the log-odds, the data generating
  distribution is
  \[
  p(y_t | \psi_t) = c(y_t) \frac{\exp(\psi_t)^{y_t}}{(1+\exp({\psi_t}))^{n_t}}.
  \]
  Thus the complete conditional $(\betap | y, \omega)$ may be simulated by using
  forward filter backward sampling with pseudo-data $z_t = \kappa_t / \omega_t$
  where $\kappa_t = y_t - n_t / 2$.
\end{example}


\begin{example}
  Suppose that one observes counts according to $y_t \sim \NB(d, q_t)$ for
  $t=1, \ldots, T$, where $d$ is the number of failures before observing $y_t$
  successes, also interpreted as a positive real-valued dispersion coefficient,
  and $q_t$ is the probability of observing a success.  Letting $\psi_t$ be the
  log-odds, the data generating distribution is
  \[
  p(y_t | \psi_t) = c(y_t, d) \frac{\exp({\psi_t})^{y_t}}{(1+\exp({\psi_t}))^{y_t+d}}.
  \]
  In negative binomial regression, it is common to model the log-mean, $\lambda_t
  = \psi_t + \log(d) = \vx_t' \bbeta_t$, instead of the log-odds.  This requires only
  a slight modification.  Following the work above, the complete conditional
  $(\omega_t | \bbeta_t, d)$ is $\PG(b_t, \psi_t)$ where $b_t = y_t + d$.  However,
  the DLM used to estimate $\betap$ is now
  % \[
  % \begin{cases}
  %   z_t = \psi_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
  %   \psi_t = \vx_t \bbeta_t - \log(d) \\
  %   \bbeta_t = \mu + \bPhi (\bbeta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W)
  % \end{cases}
  % \]
  % where $z_t = \kappa_t / \omega_t$ and $\kappa_t = (y_t - d_t) / 2$.
  \[
  \begin{cases}
    z_t = \lambda_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
    \lambda_t = \vx_t' \bbeta_t \\
    \bbeta_t = \bmu + \bPhi (\bbeta_{t-1} - \bmu) + \veta_t, & \veta_t \sim N(0, \bW)
  \end{cases}
  \]
  where $z_t = \kappa_t / \omega_t + \log(d)$ and $\kappa_t = (y_t - d) / 2$.
\end{example}

% To estimate $d$ in dynamic negative binomial regression one may use a
% Metropolis-Hastings step (Appendix \ref{sec:sample-d}).

% \subsection{AR(1) Example}

\begin{figure}
  \begin{center}
    \includegraphics[width=5.5in]{nb-ar1-tophalf.pdf}
    \caption{\label{fig:nb-ar1} Incidence of influenza-like illness in Texas,
      2008--11, together with the estimated mean from the negative-binomial AR(1)
      model.  The blanks in weeks 21-41 correspond to missing data.  The grey lines
      depict the upper and lower bounds of a 95$\%$ predictive interval.}
  \end{center}
\end{figure}

As an initial illustration, we fit a negative-binomial AR(1) model to four years
(2008--11) of weekly data on flu incidence in Texas, collected from the Texas
Department of State Health Services.  Let $y_t$ denote the number of reported
cases of influenza-like illness in week $t$.  We assume that these counts follow
a negative-binomial model, which will allow over-dispersion relative to the
Poisson.
% \begin{eqnarray*}
% %   y_t &\sim& \mbox{NB}(h, q_t) \; , \quad q_t= \frac{e^{\psi_t}}{1+e^{\psi_t}} \\
%   \psi_t &=& \alpha + \gamma_t  \; , \quad \gamma_t = \phi \gamma_{t-1} + \epsilon_t \\
%   \epsilon_t &\sim& N(0, \sigma^2) \, .
% \end{eqnarray*}
Figure \ref{fig:nb-ar1} shows the results of the fit. For simplicity, we assumed
an improper uniform prior on the dispersion parameter $d$, and fixed $\phi$ and
$\sigma^2$ to $0.98$ and 1, respectively, but it is straightforward to place
hyper-priors upon each parameter, and to sample them in a hierarchical fashion.
It is also straightforward to incorporate fixed effects in the form of
regressors.

% We emphasize that there are many ways to handle the over-dispersion present in
% this data set, and that we do not intend our model to be taken as a definitive
% analysis.  We merely intend it as a proof of concept, showing how various
% aspects of Bayesian hierarchical modeling---in this case, a simple AR(1)
% model---can be combined routinely with binomial likelihoods using the
% \Polya-Gamma scheme.

\section{Comparison}

% Dynamic versions of binomial logistic regression and negative binomial
% regression are common models.  Hence, it is important to understand which
% methods work best for each model.

Since Markov Chains generate correlated samples we compare methods by measuring
how fast each procedure produces \emph{nearly} independent samples, that is we
measure the effective sampling rate (ESR).  To that end, we employ the effective
sample size (ESS), which approximates the number of ``independent'' draws
produced by a sample of $M$ correlated draws.  One may view the ESS as an
estimate of the number of samples produced after having thinned the $M$
correlated samples so that remaining draws appear independent.  From
\cite{holmes-held-2006}, the effective sample size is
\[
ESS_{it} = M / \Big( 1 + 2 \sum_{k=1}^\ell \rho_k(\beta_{it}) \Big)
\]
where $\rho_k(\beta_{it})$ is the $k$th lagged autocorrelation of the chain
corresponding to the $i$th component of $\bbeta_t$.  The effective
sampling rate is the ratio of the effective sample size to the time taken to
generate the post-burn-in samples; thus, it measures the rate at which the
Markov Chain produces independent draws after initialization and burn-in.

To mitigate MCMC sample variation, 10 batches of 12,000 samples are taken and
the last 10,000 draws are recorded.  For batch $m$, we compute the
component-wise effective sample size $ESS_{it}^{m}$ corresponding to the
univariate chain for $\beta_{it}$.  Taking the mean over batches produces the
average, component-wise effective sample size $\overline{ESS}_{it}$ and, after
normalizing each batch by time, the average, component-wise effective sampling
rate $\overline{ESR}_{it}$.  Following,
\cite{fruhwirth-schnatter-fruhwirth-2010}, the primary metric of comparison is
the median effective sampling rate,
\[
\text{med} \; \Big\{ \overline{ESR}_{it} : i=1, \ldots, P; \;  t=1, \ldots, T \Big\}.
\]

We consider synthetic data sets with a variety of characteristics.
% since this is an empirical, as opposed to theoretical assessment.
For dynamic binomial logistic regression, we consider log odds of the form
$\alpha + \vx_t' \bbeta_t$ where $\alpha$ is a scalar and $\{\bbeta_t\}_{t=1}^T$
is a 2-dimensional AR(1) process with autocorrelation coefficient $\bPhi = 0.95
I_2$.  Four different synthetic data sets are constructed, allowing for low and
high correlation amongst $\{\vx_t\}_{t=1}^T$ and responses, $y_t \sim
\text{Binom}(n, q_t), t=1, \ldots, T$, with either $n=1$ or $n=20$ trials.  The
setup is almost identical for dynamic negative binomial regression except that
we model the log-mean as $\alpha + \vx_t' \bbeta_t$ and consider responses with
$\alpha = \log(10)$ or $\alpha = \log(100)$ corresponding to average count sizes
of roughly 10 or 100.  Further details may be found in the Appendix.

Some caution is warranted when comparing methods as the effective sampling rate
is sensitive to a procedure's implementation and the hardware on which it is
run. The routines are written primarily in R.  We use code from
\cite{binomlogit-2012} and \cite{fruhwirth-schnatter-book-2007} for the discrete
mixture of normals methods.  All benchmarks are carried out on an Ubuntu machine
with Intel Core i5-3570 3.4GHz processor and 8GB of RAM.  Some computations were
burdensome in R, and hence we wrote wrappers to C to speed up the MCMC
simulations.  In particular, both data augmentation methods implement forward
filtering and backward sampling using a C wrapper.  The \Polya-Gamma technique
calls C code to sample random variates using version 0.2-4 of the
\texttt{BayesLogit} package \citep{bayeslogit-2013}.  The conjugate updating and
backwards sampling of \cite{migon-etal-2013} is done in C.  Having a C wrapper
to forward filter and backwards sample is particularly important, as our C
implementation is much faster than the corresponding R code.  Were we to use the
slower R version, our results would favor the \Polya-Gamma method, as it has
better effective sample sizes and would spend less time, proportionally,
sampling the latent random variables.

\cite{polson-etal-2013} outline the expected performance of the \Polya-Gamma
data augmentation technique, which depends heavily on how quickly one can
generate \Polya-Gamma random variates.  In their original algorithm, sampling
\Polya-Gamma random variates from $\PG(b,\psi)$ is fast when $b$ is a small
integer, but slow when $b$ is large. \cite{windle-thesis-2013} provides an
improved method for sampling $\PG(b,\psi)$; however sampling large $b$ is still
slower than sampling small $b$.  These differences in computational cost are
important, as one must sample $\omega_t \sim \PG(b_t, \psi_t), t=1,\ldots,T$,
for each MCMC iteration under \Polya-Gamma data augmentation.  In binomial
logistic regression, $b_t = n_t$ where $n_t$ is the number of trials at each
response $y_t$.  Hence, when $n_t$ is small the PG method will do well.  In
negative binomial regression, $b_t = y_t + d_t$ where $y_t$ is a response and
$d_t$ is the dispersion.  Thus, larger average responses should lead to longer
MCMC simulations.

In general, we find these principles to hold.  The \Polya-Gamma data
augmentation technique performs well for dynamic binomial logistic regression
when the number of trials of the response is small, showing a 25\% better ESR
for binary logistic regression than \cite{fussl-etal-2013}; however,
\cite{fussl-etal-2013} does better when the number of trials is large.
Similarly, the \Polya-Gamma technique outpaces
\cite{fruhwirth-schnatter-etal-2009} in negative binomial regression when the
average number of counts is small, but loses when the average number of counts
is large.  The Metropolis-Hastings approach of \cite{migon-etal-2013} does the
worst in all of the tests.  Part of its poor performance is due to a non-linear
solve that must be made $T$ times when forward filtering.  We did not attempt to
optimize the performance of this non-linear solver, and hence some improvement
may be possible, though the disparity in ESRs suggests that any improvement
would not be enough to compete with either data augmentation approach.  As a
check upon \cite{migon-etal-2013}, we also implemented a Metropolis-Hastings
sampler that draws blocks of disturbances using Laplace approximations.  This
fared worse still.

Of note, the \Polya-Gamma method almost always has superior effective sample
sizes.  Hence faster \Polya-Gamma samplers could push the \Polya-Gamma data
augmentation technique to the top for all of the models considered.  Table
\ref{tab:benchmark-summary} provides a summary of the benchmarks; Tables
\ref{tab:dynlogit-detail} and \ref{tab:dynnb-detail} provide a more detailed
account of each benchmark.

% Summary Table
% -------------------------------------------------------------------------------
\begin{table}

  \begin{center}
    \caption{\label{tab:benchmark-summary} A summary of the benchmarking results.}
    \hrule
    \vspace{2pt}
    \hrule
  \end{center}
  \begin{center}
    \small
    \begin{tabular}{l l c c}
      \multicolumn{4}{c}{Dynamic Binom.\ Logistic Reg.} \\
      \hline
      & & Est.\ & ESR \\
      n & f & AR & PG/dRUM \\
      \hline
      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-wout.ar
      % PG / dRUM
      $1$ & low & no & 1.26 \\
      % PG / CUBS 47.5399
      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-with.ar
      % PG / PG 1
      % PG / dRUM
      $1$ & low & yes & 1.29 \\
      % PG / CUBS 287.291

      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-wout.ar
      % PG / PG 1
      % PG / dRUM
      $1$ & high & no & 1.25 \\
      % PG / CUBS 36.9012
      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-with.ar
      % PG / PG 1
      % PG / dRUM
      $1$ & high & yes & 1.26 \\
      % PG / CUBS 175.964

      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-wout.ar
      % PG / PG 1
      % PG / dRUM
      $20$ & low & no & 0.52 \\
      % PG / CUBS 29.4906
      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-with.ar
      % PG / PG 1
      % PG / dRUM
      $20$ & low & yes & 0.57 \\
      % PG / CUBS 49.9306

      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-wout.ar
      % PG / PG 1
      % PG / dRUM
      $20$ & high & no & 0.48 \\
      % PG / CUBS 13.323
      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-with.ar
      % PG / PG 1
      % PG / dRUM
      $20$ & high & yes & 0.58
      % PG / CUBS 38.9124
    \end{tabular}
    % 
    \hspace{12pt}
    % 
    \begin{tabular}{l l c c}
      \multicolumn{4}{c}{Dynamic Neg.\ Binomial Reg.} \\
      \hline
      & & Est.\ & ESR \\
      $\mu$ & f & AR & PG/FS \\
      \hline
      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-wout.ar
      % PG / PG 1
      % PG / FS
      $10$ & low & no & 1.03 \\
      % PG / CUBS 42.3582
      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-with.ar
      % PG / PG 1
      % PG / FS
      $10$ & low & yes & 1.86 \\
      % PG / CUBS 59.1934

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-wout.ar
      % PG / PG 1
      % PG / FS
      $10$ & high & no & 1.15 \\
      % PG / CUBS 20.6336
      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-with.ar
      % PG / PG 1
      % PG / FS
      $10$ & high & yes & 1.06 \\
      % PG / CUBS 18.0189

      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-wout.ar
      % PG / PG 1
      % PG / FS
      $100$ & low & no & 0.76 \\
      % PG / CUBS 103.273
      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-with.ar
      % PG / PG 1
      % PG / FS
      $100$ & low & yes & 0.82 \\
      % PG / CUBS 102.229

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-wout.ar
      % PG / PG 1
      % PG / FS
      $100$ & high & no & 0.76 \\
      % PG / CUBS 34.7778
      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-with.ar
      % PG / PG 1
      % PG / FS
      $100$ & high & yes & 0.78 \\
      % PG / CUBS 69.6132

    \end{tabular}
  \end{center}

  NOTE: ESR PG/dRUM and ESR PG/FS report the ratio of the median effective
  sampling rate for the PG method compared to the best alternative, which in both
  cases is the discrete mixture of normal approaches \citep{fussl-etal-2013,
    fruhwirth-schnatter-etal-2009}.  A higher ratio corresponds to the PG method
  doing better.  $n$ corresponds to the number of trials for the binomial response
  while $\mu$ corresponds to the approximate mean for the negative binomial
  response.  $f$ determines whether there is a low or high amount of correlation
  between the covariates.  Est.\ AR indicates whether the parameters of the AR
  process were estimated or not.

\end{table}

\section{Conclusion}

Considering efficiency, we suggest using the \Polya-Gamma method as the default
approach for dynamic binomial logistic regression and the method of
\cite{fruhwirth-schnatter-etal-2009} as the default method for dynamic negative
binomial regression.  There are two exceptions to this rule: (1) if the number
of trials in the dynamic binomial logistic regression is large, it may be better
to use the method of \cite{fussl-etal-2013} and (2) if the average count size in
the dynamic negative binomial regression is small, it may be better to use the
\Polya-Gamma data augmentation approach.

Usability is as important as efficiency, if not more so, and from that vantage
we recommend the \Polya-Gamma method in all cases.  The \Polya-Gamma approach,
adhering to the first principle of engineering, is simple, in both concept and
practice.  Conceptually, one need only understand an algebraic relationship
(Equation \ref{eqn:pg-cancellation}) to follow its derivation, unlike the
discrete mixture of normals approach, which is based upon the data generating
process and hence the mixing of distributions.  Practically, one need only be
comfortable with heteroscedastic Gaussian likelihoods---familiar territory for
most everyone---as the \Polya-Gamma auxiliary variables enter into the Gaussian
form in terms of the variance and do not require another latent quantity like
\cite{fruhwirth-schnatter-etal-2009} and \cite{fussl-etal-2013}.  These
assertions hold, not just for the AR(1) prior presented previously, but for
\emph{any} bespoke linear evolution equations, such as dynamic factor models.
Since the requisite Gaussian machinery is well established, the only challenge
is sampling the conditional distribution of the latent variables, but this
reduces to simulating \Polya-Gamma random variates, which can be done using the
\texttt{BayesLogit} R Package.  Thus, the only novel aspect of implementing the
\Polya-Gamma data augmentation approach is one line of code within an R script.
(The C code used by the \texttt{BayesLogit} package is freely available, so this
applies equally well to any scripting language that can make calls to external C
routines.)  Further, for all models benchmarked herein, the \Polya-Gamma
technique has superior effective sampling size, so that more efficient
\Polya-Gamma samplers will diminish any gaps in performance.  When improved
samplers arise, users of the \Polya-Gamma technique need not make any changes to
their code, they can simply download the latest version of the R package.  The
overwhelming simplicity and adaptability of the method makes it a good choice
regardless of its efficiency.

% \appendix
\section*{Appendix: Benchmark Data Sets}
\label{sec:benchmark-details}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%% 06 Benchmarks %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

For both binomial responses and negative binomial responses, we create four
synthetic data sets whose log-odds and log-mean, respectively, take the form
\begin{equation}
  \label{eqn:synth-psi}
  \psi_t = \alpha + \vx_t' \bbeta_t, \; t = 1, \ldots, T
\end{equation}
with
\[
\bbeta_t = \bPhi \bbeta_{t-1} + \ep_t, \; \ep_t \sim N(0, \bW)
\]
where $\bPhi = 0.95 \bI_2$ and $\bW = 0.172 \bI_2$.  The covariates $\vx_t \in
\bbR^2$ are
\[
x_{tj} = \cos(\pi \vf_j s_t) / \sqrt{8}
\]
where $\{s_t\}_{t=1}^T$ is an equally spaced partition of $[-1,1]$ and $\vf$ is
either $\vf^{(low)} = (1,2)$ or $\vf^{(high)} = (1,1.1)$, corresponding to
covariates with a lower or higher degree of correlation.  The binomial response
is
\[
y_t \sim \text{Binom}(n, q_t)
\]
where $q_t = \exp({\psi_t}) / (1+\exp({\psi_t}))$, $n$ is either $1$ or $20$,
and $\alpha$ from (\ref{eqn:synth-psi}) set to 1.4.  The negative binomial
response is
\[
y_t \sim \text{NB}(d, \mu_t)
\]
where $d=4$, $\mu_t = \exp({\psi_t})$, and $\alpha$ from (\ref{eqn:synth-psi})
is either $\log(10)$ or $\log(100)$.  Thus, for dynamic binomial logistic
regression, we create four data sets by varying $n$ and $f$, and for dynamic
negative binomial regression, we create four data sets by varying $f$ and
$\alpha$.

For each data set and each method, we run the MCMC simulations twice, once
assuming that the parameters of the AR model are known and again assuming that
they are unknown.  When the parameters are unknown, both $\bPhi$ and $\bW$ are
assumed to be diagonal; the prior for $\bPhi$ is $\bPhi_{ii} \sim N(0.95, 0.1)
\bbI_{(0,1)}$ and the prior for $\bW$ is $\bW_{ii} \sim \Ga(300/2, 30/2)$.  For
all simulations, the prior for $\alpha$ is $N(0, 100)$ and the prior for
$\bbeta_0$ is $N(0, 100 \bI_2)$.  For the negative binomial benchmarks, $d$ is
taken to be known.
%% My note: $d$ unknown will ``hurt'' results in that it will induce extra
%% correlation, which presumably will hurt PG,mu=10 results.

Four data sets for each type of response, along with two prior structures yields
eight MCMC simulations for each method of posterior inference.  Tables
\ref{tab:dynlogit-detail} and \ref{tab:dynnb-detail} provide a complete summary
of the results.

% -------------------------------------------------------------------------------
% \section{Sampling $d$ in dynamic negative binomial regression}
% \label{sec:sample-d}

% We may sample $d$ by an independent Metropolis-Hastings or a random-walk
% Metropolis-Hastings.  In either case, we need the log-likelihood of $d$.

% The conditional density for a single term $y_t \sim N(d, q_t)$ is
% \[
% \frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} q_t^{y_t} (1-q_t)^{d} ,
% \]
% or
% \[
% \frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t}
% \Big( \frac{\mu_t}{\mu_t + d} \Big)^{y_t} \Big( \frac{d}{\mu_t + d} \Big)^d
% \]
% in terms of the mean $\mu_t$.  Since we chose to model $\log \mu_t = \vx_t
% \beta_t$ it will be easier to work with the latter version.  In that case, the
% log-likelihood is
% \[
% \ell(d|\mu, y) = \sum_{t=1}^T [ \log \Gamma(y_t + d)  - \log \Gamma(d) ]  +
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]

% If we assume that $d$ is a natural number then we can rid ourselves of $T$
% evaluations of the gamma function.  We may expand $\Gamma(y_t + d)$ and
% $\Gamma(d)$ as products, in which case the log-likelihood is
% \[
% \ell(d|\mu, y) = \sum_{t=1}^T \sum_{j=0}^{y_t-1} \log(d + j) +
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]
% We can rewrite the first term of the sum as
% \[
% \sum_{t=1}^T \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{k-1} \log(d+j) \one \{y_t = k\}
% \]
% which becomes
% \[
% \sum_{k=1}^{\max(y_t)} n_k \sum_{j=0}^{k-1} \log(d+j)
% \]
% where $n_k = \{ \# y_t = k \}$.  We have thus
% \begin{align*}
%   \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{\max(y_t)-1} n_k \log(d+j) \one\{j < k\}
%   & =
%   \sum_{j=0}^{\max(y_t)-1} \log(d+j) \sum_{k=j+1}^{\max(y_t)} n_k \\
%   & = \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j
% \end{align*}
% where $G_j = \{ \# y_t > j \}$.  Notice that $1-G_j = F(j) = \{\# y_t \leq j\}$
% and that $G$ may be pre-processed.  Hence the log-likelihood is
% \[
% \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j +
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]
% Preprocessing $G$ saves us from having to compute a doubly indexed summation.

% BIBLIOGRAPHY
% -------------------------------------------------------------------------------

% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliography{bayeslogit}{}
\bibliographystyle{asa}

% TABLES
% -------------------------------------------------------------------------------
% dyn logit

% More detailed tables
% -------------------------------------------------------------------------------
\begin{table}

  \begin{center}
    \caption{\label{tab:dynlogit-detail} Dynamic binomial logistic benchmarks.}
    \hrule
    \vspace{2pt}
    \hrule
  \end{center}

  \small

  \begin{center}
    % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-wout.ar
    \begin{tabular}{l r r r r r r r r }
      Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &
      ESR.min  &  ESR.med  &  ESR.max  \\
      \hline

      \multicolumn{9}{c}{$n=1$, $f = (1,2)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    28.66 &     1.00 &   8333.94 &   9388.18 &   9904.42 &    290.79 &    327.55 &    345.57 \\
      dRUM   &    29.13 &     1.00 &   5344.83 &   7550.10 &   9757.91 &    183.48 &    259.19 &    334.98 \\
      CUBS   &   612.14 &     0.71 &   2799.89 &   4219.25 &   5845.51 &      4.57 &      6.89 &      9.55
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-with.ar
      % \begin{tabular}{l r r r r r r r r }

      \multicolumn{9}{c}{$n=1$, $f = (1,2)$, unknown AR parameters} \\

      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    31.33 &     1.00 &   1855.82 &   7109.57 &   9739.31 &     59.24 &    226.96 &    310.89 \\
      dRUM   &    31.95 &     1.00 &   1535.90 &   5632.82 &   8921.27 &     48.07 &    176.30 &    279.22 \\
      CUBS   &   612.37 &     0.56 &    242.71 &    487.21 &    775.24 &      0.39 &      0.79 &      1.26
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-wout.ar
      % \begin{tabular}{l r r r r r r r r }

      \multicolumn{9}{c}{$n=1$, $f = (1,1.1)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    28.60 &     1.00 &   8596.25 &   9402.62 &   9933.72 &    300.60 &    328.79 &    347.36 \\
      dRUM   &    29.13 &     1.00 &   6095.80 &   7675.76 &   9634.57 &    209.27 &    263.51 &    330.77 \\
      CUBS   &   620.95 &     0.72 &   4120.25 &   5531.41 &   6817.09 &      6.64 &      8.91 &     10.98
      \\ %\end{tabular}


      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=1$, $f = (1,1.1)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    31.37 &     1.00 &    765.97 &   4636.70 &   9415.57 &     24.42 &    147.81 &    300.15 \\
      dRUM   &    31.96 &     1.00 &    656.58 &   3734.29 &   8079.58 &     20.55 &    116.85 &    252.82 \\
      CUBS   &   615.39 &     0.56 &    211.12 &    516.69 &    998.70 &      0.34 &      0.84 &      1.61
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=20$, $f = (1,2)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    73.54 &     1.00 &   7161.18 &   9238.58 &   9923.40 &     97.38 &    125.63 &    134.95 \\
      dRUM   &    28.87 &     1.00 &   3528.34 &   6959.36 &   9893.99 &    122.21 &    241.04 &    342.69 \\
      CUBS   &   650.67 &     0.51 &    727.34 &   2770.01 &   3699.75 &      1.12 &      4.26 &      5.69
      \\ %\end{tabular}


      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=20$, $f = (1,2)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    76.26 &     1.00 &   1897.09 &   5482.92 &   9758.10 &     24.88 &     71.90 &    127.95 \\
      dRUM   &    31.69 &     1.00 &   1754.39 &   4003.81 &   9536.59 &     55.36 &    126.33 &    300.91 \\
      CUBS   &   662.42 &     0.47 &    498.89 &    953.03 &   1980.98 &      0.75 &      1.44 &      2.99
      \\ %\end{tabular}


      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=20$, $f = (1,1.1)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    73.16 &     1.00 &   7918.31 &   9416.01 &   9810.23 &    108.23 &    128.70 &    134.09 \\
      dRUM   &    28.88 &     1.00 &   4654.77 &   7771.95 &   9041.13 &    161.17 &    269.10 &    313.05 \\
      CUBS   &   659.75 &     0.63 &   3813.36 &   6375.18 &   6741.68 &      5.78 &      9.66 &     10.22
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=20$, $f = (1,1.1)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    75.98 &     1.00 &    408.79 &   5735.76 &   9484.30 &      5.38 &     75.49 &    124.83 \\
      dRUM   &    31.71 &     1.00 &    418.48 &   4106.12 &   8269.33 &     13.20 &    129.48 &    260.76 \\
      CUBS   &   665.71 &     0.47 &    273.76 &   1290.78 &   1893.79 &      0.41 &      1.94 &      2.84
    \end{tabular}
  \end{center}

  NOTE: $n$ corresponds to the number of trials for each response; $f$ corresponds
  to the frequency used when constructing the covariates.  When $f = (1,2)$ the
  covariates are less correlated when $f=(1,1.1)$ the covariates are more
  correlated.  The PG method does well when $n=1$.  The FS method does well when
  $n=20$.  Time refers to the time taken to produce 10,000 post burn-in samples.
  ARate refers to the Metroplis-Hastings acceptance rate.
\end{table}

% -------------------------------------------------------------------------------
% dyn-nb

\begin{table}

  \begin{center}
    \caption{\label{tab:dynnb-detail} dynamic negative binomial benchmarks.}
    \hrule
    \vspace{2pt}
    \hrule
  \end{center}

  % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-wout.ar
  \begin{center}
    \small
    \begin{tabular}{l r r r r r r r r }
      Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  & ESR.med  &  ESR.max \\
      \hline

      \multicolumn{9}{c}{$\mu=10$, $f = (1,2)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    42.50 &     1.00 &   7972.97 &   9649.48 &   9947.01 &    187.60 &    227.04 &    234.04 \\
      FS   &    29.54 &     1.00 &   1861.10 &   6530.79 &   9831.67 &     62.99 &    221.05 &    332.77 \\
      CUBS   &   655.71 &     0.61 &    800.95 &   3517.15 &   5603.81 &      1.22 &      5.36 &      8.55
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=10$, $f = (1,2)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    45.29 &     1.00 &    156.63 &   4852.66 &   9812.26 &      3.46 &    107.14 &    216.67 \\
      FS   &    32.31 &     1.00 &    135.06 &   1856.03 &   7837.19 &      4.18 &     57.45 &    242.58 \\
      CUBS   &   674.23 &     0.55 &    116.36 &   1223.44 &   2700.52 &      0.17 &      1.81 &      4.01
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=10$, $f = (1,1.1)$, known AR parameters} \\
      % Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
      PG  &    42.39 &     1.00 &   9142.96 &   9692.30 &   9943.59 &    215.67 &    228.62 &    234.55 \\
      FS  &    29.55 &     1.00 &   2820.14 &   5898.99 &   9765.03 &     95.43 &    199.61 &    330.43 \\
      CUBS  &   656.58 &     0.73 &   4509.25 &   7276.52 &   7891.49 &      6.87 &     11.08 &     12.02

      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=10$, $f = (1,1.1)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    45.16 &     1.00 &    276.72 &   1725.30 &   9555.14 &      6.13 &     38.20 &    211.58 \\
      FS   &    32.28 &     1.00 &    263.56 &   1164.83 &   5458.53 &      8.16 &     36.08 &    169.10 \\
      CUBS   &   677.20 &     0.62 &    405.83 &   1438.07 &   3812.09 &      0.60 &      2.12 &      5.63
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=100$, $f = (1,2)$, known AR parameters} \\
      % Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
      PG  &    38.56 &     1.00 &   1860.03 &   7287.72 &   9927.30 &     48.23 &    188.99 &    257.43 \\
      FS  &    29.17 &     1.00 &   2352.61 &   7301.91 &   9827.50 &     80.65 &    250.31 &    336.89 \\
      CUBS  &   669.34 &     0.38 &    228.97 &   1226.28 &   2833.25 &      0.34 &      1.83 &      4.23

      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=100$, $f = (1,2)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    41.33 &     1.00 &   1798.58 &   4986.44 &   9178.54 &     43.51 &    120.63 &    222.06 \\
      FS   &    32.28 &     1.00 &   2098.04 &   4727.49 &   9132.24 &     65.00 &    146.45 &    282.90 \\
      CUBS   &   677.94 &     0.32 &    459.22 &    799.84 &   1194.99 &      0.68 &      1.18 &      1.76
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=100$, $f = (1,1.1)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    40.73 &     1.00 &   4061.25 &   7265.75 &   9856.66 &     99.72 &    178.41 &    242.03 \\
      FS   &    29.44 &     1.00 &   3396.35 &   6938.80 &   9867.25 &    115.36 &    235.66 &    335.13 \\
      CUBS   &   668.03 &     0.42 &   1676.35 &   3429.40 &   4146.91 &      2.51 &      5.13 &      6.21
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=100$, $f = (1,1.1)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    43.57 &     1.00 &   1186.07 &   3214.67 &   6880.24 &     27.22 &     73.79 &    157.93 \\
      FS   &    32.25 &     1.00 &   1115.12 &   3071.79 &   6879.66 &     34.58 &     95.25 &    213.31 \\
      CUBS   &   682.59 &     0.30 &    412.62 &    723.17 &   1028.33 &      0.60 &      1.06 &      1.51
    \end{tabular}
  \end{center}

  NOTE: $\mu$ corresponds to the mean of the response; $f$ corresponds to the
  frequency used when constructing the covariates.  When $f = (1,2)$ the
  covariates are less correlated when $f=(1,1.1)$ the covariates are more
  correlated.  The PG method does well when $\mu=10$.  The FS method does well
  when $\mu=100$. Time refers to the time taken to produce 10,000 post burn-in
  samples.  ARate refers to the Metroplis-Hastings acceptance rate.

\end{table}

\end{document}

% \begin{tabular}{l r r r r r r r r }
%   \hline
%   \multicolumn{9}{c}{Tokyo Rainfall} \\
%   \hline
%   Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\
%   CUBS  &   233.98 &     1.00 &   652.86 &   761.98 &   910.62 &     2.79 &     3.26 &     3.89 \\
%   PG  &    21.72 &     1.00 &  4352.80 &  7735.08 & 10081.46 &   200.43 &   356.18 &   464.22 \\
%   dRUM  &    20.60 &     1.00 &  1627.61 &  3649.46 &  5428.93 &    79.03 &   177.19 &   263.60
% \end{tabular}
% \end{table}

% \begin{table}
%   \centering
%   \begin{tabular}{l r r r r r r r r }
%     \hline
%     \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta$.} \\
%     \hline
%     Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
%     PG  &    22.59 &     1.00 &   7367.68 &   9229.91 &  10189.37 &    326.10 &    408.52 &    450.97 \\
%     dRUM  &    23.19 &     1.00 &   2759.83 &   7260.46 &   9967.66 &    118.99 &    313.04 &    429.77
%   \end{tabular}

%   \begin{tabular}{l r r r r r r r r }
%     \hline
%     \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta$.} \\
%     \hline
%     Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
%     PG  &    22.52 &     1.00 &   8494.04 &   9662.06 &  10225.35 &    377.13 &    428.98 &    454.00 \\
%     dRUM  &    23.17 &     1.00 &   4963.68 &   8105.02 &   9936.46 &    214.26 &    349.86 &    428.91
%   \end{tabular}

% \end{table}


% \begin{table}
%   \centering
%   \begin{tabular}{l r r r r r r r r }
%     \hline
%     \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta, \phi, W$.} \\
%     \hline
%     Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
%     PG  &    25.04 &     1.00 &    643.23 &   6531.34 &  10420.88 &     25.69 &    260.82 &    416.13 \\
%     dRUM  &    25.73 &     1.00 &    430.79 &   4490.34 &  10124.47 &     16.74 &    174.55 &    393.48
%   \end{tabular}

%   \begin{tabular}{l r r r r r r r r }
%     \hline
%     \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta, \phi, W$.} \\
%     \hline
%     Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
%     PG  &    25.16 &     1.00 &    988.83 &   5654.23 &  10337.07 &     39.31 &    224.75 &    410.87 \\
%     dRUM  &    25.84 &     1.00 &   1040.12 &   5496.94 &   9760.64 &     40.25 &    212.74 &    377.75
%   \end{tabular}



