\documentclass[11pt]{article}

\input{commands}

\usepackage{natbib}

\newcommand{\Polya}{P\'{o}lya}
\newcommand{\PG}{\text{PG}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Ga}{\text{Ga}}
\newcommand{\NB}{\text{NB}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\eqd}{\,{\buildrel d \over =}\,}
\newcommand{\KS}{\text{KS}}
\newcommand{\bbeta}{\boldsymbol{\beta}}

\newcommand{\point}{\noindent $\bullet$ }

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
%\parindent 0in
%\parskip 12pt

% Set Fancy Style
%\pagestyle{fancy}

%\lhead{Left Header}
%\rhead{Right Header}

\title{Dynamic Generalized Linear Models using \Polya-Gamma augmentation}
\author{Carlos M. Carvalho, James G. Scott, Liang Sun, Jesse Windle}

\begin{document}

\maketitle
\tableofcontents

\newpage

\section{Introduction}

In this paper, we show how the \Polya-Gamma data augmentation technique of
\cite{polson-etal-2012} may be used for posterior inference in dynamic
generalized linear models with logistic likelihoods.  Further, we show that this
method outperforms several competitors.

Generalized linear models extend linear models by accomodating non-normal
responses \citep{mccullagh-nelder-1989}.  In a generalized linear model (GLM)
the response $y_t \sim P(\psi_t)$ depends on $\psi_t$ in a possibly non-linear
way while $\psi_t$ itself is a linear combination of predictors, $x_t' \beta$.
Extensions to the dynamic case are straightforward via a dynamic regression:
$\psi_t = x_t' \beta_t$, where one must specify the dynamics of $\beta_t$ to
complete the model.  Unlike Gaussian, linear models, most likelihoods found in
generalized linear models do not lead to convenient posterior distributions for
$\beta$ or $\{\beta_t\}_{t=0}^T$.  Thus, when generating posterior samples one
must appeal to Metropolis-Hastings, Gibbs sampling with data augmentation, or
some other MCMC method.

The challenge of generating posterior samples is exacerbated for dynamic models
since there are many more parameters, or states rather, to estimate, which makes
direct extensions of Metropolis-Hastings samplers from the static to dynamic
case difficult.  This is the case for GLMs with logistic likelihoods, a subclass
that has observation densities of the form
\[
p(y | \psi) \propto h(y) \prod_{i=1}^n
\frac{(e^{\psi_i})^{a_i(y)}}{(1+e^{\psi_i})^{b_i(y)}}
\]
and for which \Polya-Gamma data augmentation is applicable.  Such likelihoods
arise when modeling proportions or counts.  For instance, binomial logistic
regression and negative binomial regression may be written in the form above.
These two models alone are useful in a wide variety of applications.  The
dynamic versions of these models are used in economics [McFadden], epidemiology
[cite Lauren], ecology [cite Lauren], and neuroscience [cite Jonathan and Larry]
amongst others.  Hence, it is important to know the most efficient methods for
generating posterior samples for such models.

This paper examines the relative merits of the three approaches for posterior
inference in dynamic generalized linear models (GDLM) with logistic likelihoods:
linear Bayes with conjugate updating and a Metroplis-Hastings step
\citep{ravines-etal-2006}, a discrete mixture of normals approximation
\citep{fruhwirth-schnatter-etal-2009, fussl-etal-2013}, and the \Polya-Gamma
data augmentation scheme.  We show that the \Polya-Gamma technique has superior
effective sample sizes and often superior effective sampling rates compared to
the other methods.

The outline of the paper is as follows: Section 2 reviews other methods of
inference for dynamic generalized linear models; Section 3 shows how one may use
the \Polya-Gamma data augmentation technique in the dynamic setting; Section 4
presents the benchmarking results; Section 5 concludes.

\section{Previous Efforts}

Bayesian inference for dynamic generalized linear models dates back to at least
\cite{west-etal-1985} who used conjugate updating with backwards sampling by
linear Bayes (CUBS) to sample the dynamic regression coefficients of DGLMs when
the observation $(y_i | \psi_i)$ comes from an exponential family; but their
method is only approximate.  Much effort has been devoted to developing exact
posterior samplers, though none has proved to be completely satisfactory.  A
primary goal of any such sampler is to sample states jointly, like the filter
forward backwards sampler (FFBS) of \cite{fruhwirth-schnatter-1994} and
\cite{carter-kohn-1994}, since jointly sampling states tends to result in less
autocorrelation than sampling the states component-wise, a technique suggested
by \cite{carlin-etal-1992} prior to the advent of the advent of the FFBS.
However, the FFBS procedure requires Gaussian, linear state-space evolution
equations and observation equations.  Absent these assumptions, as is the case
with exponential families, the machinery of the FFBS breaks down.  To resurrect
the FFBS one may approximate the posterior using a dynamic linear model (DLM)
and then accept or reject using Metropolis-Hastings. or one may use data
augmentation so that, conditinoally, the observations and states are generated
by a DLM, though neither method is gaurenteed to work well.

\cite{gamerman-1998} discusses various Metropolis-Hastings based approaches, all
of which rely on a Laplace approximation, that is iteratively reweighted least
squares \citep{wedderburn-1974}, for generating proposals. Component-wise
proposals have decent acceptance rates, though high autocorrelation between
consecutive samples.  Joint proposals suffer from unacceptable small acceptance
rates.  Gamerman's solution is to transform the problem so that one samples the
innovation variances componenent-wise using a Laplace approximation and
Metropolis-Hastings, arguing that the new coordinate system possesses less
intrincinc correlation.  But this approach is more computationally intensize
since one must transform the proposals back to the original coordinate system at
each iteration to evaluate Metropolis-Hastings acceptance probability.

\cite{shephard-pitt-1997} attempt to strike a balance between the
autocorrelation of consecutive samples and the acceptance probabilities of
proposed samples by drawing blocks of states.  Sampling in blocks reduces
autocorrelation; restraining the size of the blocks ensures a reasonable
Metropolis-Hastings acceptance probability.  However, their method suffers too
since it cannot take a joint draw of the hidden states.

More recently, techniques have emerged that do generate joint draws of the
states.  \cite{ravines-etal-2006} built upon \cite{west-etal-1985} by adding a
Metropolis-Hastings step to sample the states exactly.  Though this at first
would seem like a poor choice due to high-dimensionality encountered in time
series, they find that, in fact, the technique results in reasonable acceptance
rates unlike a global Lapalce approximation.  The fact that conjugate update
improves the Metropoli-Hastings proposal enough to allow for ``efficient'' joint
draw is somewhat surprising.

Fr\"{u}wirth-Schnatter and her colleagues have explored data augmentation
techniques that rely upon discrete mixtures of normals to arrive at
conditionally Gaussian posteriors in a variety of settings including binomial
logistic regression and multinomial regression
\citep{fruhwirth-schnatter-fruhwirth-2007, fruhwirth-schnatter-fruhwirth-2010,
  fussl-etal-2013}, Poisson regression \citep{fruhwirth-schnatter-wagner-2006,
  fruhwirth-schnatter-etal-2009}, and negative binomial regression for count
data \citep{fruhwirth-schnatter-etal-2009}.  All of these techniques may be used
in static and dynamic regressions.  While these methods work well, several rely
upon precomputing large tables of weights, means, and variances for the
components of a collection normal mixtures that approximates an entire family of
distributions.  Further all of the discrete mixture of normal techniques make
use of at least three layers of auxiliary variables.  One would prefer to avoid
many layers of latents since this inhibits traversing the posterior landscape.

Posterior inference for binomial logistic regression in
\cite{fruhwirth-schnatter-fruhwirth-2007} is based upon the latent utilities
interpretation of \cite{mcfadden-1974}.  Later, in a discrete mixture of normals
approach based upon \cite{holmes-held-2006} is found to be superior
\citep{fruhwirth-schnatter-fruhwirth-2010}.  This approximation outperforms
\cite{holmes-held-2006} as well since due to its computation efficiency.  The
2010 paper includes an extensive comparison of various data augmentation and
Metropolis-Hastings methods in the static case.  Most recently,
\cite{fussl-etal-2013} put forth another generation of posterior samplers for
binomial logistic regression using discrete mixtures of normals that outperforms
its ancestors.  Much of this work can be traced back the
\cite{albert-chib-1993}, which provides a data augmentation approach to probit
regression and which has a dynamic analog.  We limit our comparison below to
\cite{fussl-etal-2013} since it appears to be the best choice for dynamic
binomial logistic regression within the Fr\"{u}wirth-Schnatter school.

DGLMs can be cast within the more general framework of non-linear non-Gaussian
state-space models.  \cite{geweke-tanizaki-2001} highlight the various works of
Kitigawa, Tanizaki, and Mariano, amongst others, and to filter, smooth, or
simulate states within this context using numerical integration, resampling, or
rejection sampling.  However, the more general setting does not provide more
insight into how one may jointly samples states in DGLMs.  Each of the
approaches they review is flawed: numerical integration does not work well for
any but the simplest settings, sampling marginally smoothed states using
sequential methods is time consuming, and rejection sampling may have poor
acceptance probabilities.  None of the methods cited by Geweke and Tanizaki are
useful for generating posterior samples of the states jointly, an extremely
desirable property.  Their solution is to sample the states component-wise using
a Laplace approximation and a Metropolis-Hastings step, which in the case of
exponential families returns us to the methods dicussed by
\citep{gamerman-1998}.  \cite{godsill-etal-2004} show how one may jointly sample
states using particle filters, a potentially competitive approach that we do not
consider here.

\section{\Polya-Gamma Data Augmentation}

The \Polya-Gamma data augmentation technique has a single layer of latent
variables that yields conditionally Gaussian posterior distributions when
working with logistic likelihoods.  The two common models considered here are
the binomial logistic model with identical number of trials, whose measurement
density is
\[
p(y_i | p_i) \propto p_i^{y_i} (1-p_i)^{n - y_i},
\]
and negative binomial models for count data, whose measurement density is \(
p(y_i | p_i) \propto p_i^{y_i} (1-p_i)^{d}.  \) In either case, one may use the
log-odds scale, $\psi_i = \log \frac{p_i}{1-p_i}$, is used for regression.
Under this coordinate system, the measurement density of the binomial
observations is
\[
p(y_i | \psi_i) \propto \frac{(e^{\psi_i})^{y_i}}{(1+e^{\psi_i})^{n_i}}.
\]
A similar form emerges in the case of negative binomial observations.  In
general, DGLMs with logistic likelihoods have densities of the form
\[
p(y, \bbeta) = h(y) \Big[ \prod_{i=1}^n
\frac{(e^{\psi_i})^{a_i}}{(1+e^{\psi_i})^{b_i}} \bbI \{\psi_i = x_i \beta_i\}
\Big] p(\bbeta)
\]
where $a_i$ and $b_i$ may be functions of $y$ or $\theta$, $\theta$ a set of
parameters that we suppress from the notation above and below; $\bbeta =
\{\beta_t\}$; and $p(\bbeta)$ governs the dynamics of $\bbeta$.  For instance,
$p(\bbeta)$ might be the density of an VAR(1) process.  As shown in
\cite{polson-etal-2012}, if $p(\bbeta)$ is a Gaussian ``prior'' and if one
augments this density with
\begin{equation}
\label{eqn:augmentation}
p(\omega | y, \bbeta) = \prod_{i=1}^n \Big[ p(\omega_i | b_i, \psi_i)
\Big]
\end{equation}
where $\omega_i \sim \PG(b_i, \psi_i)$, then the complete conditional posterior
distribution for $\bbeta$ will be Gaussian.  In particular, $p(\omega_i | b_i,
\psi_i) \propto \cosh^{b_i}(\psi_i/2) e^{-\omega_i \psi_i^2 / 2} p(\omega_i |
b_i, 0)$ so that
% \[
% p(\beta | y, \omega) \propto 
% \Big[ \prod_{i=1}^n e^{\kappa_i \psi_i - \frac{\omega_i}{2} \psi_i^2}
% \bbI \{\psi_i = x_i' \beta\} \Big] p(\beta)
% \]
% where $\kappa_i = a_i - b_i / 2$.  A single term from the product has
% \[
% \exp \Big( \kappa_i \psi_i - \frac{\omega_i}{2} \psi_i^2 \Big)
% \propto
% \exp \Big(-\frac{1}{2} (\kappa_i / \omega_i - \psi_i)^2 \omega_i \Big),
% \]
% which is identical to the likelihood of an observation $z_i = \kappa_i /
% \omega_i$ drawn from
% \[
% z_i \sim N(x_i' \beta, \omega_{i}^{-1}).
% \]
\[
p(\bbeta | y, \omega) 
\propto 
\Big[ \prod_{i=1}^n \exp 
\Big(-\frac{\omega_i}{2} \Big(\frac{\kappa_i}{\omega_i} - \psi_i\Big)^2 \Big)
\bbI \{\psi_i = x_i' \beta\} \Big] p(\bbeta)
\]
where $\kappa_i = a_i - b_i / 2$.  A single term from the product is identical
to the likelihood of a pseudo-data point $z_i = \kappa_i / \omega_i$ drawn from
$z_i \sim N(\psi_i, 1/\omega_i)$.  Thus, if $p(\bbeta)$ specifies that $\bbeta$
is a Gaussian VAR(1) process, then sampling complete conditional for $\bbeta$ is
equivalent to sampling $\bbeta$ from the DLM
\[
\begin{cases}
z_t = \psi_t + \nu_t, & \nu_t \sim N(0, 1/\omega_t) \\
\psi_t = x_t \beta_t \\
\beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]

\begin{example}
Suppose one observes a binomial outcome $y_t$ where $P(y_t = 1) = p_t$ out of
$n_t$ trials at time $t$.  Letting $\psi_t$ be the log-odds, the data generating
distribution is
\[
p(y_t | \psi_t) = c(y_t) \frac{(e^\psi_t)^{y_t}}{(1+e^{\psi_t})^{n_t}}.
\]
Thus the complete conditional $(\bbeta | y, \omega)$ may be simulated by using
filter forward backward sampling with pseudo-data $z_t = \kappa_t / \omega_t$
where $\kappa_t = y_t - n_t / 2$.
\end{example}


\begin{example}
Suppose that one observes counts according to $y_t \sim \NB(d, p_t)$ where $d$
is the number of ``failures'' before observing $y_t$ successes and $p_t$ is the
probability of observing a success.  Letting $\psi_t$ be the log-odds, the data
generating distribution is
\[
p(y_t | \psi_t) = c(y_t, d) \frac{(e^{\psi_t})^{y_t}}{(1+e^{\psi_t})^{y_t+d}}.
\]
In negative binomial regression, it is common to model the log-mean, $\zeta_t =
\psi_t + \log(d) = x_t \beta_t$, instead of the log-odds.  This requires only a
slight modification.  Following the work above, the complete conditional
$(\omega_t | \beta_t, d)$ is $\PG(b_t, \psi_t)$ where $b_t = y_t + d$.
However, the DLM used to estimate $\bbeta$ is now
% \[
% \begin{cases}
% z_t = \psi_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
% \psi_t = x_t \beta_t - \log(d) \\
% \beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W)
% \end{cases}
% \]
% where $z_t = \kappa_t / \omega_t$ and $\kappa_t = (y_t - d_t) / 2$.
\[
\begin{cases}
z_t = \zeta_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
\zeta_t = x_t \beta_t \\
\beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W)
\end{cases}
\]
where $z_t = \kappa_t / \omega_t + \log(d)$ and $\kappa_t = (y_t - d) / 2$.
\end{example}

For either model, posterior samples may be generated via Gibbs sampling.  One
can estimate the parameters of the VAR(1) process using standard conjugate
updating procedures and the complete conditional $(\omega | \bbeta, y)$, is
identical to (\ref{eqn:augmentation}).  \cite{polson-etal-2012} describe how to
sample from $\PG$ distributions and implement this sampler in the \texttt{R}
package \texttt{BayesLogit}.  To estimate $d$ in dynamic negative binomial
regression one may use a Metropolis-Hastings step (Appendix \ref{sec:sample-d}).

\subsection{AR(1) Example}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{nb-ar1-tophalf.pdf}
\caption{\label{fig:nb-ar1} Incidence of influenza-like illness in Texas,
  2008--11, together with the estimated mean $\lambda_t$ from the
  negative-binomial AR(1) model.  The blanks in weeks 21-41 correspond to
  missing data.  The grey lines depict the upper and lower bounds of a 95$\%$
  predictive interval.}
\end{center}
\end{figure}

As an initial illustration of the point, we fit a negative-binomial AR(1) model
to four years (2008--11) of weekly data on flu incidence in Texas, collected
from the Texas Department of State Health Services.  Let $y_t$ denote the number
of reported cases of influenza-like illness (ILI) in week $t$.  We assume that
these counts follow a negative-binomial model, which will allow overdispersion
relative to the Poisson:
\begin{eqnarray*}
y_t &\sim& \mbox{NB}(h, p_t) \; , \quad p_t= \frac{e^{\psi_t}}{1+e^{\psi_t}} \\
\psi_t &=& \alpha + \gamma_t  \; , \quad \gamma_t = \phi \gamma_{t-1} + \epsilon_t \\
\epsilon_t &\sim& N(0, \sigma^2) \, .
\end{eqnarray*}
Figure \ref{fig:nb-ar1} shows the results of the fit.  It is trivial to sample
from this model by combining the \Polya-Gamma framework together with standard
results on AR(1) models.  In our analysis, we assumed an improper uniform prior
on the dispersion parameter $h$, and fixed $\phi$ and $\sigma^2$ to $0.98$ and
1, respectively.  But it would be equally straightforward to place hyper-priors
upon each parameter, and to sample them in a hierarchical fashion.  It would
also be straightforward to incorporate fixed effects in the form of regressors.

We emphasize that there are many ways to handle the overdispersion present in
this data set, and that we do not intend our model to be taken as a definitive
analysis.  We merely intend it as a proof of concept, showing how various
aspects of Bayesian hierarchical modeling---in this case, a simple AR(1)
model---can be combined routinely with binomial likelihoods using the
\Polya-Gamma scheme.

\section{Comparison}

We compare methods by median effective sample size (ESS) and median effective
sampling rate (ESR).  The effective sample size is an of estimate the number of
independent samples that would need to be generated to produce a population of
equivalent quality.  The effective sampling rate is the ratio of effective
sample size to the time taken to generate samples post burn-in.  Thus the time
used to normalize the effective sample size does not include burn-in or any
preliminary computations.

For each data set, 10 simulations of 12,000 samples, the first 2,000 being
discarded as burn-in, are run.  For a single simulation the component-wise
effective sample size of each element of $\bbeta$ is calculated.  These
component-wise values are then averaged over the 10 runs.  The minimum, median,
and maximum average (along simulations) effective sample sizes are reported in
the tables herein.  Similarly, for each run a component wise effective sampling
rate is calculated.  The minimum, median, and maximum average (along
simulations) effective sampling rate is our primary measure of comparison
because it measures how quickly each method generates independent samples.

In terms of effective sampling rate, the \Polya-Gamma technique beats both
\cite{ravines-etal-2006} and \cite{fussl-etal-2013} for dynamic negative
binomial regression.  The method of \cite{fruhwirth-schnatter-etal-2009} works
well for dyanmic negative binomial regression when there are moderate to large
count sizes.  This is due to the current inefficiency in generating random
variates from $\PG(n, \psi)$ when $n$ is large.  In terms of effective sampling
size, the \Polya-Gamma technique is superior to the other methods in both
settings.  Thus, if Polson et al. improve the speed of their sampler it will
likely surpass \cite{fussl-etal-2013} in terms of ESR for negative binomial
regression.

\section{Conclusion}

\appendix

\section{Sampling $d$ in dynamic negative binomial regression}
\label{sec:sample-d}

We may sample $d$ by an indepedent Metropolis-Hastings or a random-walk
Metropolis-Hastings.  In either case, we need the log-likelihood of $d$.

The conditional density for a single term $y_t \sim N(d, p_t)$ is
\[
\frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} p_t^{y_t} (1-p_t)^{d} ,
\]
or 
\[
\frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} 
\Big( \frac{\mu_t}{\mu_t + d} \Big)^{y_t} \Big( \frac{d}{\mu_t + d} \Big)^d
\]
in terms of the mean $\mu_t$.  Since we chose to model $\log \mu_t = x_t
\beta_t$ it will be easier to work with the latter version.  In that case, the
log-likelihood is
\[
\ell(d|\mu, y) = \sum_{t=1}^T [ \log \Gamma(y_t + d)  - \log \Gamma(d) ]  + 
\sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
\]

If we assume that $d$ is a natural number then we can rid ourselves of $T$
evaluations of the gamma function.  We may expand $\Gamma(y_t + d)$ and
$\Gamma(d)$ as products, in which case the log-likelihood is
\[
\ell(d|\mu, y) = \sum_{t=1}^T \sum_{j=0}^{y_t-1} \log(d + j) + 
\sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
\]
We can rewrite the first term of the sum as
\[
\sum_{t=1}^T \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{k-1} \log(d+j) \one \{y_t = k\}
\]
which becomes
\[
\sum_{k=1}^{\max(y_t)} n_k \sum_{j=0}^{k-1} \log(d+j)
\]
where $n_k = \{ \# y_t = k \}$.  We have thus
\begin{align*}
\sum_{k=1}^{\max(y_t)} \sum_{j=0}^{\max(y_t)-1} n_k \log(d+j) \one\{j < k\}
& = 
\sum_{j=0}^{\max(y_t)-1} \log(d+j) \sum_{k=j+1}^{\max(y_t)} n_k \\
& = \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j
\end{align*}
where $G_j = \{ \# y_t > j \}$.  Notice that $1-G_j = F(j) = \{\# y_t \leq j\}$
and that $G$ may be pre-processed.  Hence the log-likelihood is
\[
\sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j + 
\sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
\]
Preprocessing $G$ saves us from having to compute a doubly indexed summation.

\appendix

\section{Benchmarks}

\begin{table}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Tables/table.tokyo} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    21.72 &     1.00 &  4352.80 &  7735.08 & 10081.46 &   200.43 &   356.18 &   464.22 \\ 
            dRUM  &    20.60 &     1.00 &  1627.61 &  3649.46 &  5428.93 &    79.03 &   177.19 &   263.60 \\ 
            CUBS  &   233.98 &     0.47 &   652.86 &   761.98 &   910.62 &     2.79 &     3.26 &     3.89
 \end{tabular}


\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynlogit-low-2-n-1-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    28.34 &     1.00 &  8309.00 &  9395.80 &  9894.45 &   293.16 &   331.50 &   349.09 \\ 
            dRUM  &    29.08 &     1.00 &  5299.68 &  7646.07 &  9800.60 &   182.26 &   262.96 &   337.05 \\ 
            CUBS  &   609.04 &     0.68 &  2424.95 &  3930.01 &  5168.38 &     3.98 &     6.45 &     8.49
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynlogit-high-2-n-1-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    28.46 &     1.00 &  8611.53 &  9441.44 &  9894.56 &   302.60 &   331.76 &   347.68 \\ 
            dRUM  &    29.19 &     1.00 &  6204.70 &  7879.60 &  9656.77 &   212.54 &   269.91 &   330.78 \\ 
            CUBS  &   610.43 &     0.61 &  2562.38 &  4291.96 &  6932.42 &     4.20 &     7.03 &    11.36
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynlogit-low-2-n-10-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    49.77 &     1.00 &  7116.22 &  9374.76 &  9911.13 &   142.98 &   188.36 &   199.13 \\ 
            dRUM  &    28.98 &     1.00 &  3540.13 &  7482.71 &  9884.88 &   122.14 &   258.16 &   341.04 \\ 
            CUBS  &   638.99 &     0.60 &  1233.54 &  3317.85 &  5278.43 &     1.93 &     5.19 &     8.26
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynlogit-high-2-n-10-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    50.17 &     1.00 &  7327.10 &  9211.07 &  9826.71 &   146.05 &   183.61 &   195.88 \\ 
            dRUM  &    28.97 &     1.00 &  3939.55 &  6967.89 &  9766.74 &   135.99 &   240.52 &   337.14 \\ 
            CUBS  &   643.20 &     0.52 &  2370.69 &  4240.41 &  5878.98 &     3.69 &     6.59 &     9.14
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynlogit-low-2-n-1-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    31.19 &     1.00 &  1110.73 &  7062.17 &  9930.51 &    35.61 &   226.43 &   318.38 \\ 
            dRUM  &    32.01 &     1.00 &   780.11 &  6088.64 &  9712.44 &    24.37 &   190.20 &   303.39 \\ 
            CUBS  &   652.12 &     0.79 &   570.30 &  1873.22 &  4299.12 &     0.87 &     2.87 &     6.59
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynlogit-high-2-n-1-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    31.25 &     1.00 &   450.30 &  4953.18 &  9255.15 &    14.41 &   158.50 &   296.14 \\ 
            dRUM  &    32.05 &     1.00 &   396.48 &  4451.72 &  8994.45 &    12.37 &   138.90 &   280.64 \\ 
            CUBS  &   647.37 &     0.75 &   300.94 &  1587.59 &  4674.42 &     0.47 &     2.46 &     7.22
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynlogit-low-2-n-10-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    52.54 &     1.00 &   626.75 &  5161.88 &  9855.10 &    11.93 &    98.24 &   187.56 \\ 
            dRUM  &    31.84 &     1.00 &   674.18 &  3477.35 &  9119.24 &    21.17 &   109.21 &   286.40 \\ 
            CUBS  &   661.84 &     0.56 &   364.16 &  1443.76 &  3065.32 &     0.55 &     2.18 &     4.63
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynlogit-high-2-n-10-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    52.99 &     1.00 &   174.62 &  1001.65 &  8922.27 &     3.29 &    18.91 &   168.37 \\ 
            dRUM  &    31.79 &     1.00 &   151.89 &   867.68 &  7060.28 &     4.78 &    27.30 &   222.06 \\ 
            CUBS  &   651.22 &     0.47 &   192.77 &   772.78 &  2680.11 &     0.30 &     1.19 &     4.12
 \end{tabular}
\end{table}

\newpage

\begin{table}
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-low-2-mu-10-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    63.86 &     1.00 &  8305.72 &  9668.71 & 10000.00 &   130.07 &   151.41 &   156.60 \\ 
              FS  &    29.70 &     1.00 &  1755.48 &  6508.09 &  9870.64 &    59.11 &   219.11 &   332.32 \\ 
            CUBS  &   648.25 &     0.60 &   384.08 &  3407.23 &  5894.51 &     0.59 &     5.26 &     9.09
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-high-2-mu-10-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    59.63 &     1.00 &  8967.77 &  9684.87 & 10000.00 &   150.39 &   162.42 &   167.70 \\ 
              FS  &    29.65 &     1.00 &  2844.82 &  5812.03 &  9773.66 &    95.95 &   196.02 &   329.64 \\ 
            CUBS  &   656.19 &     0.74 &  4605.04 &  7412.28 &  8063.25 &     7.02 &    11.30 &    12.29
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-low-2-mu-100-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &   393.08 &     1.00 &  1898.18 &  7298.22 &  9990.29 &     4.83 &    18.57 &    25.42 \\ 
              FS  &    29.68 &     1.00 &  2386.51 &  7229.31 & 10000.00 &    80.40 &   243.58 &   336.93 \\ 
            CUBS  &   661.62 &     0.37 &   198.71 &  1150.20 &  2867.16 &     0.30 &     1.74 &     4.33
 \end{tabular}


\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-high-2-mu-100-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &   292.81 &     1.00 &  4090.32 &  7283.70 &  9697.62 &    13.97 &    24.87 &    33.12 \\ 
              FS  &    29.65 &     1.00 &  3417.13 &  6864.88 &  9566.54 &   115.26 &   231.58 &   322.68 \\ 
            CUBS  &   660.24 &     0.39 &  1065.38 &  3124.58 &  4242.90 &     1.61 &     4.73 &     6.43
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-low-2-mu-10-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    66.80 &     1.00 &   149.03 &  4880.10 &  9833.67 &     2.23 &    73.06 &   147.21 \\ 
              FS  &    32.65 &     1.00 &   149.47 &  1793.39 &  7952.70 &     4.58 &    54.92 &   243.58 \\ 
            CUBS  &   667.39 &     0.55 &   135.02 &  1249.47 &  2792.73 &     0.20 &     1.87 &     4.18
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-high-2-mu-10-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    62.28 &     1.00 &   244.92 &  2039.14 &  9767.92 &     3.93 &    32.73 &   156.83 \\ 
              FS  &    32.51 &     1.00 &   284.53 &  1219.91 &  5614.48 &     8.75 &    37.52 &   172.66 \\ 
            CUBS  &   669.68 &     0.62 &   363.77 &  1609.55 &  4586.74 &     0.54 &     2.40 &     6.85
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-low-2-mu-100-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &   395.37 &     1.00 &  1766.07 &  4793.62 &  9445.17 &     4.47 &    12.12 &    23.89 \\ 
              FS  &    32.61 &     1.00 &  2217.92 &  4814.16 &  9682.45 &    68.02 &   147.64 &   296.91 \\ 
            CUBS  &   670.26 &     0.32 &   347.70 &   783.63 &  1561.49 &     0.52 &     1.17 &     2.33
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-high-2-mu-100-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &   295.55 &     1.00 &  1170.90 &  3414.61 &  8363.79 &     3.96 &    11.55 &    28.30 \\ 
              FS  &    32.48 &     1.00 &  1139.88 &  3228.07 &  7966.98 &    35.10 &    99.39 &   245.29 \\ 
            CUBS  &   674.96 &     0.30 &   381.00 &   744.60 &  1115.24 &     0.56 &     1.10 &     1.65
 \end{tabular}



\end{table}



% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliography{bayeslogit}{}
\bibliographystyle{abbrvnat}

\end{document}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Tokyo Rainfall} \\
% \hline
%           Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
%             CUBS  &   233.98 &     1.00 &   652.86 &   761.98 &   910.62 &     2.79 &     3.26 &     3.89 \\ 
%               PG  &    21.72 &     1.00 &  4352.80 &  7735.08 & 10081.46 &   200.43 &   356.18 &   464.22 \\ 
%             dRUM  &    20.60 &     1.00 &  1627.61 &  3649.46 &  5428.93 &    79.03 &   177.19 &   263.60
%  \end{tabular}
% \end{table}

% \begin{table}
% \centering
% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta$.} \\
% \hline
%           Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%              PG  &    22.59 &     1.00 &   7367.68 &   9229.91 &  10189.37 &    326.10 &    408.52 &    450.97 \\ 
%            dRUM  &    23.19 &     1.00 &   2759.83 &   7260.46 &   9967.66 &    118.99 &    313.04 &    429.77
%          \end{tabular}

% \begin{tabular}{l r r r r r r r r }
% \hline
% \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta$.} \\
% \hline 
%           Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%              PG  &    22.52 &     1.00 &   8494.04 &   9662.06 &  10225.35 &    377.13 &    428.98 &    454.00 \\ 
%            dRUM  &    23.17 &     1.00 &   4963.68 &   8105.02 &   9936.46 &    214.26 &    349.86 &    428.91
% \end{tabular}

% \end{table}


% \begin{table}
% \centering
% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta, \phi, W$.} \\
% \hline
%           Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%             PG  &    25.04 &     1.00 &    643.23 &   6531.34 &  10420.88 &     25.69 &    260.82 &    416.13 \\ 
%           dRUM  &    25.73 &     1.00 &    430.79 &   4490.34 &  10124.47 &     16.74 &    174.55 &    393.48
%  \end{tabular}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta, \phi, W$.} \\
% \hline
%           Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%             PG  &    25.16 &     1.00 &    988.83 &   5654.23 &  10337.07 &     39.31 &    224.75 &    410.87 \\ 
%           dRUM  &    25.84 &     1.00 &   1040.12 &   5496.94 &   9760.64 &     40.25 &    212.74 &    377.75
%  \end{tabular}