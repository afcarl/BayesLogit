\documentclass[12pt]{article}

\input{commands}

\usepackage{setspace}
\usepackage{natbib}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\newcommand{\Polya}{P\'{o}lya}
\newcommand{\PG}{\text{PG}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Ga}{\text{Ga}}
\newcommand{\NB}{\text{NB}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\eqd}{\,{\buildrel d \over =}\,}
\newcommand{\KS}{\text{KS}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\oomega}{\boldsymbol{\omega}}
\newcommand{\yy}{\boldsymbol{y}}

\newcommand{\point}{\noindent $\bullet$ }

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
%\parindent 0in
%\parskip 12pt

% Set Fancy Style
%\pagestyle{fancy}

%\lhead{Left Header}
%\rhead{Right Header}

\title{\Polya-Gamma Data Augmentation for Dynamic Models}
\author{
Jesse Windle\footnote{jwindle@ices.utexas.edu} ,
Carlos M. Carvalho\footnote{carlos.carvalho@mccombs.utexas.edu},
James G. Scott\footnote{james.scott@mccombs.utexas.edu}, 
Liang Sun\footnote{sallylia@gmail.com}, 
\\
\textit{The Univeristy of Texas at Austin}
}

\begin{document}

\maketitle

\abstract{Dynamic linear models with Gaussian observations and Gaussian states
  lead to closed-form formulas for posterior simulation.  However, these
  closed-form formulas break down when the response or state evolution ceases to
  be Gaussian.  Dynamic, generalized linear models (DGLMs) exemplify a class of
  models for which this is the case, and include, amongst other models, dynamic
  binomial logistic regression and dynamic negative binomial regression.
  Finding and appraising posterior simulation techniques for these models is
  important since modeling temporally correlated categories or counts is useful
  in a variety of disciplines, including ecology, economics, epidemiology,
  medicine, and neuroscience.  In this paper, we present one such technique,
  \Polya-Gamma data augmentation, and compare it with the discrete mixture of
  normals approach of Fr\"uhwirth-Schnatter \cite{fruhwirth-schnatter-etal-2009}
  and \cite{fussl-etal-2013}, and the Metropolis-Hastings approach of
  \cite{ravines-etal-2006}.  We find that the \Polya-Gamma approach works well
  for dynamic logistic regression and for dynamic negative binomial regression
  when the count sizes are small.

  \smallskip \textbf{Keywords:} {binomial, dynamic generalized linear models,
    logistic regression, negative binomial regression} }

%\tableofcontents
%\newpage

\doublespace

\section{Introduction}

%The method presented herein is useful for generating posterior samples for
%state-space models with categorical or integral responses and linear, Gaussian
%evolution equations.

This paper presents a data augmentation technique that may be used to produce
posterior samples from state-space models with binomial or negative binomial
observations and linear, Gaussian evolution equations.  We begin with a brief
overview of the modeling scenario, starting with the static analog, building the
dynamic model, and then describing the challenges faced when generating
posterior samples.

A linear model enforces an affine relationship between the expectation of the
response, $\bbE[y_t]$, and the covariates, $x_t \in \bbR^P$ for $t=1, \ldots,
T$.  A generalized linear model (GLM) allows a non-linear relationship between
the expectation and a linear combination of the predictors, $\psi_t = x_t'
\beta$.  This flexibility is useful when modeling categories, counts,
proportions, and positive quantities \citep{mccullagh-nelder-1989}.  Strictly
speaking, one need not specify the distribution of the response, just its first
two moments, but since we are interested in Bayesian inference we will restrict
our attention to situations in which the likelihood, $f(y_t | \psi_t)$, is
specified, focusing on binomial likelihoods.  When the likelihood comes from the
exponential family of distributions, one may easily calculate the posterior mode
and associated error estimates \citep{wedderburn-1974}; however, most
likelihoods lead to posterior distributions which are difficult to sample,
requiring Metropolis-Hastings sampling, Gibbs sampling via data augmentation, or
some other MCMC method.

Dynamic generalized linear models (DGLMs) permit an evolving relationship
between the response and the covariates via time-varying regression
coefficients, $\{\beta_t\}_{t=1}^T$, so that the log odds at time $t$ is $\psi_t
= x_t' \beta_t$.  One must specify a prior for $\{\beta_t\}_{t=1}^T$ to complete
the model.  The prior controls how quickly $\{\beta_t\}_{t=1}^T$ may change.  It
is common to let $\{\beta_t\}_{t=1}^T$ be an AR(1) process or a random walk,
both with Gaussian innovations.  In either case, one recovers a state-space
model characterized by the observation distribution $f(y_t | \psi_t)$ for the
response and the Markovian evolution distribution $g(\beta_t | \beta_{t-1})$ for
the hidden states.

To clarify the challenge of simulating $(\{\beta_t\}_{t=1}^T | \{y_t\}_{t=1}^T)$
for dynamic generalized linear models, it is helpful to consider more and less
tractable state-space models.  State-space models with non-Gaussian, non-linear
responses and non-linear evolution equations are less tractable.  One may
efficiently approximate the filtered distributions of such models, $p(\beta_t |
\{y_s\}_{s=1}^t) d\beta_t$, using sequential methods, but the lack of structure
makes sampling from the posterior of $\{\beta_t\}_{t=1}^T$ difficult.  Dynamic
linear models (DLMs) with Gaussian states and Gaussian observations are more
tractable: one may sample the posterior distribution of the states using the
forward filter backward sample (FFBS) algorithm \citep{carter-kohn-1994,
  fruhwirth-schnatter-1994}, a procedure that is linear in the number of
observations and thus quite efficient.  However, the FFBS algorithm breaks down
outside of these vary narrow assumptions.  Dynamic generalized linear models
that have Gaussian, linear evolution equations, but non-Gaussian, non-linear
response distributions sit between these extremes.  Within this class, the FFBS
algorithm is not immediately applicable, but the linear evolution equations give
one hope of finding a clever way to reintroduce it.  Many approaches follow this
path, combining techniques used for generalized linear models with the FFBS
algorithm.  However, these amalgamations are not always as effective as their
constituent parts.  In particular, Metropolis-Hastings based techniques for GLMs
do not translate well to the dynamic setting, as $\{\beta_t\}_{t=1}^T$ is of
much higher dimension than $\beta$.  Data augmentation techniques for GLMs that
lead to a Gaussian complete conditional for $\beta$ provide a preferable
approach since the corresponding complete conditional for $\{\beta_t\}_{t=1}^T$
will coincide with the likelihood of a DLM and hence the FFBS algorithm.  But
such data augmentation tricks are difficult to find.  Recently,
\cite{polson-etal-2013} introduced a data augmentation trick for GLMs with
binomial likelihoods and that is what we exploit here.

This paper presents \Polya-Gamma data augmentation for dynamic regression models
with binomial likelihoods and compares it to a pair of other approaches:
conjugate updating and backward sampling with a Metroplis-Hastings step
\citep{ravines-etal-2006} and forward filter and backward sampling using data
augmentation and a discrete mixture of normals approximation
\citep{fruhwirth-schnatter-etal-2009, fussl-etal-2013}.  Our method is pleasing
because of its parsimony, ease of implementation, and efficiency.  

Dynamic models of counts or categories, like the ones studied herein, may arise
across a range of disciplines, such as ecology, epidemiology, neuroscience,
economics, and medicine.  Hence, having a guide to the most efficient posterior
simulation techniques for these models is important for practitioners.  To that
end, we benchmark two dynamic generalized linear models and show that the
\Polya-Gamma approach is the best choice for dynamic logistic regression and
dynamic negative binomial regression when the count size is small.

The outline of the paper is as follows: Section 2 reviews other methods of
inference for dynamic generalized linear models; Section 3 shows how one may use
the \Polya-Gamma data augmentation technique in the dynamic setting; Section 4
presents the benchmarking results; Section 5 concludes.

\section{Previous Efforts}
\label{sec:previous-efforts}

Bayesian inference for dynamic generalized linear models dates back to
at least \cite{west-etal-1985} who used conjugate updating with backwards
sampling (CUBS) to sample the dynamic regression coefficients of DGLMs when the
observation $(y_t | \psi_t)$ comes from an exponential family; but their method
is only approximate.  Much effort has been devoted to developing exact posterior
samplers, though none has proved to be completely satisfactory.  A primary goal
of any such sampler is to sample states jointly, like the forward filter
backwards sampling (FFBS) of \cite{fruhwirth-schnatter-1994} and
\cite{carter-kohn-1994}, since jointly sampling states usually results in less
autocorrelation than sampling the states component-wise, an approach suggested
by \cite{carlin-etal-1992} prior to the advent of the advent of the FFBS.
However, the FFBS procedure requires Gaussian, linear state-space evolution and
observation equations.  Outside of these assumptions, as is generally the case
with DGLMs, the machinery of the FFBS breaks down.  To resurrect the FFBS, one
may approximate the posterior with a DLM proposal density and then accept or
reject using Metropolis-Hastings, or one may use data augmentation so that,
conditionally, the observations and states are generated by a DLM.  Neither
method is guaranteed to work well.

\cite{gamerman-1998} discusses various Metropolis-Hastings based approaches, all
of which rely on the Laplace approximation \citep{robert-casella-2005-book}, and
all of which are flawed: component-wise proposals have decent acceptance rates,
though high autocorrelation between consecutive samples, while joint proposals
suffer from unacceptably small acceptance rates.  Gamerman's solution is to
transform the problem so that one samples the state disturbances
component-wise using a Laplace approximation with Metropolis-Hastings update,
arguing that the new coordinate system possesses less intrinsic correlation.
But this approach is more computationally intensive since one must transform the
proposals back to the original coordinate system at each iteration to evaluate
the Metropolis-Hastings acceptance probability.

\cite{shephard-pitt-1997} attempt to strike a balance between the
autocorrelation of consecutive samples and the acceptance probabilities of
proposed samples by drawing blocks of disturbances.  One can proceed similarly
by sampling blocks of states.  Sampling in larger blocks reduces Markov Chain
autocorrelation, while sampling smaller blocks encourages a reasonable
Metropolis-Hastings acceptance probability.  However, sampling in blocks can
still suffer from high autocorrelation between consecutive draws.  Sampling
blocks of the disturbances presents similar problems to those discussed above,
in particular, it is more computationally intensive than sampling blocks of
states directly.

More recently, techniques have emerged that generate joint draws of the entire
set of states.  \cite{ravines-etal-2006} built upon \cite{west-etal-1985} by
adding a Metropolis-Hastings step to sample the states exactly.  Though this at
first would seem like a poor choice due to the high-dimensionality often
encountered in time series, they find that, in fact, the technique results in
reasonable acceptance rates unlike a global Laplace approximation.  We verified
this assertion, implementing a Metropolis-Hastings scheme that sampled blocks of
disturbances using the Laplace approximation and finding that it did not fare as
well as the method of \cite{ravines-etal-2006}.
% The fact that conjugate
% update improves the Metropolis-Hastings proposal enough to allow for
% ``efficient'' joint draws is somewhat surprising.

Fr\"{u}hwirth-Schnatter and her colleagues have explored data augmentation
techniques that rely upon discrete mixtures of normals to arrive at
conditionally Gaussian posteriors in a variety of settings including binomial
logistic regression and multinomial regression
\citep{fruhwirth-schnatter-fruhwirth-2007, fruhwirth-schnatter-fruhwirth-2010,
  fussl-etal-2013}, Poisson regression \citep{fruhwirth-schnatter-wagner-2006,
  fruhwirth-schnatter-etal-2009}, and negative binomial regression for count
data \citep{fruhwirth-schnatter-etal-2009}.  All of these techniques may be used
in static and dynamic regressions.  While these methods work well, several rely
upon precomputing large tables of weights, means, and variances for the
components of a collection normal mixtures that approximate an entire family of
distributions.  Further all of the discrete mixture of normal techniques make
use of at least two layers of auxiliary variables.  One would prefer to avoid
many layers of latents since this may inhibit traversing the posterior landscape
and enlarges the memory footprint when storing the latent states.

Much of this work can be traced back to \cite{albert-chib-1993}, who develop a
data augmentation approach to probit regression, \cite{mcfadden-1974}, who
provides a data augmentation scheme for logistic regression, and
\cite{holmes-held-2006}, who produce another data augmentation scheme for
logistic regression.  For dynamic binomial logistic regression, we limit our
comparison to \cite{fussl-etal-2013} since it appears to be the best choice
within Fr\"{u}wirth-Schnatter et al.'s cornucopia of methods.

DGLMs can be cast within the more general framework of non-linear, non-Gaussian
state-space models.  \cite{geweke-tanizaki-2001} highlight the various works of
Kitagawa, Tanizaki, and Mariano, amongst others, for filtering, smoothing, or
simulating states using numerical integration, resampling, or rejection sampling
within this context.  However, the more general setting does not provide more
insight into how one may jointly samples states in DGLMs.  Each of the
approaches they review is unsatisfactory: numerical integration does not work
well for any but the simplest settings, sampling marginally smoothed states
using sequential methods is time consuming, and rejection sampling may have poor
acceptance probabilities.  None of the methods cited by Geweke and Tanizaki are
useful for generating posterior samples of the states jointly, an extremely
desirable property.  Their solution is to sample the states component-wise using
a Laplace approximation and a Metropolis-Hastings step, which in the case of
exponential families returns us to the methods discussed by
\citep{gamerman-1998}.  \cite{godsill-etal-2004} show how one may jointly sample
states using particle methods; however, joint sampling using particle methods is
far less efficient than generating filtered samples and is not considered
herein.

\section{\Polya-Gamma Data Augmentation}

We consider DGLMs with binomial likelihoods: \( f(y_t | p_t) = c(y_t) p_t^{a_t}
(1-p_t)^{d_t}, \) where $a_t$ and $d_t$ do not depend upon $p_t$.  Logistic
regression and negative binomial regression are two common models that fit
within this regime.  It is preferable to express this likelihood in terms of the
log odds, $\psi_t = \log \frac{p_t}{1-p_t}$, since this is the scale on which we
linearly model the covariates:
\[
f(y_t | \psi_t) = c(y_t) \, \frac{(e^{\psi_t})^{a_t}}{(1+e^{\psi_t})^{b_t}},
\]
where $b_t = a_t + d_t$.  Given a collection of observations $\yy =
\{y_t\}_{t=1}^T$, the posterior distribution a DGLM with a binomial likelihood
is
\[
p(\bbeta | \yy) = c(\yy) \Big[ \prod_{t=1}^T
\frac{(e^{\psi_t})^{a_t}}{(1+e^{\psi_t})^{b_t}}
 \Big] p(\bbeta),
\]
where $\psi_t = x_t' \beta_t$ and $\bbeta = \{\beta_t\}_{t=1}^T$.  Following
\cite{polson-etal-2013}, one may introduce \Polya-Gamma random variates $\oomega
= \{\omega_t\}_{t=1}^T$, $\omega_t \sim \PG(b_t, \psi_t)$ for $t=1, \ldots, T$
to construct the joint distribution 
\[
p(\bbeta, \oomega | \yy) = c(\yy) \Big[ \prod_{t=1}^T
\frac{(e^{\psi_t})^{a_t}}{(1+e^{\psi_t})^{b_t}} p(\omega_t | b_t, \psi_t)
\Big] p(\bbeta) \, .
\]
via the conditional structure $p(\bbeta, \oomega | \yy) = p(\oomega | \bbeta,
\yy) p(\bbeta | \yy)$.  The \Polya-Gamma density possesses the special form
\[
p(\omega_t | b_t, \psi_t) = \cosh^{-b_t}(\psi_t/2) e^{- \omega_t \psi_t^2 / 2}
p(\omega_t),
\]
so that the complete conditional of $\bbeta$ is
% \[
% p(\beta | y, \omega) \propto 
% \Big[ \prod_{t=1}^n e^{\kappa_t \psi_t - \frac{\omega_t}{2} \psi_t^2}
% \bbI \{\psi_t = x_t' \beta\} \Big] p(\beta)
% \]
% where $\kappa_t = a_t - b_t / 2$.  A single term from the product has
% \[
% \exp \Big( \kappa_t \psi_t - \frac{\omega_t}{2} \psi_t^2 \Big)
% \propto
% \exp \Big(-\frac{1}{2} (\kappa_t / \omega_t - \psi_t)^2 \omega_t \Big),
% \]
% which is identical to the likelihood of an observation $z_t = \kappa_t /
% \omega_t$ drawn from
% \[
% z_t \sim N(x_t' \beta, \omega_{i}^{-1}).
% \]
\[
p(\bbeta | \oomega, \yy) \propto \Big[ \prod_{t=1}^T \exp \Big(-\frac{\omega_t}{2}
\Big(\frac{\kappa_t}{\omega_t} - \psi_t\Big)^2 \Big) \Big] p(\bbeta), \; \psi_t
= x_t' \beta_t,
\]
where $\kappa_t = a_t - b_t / 2$.  A single term from the product above is
identical to the likelihood of a pseudo-data point $z_t = \kappa_t / \omega_t$
drawn from $z_t \sim N(\psi_t, 1/\omega_t)$.  Thus, if $p(\bbeta)$ specifies
that $\bbeta$ is a Gaussian AR(1) process, then sampling the complete
conditional for $\bbeta$ is equivalent to sampling $(\bbeta | \{z_t\}_{t=1}^T)$
from the DLM
\[
\begin{cases}
z_t = \psi_t + \nu_t, & \nu_t \sim N(0, 1/\omega_t) \\
\psi_t = x_t' \beta_t \\
\beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]

Collecting these two complete conditionals, one finds that posterior samples may
be generated via Gibbs sampling: the states $(\bbeta|\oomega, \yy)$ via the FFBS
algorithm and $(\oomega | \bbeta, \yy)$ by taking independent samples $\omega_t
\sim \PG(b_t, \psi_t)$ for $t=1, \ldots, T$.  \cite{polson-etal-2013} describe
how to sample from $\PG$ distributions and implement this sampler in the
\texttt{R} package \texttt{BayesLogit}.  Sampling any hyperparameters, like the
autocorrelation coefficient or the innovation variance of the AR(1) process,
follows using standard conjugate or MCMC techniques.

\begin{example}
  Suppose one observes a binomial outcome $y_t \sim \text{Binom}(n_t, p_t)$ at
  time $t$.  Letting $\psi_t$ be the log-odds, the data generating distribution
  is
\[
p(y_t | \psi_t) = c(y_t) \frac{(e^\psi_t)^{y_t}}{(1+e^{\psi_t})^{n_t}}.
\]
Thus the complete conditional $(\bbeta | y, \omega)$ may be simulated by using
forward filter backward sampling with pseudo-data $z_t = \kappa_t / \omega_t$
where $\kappa_t = y_t - n_t / 2$.
\end{example}


\begin{example}
  Suppose that one observes counts according to $y_t \sim \NB(d, p_t)$ where $d$
  is the number of failures before observing $y_t$ successes, or dispersion,
  and $p_t$ is the probability of observing a success.  Letting $\psi_t$ be the
  log-odds, the data generating distribution is
\[
p(y_t | \psi_t) = c(y_t, d) \frac{(e^{\psi_t})^{y_t}}{(1+e^{\psi_t})^{y_t+d}}.
\]
In negative binomial regression, it is common to model the log-mean, $\lambda_t
= \psi_t + \log(d) = x_t \beta_t$, instead of the log-odds.  This requires only
a slight modification.  Following the work above, the complete conditional
$(\omega_t | \beta_t, d)$ is $\PG(b_t, \psi_t)$ where $b_t = y_t + d$.  However,
the DLM used to estimate $\bbeta$ is now
% \[
% \begin{cases}
% z_t = \psi_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
% \psi_t = x_t \beta_t - \log(d) \\
% \beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W)
% \end{cases}
% \]
% where $z_t = \kappa_t / \omega_t$ and $\kappa_t = (y_t - d_t) / 2$.
\[
\begin{cases}
z_t = \lambda_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
\lambda_t = x_t' \beta_t \\
\beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W)
\end{cases}
\]
where $z_t = \kappa_t / \omega_t + \log(d)$ and $\kappa_t = (y_t - d) / 2$.
\end{example}

% To estimate $d$ in dynamic negative binomial regression one may use a
% Metropolis-Hastings step (Appendix \ref{sec:sample-d}).

%\subsection{AR(1) Example}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{nb-ar1-tophalf.pdf}
\caption{\label{fig:nb-ar1} Incidence of influenza-like illness in Texas,
  2008--11, together with the estimated mean from the negative-binomial AR(1)
  model.  The blanks in weeks 21-41 correspond to missing data.  The grey lines
  depict the upper and lower bounds of a 95$\%$ predictive interval.}
\end{center}
\end{figure}

As an initial illustration, we fit a negative-binomial AR(1) model to four years
(2008--11) of weekly data on flu incidence in Texas, collected from the Texas
Department of State Health Services.  Let $y_t$ denote the number of reported
cases of influenza-like illness in week $t$.  We assume that these counts follow
a negative-binomial model, which will allow over-dispersion relative to the
Poisson.
% \begin{eqnarray*}
% % y_t &\sim& \mbox{NB}(h, p_t) \; , \quad p_t= \frac{e^{\psi_t}}{1+e^{\psi_t}} \\
% \psi_t &=& \alpha + \gamma_t  \; , \quad \gamma_t = \phi \gamma_{t-1} + \epsilon_t \\
% \epsilon_t &\sim& N(0, \sigma^2) \, .
% \end{eqnarray*}
Figure \ref{fig:nb-ar1} shows the results of the fit. For simplicity, we assumed
an improper uniform prior on the dispersion parameter $d$, and fixed $\phi$ and
$\sigma^2$ to $0.98$ and 1, respectively, but it is straightforward to place
hyper-priors upon each parameter, and to sample them in a hierarchical fashion.
It is also straightforward to incorporate fixed effects in the form of
regressors.

% We emphasize that there are many ways to handle the over-dispersion present in
% this data set, and that we do not intend our model to be taken as a definitive
% analysis.  We merely intend it as a proof of concept, showing how various
% aspects of Bayesian hierarchical modeling---in this case, a simple AR(1)
% model---can be combined routinely with binomial likelihoods using the
% \Polya-Gamma scheme.

\section{Comparison}

% Dynamic versions of binomial logistic regression and negative binomial
% regression are common models.  Hence, it is important to understand which
% methods work best for each model.

Since Markov Chains generate correlated samples we compare methods by measuring
how fast each procedure produces \emph{nearly} independent samples, that is we
measure the effective sampling rate (ESR).  To that end, we employ the effective
sample size (ESS), which approximates the number of ``independent'' draws
produced by a sample of $M$ correlated draws.  One may view the ESS as an
estimate of the number of samples produced after having thinned the $M$
correlated samples so that remaining draws appear independent.  From
\cite{holmes-held-2006}, the effective sample size is
\[
ESS_{it} = M / \Big( 1 + 2 \sum_{k=1}^\ell \rho_k(\beta_{it}) \Big)
\]
where $\rho_k(\beta_{it})$ is the $k$th lagged autocorrelation of the chain
corresponding to the $i$th component of $\beta_t$.  The effective
sampling rate is the ratio of the effective sample size to the time taken to
generate the post-burn-in samples; thus, it measures the rate at which the
Markov Chain produces independent draws after initialization and burn-in.

To mitigate MCMC sample variation, 10 batches of 12,000 samples are taken and
the last 10,000 draws are recorded.  For batch $m$, we compute the
component-wise effective sample size $ESS_{it}^{m}$ corresponding to the
univariate chain for $\beta_{it}$.  Taking the mean over batches produces the
average, component-wise effective sample size $\overline{ESS}_{it}$ and,
normalizing by time, the average, component-wise effective sampling rate
$\overline{ESR}_{it}$.  Following, \cite{fruhwirth-schnatter-fruhwirth-2010},
the primary metric of comparison is the median effective sampling rate,
\[
\text{med} \; \Big\{ \overline{ESR}_{it} : i=1, \ldots, P; \;  t=1, \ldots, T \Big\}.
\]

We consider synthetic data sets with a variety of characteristics.
% since this is an empirical, as opposed to theoretical assessment.  
For dynamic binomial logistic regression, we consider log odds of the form
$\alpha + x_t' \beta_t$ where $\alpha$ is a scalar and $\{\beta_t\}_{t=1}^T$ is
a 2-dimensional AR(1) process with autocorrelation $\Phi = 0.95 I_2$.  Four
different synthetic data sets are constructed, allowing for low and high
correlation amongst $\{x_t\}_{t=1}^T$ and responses, $y_t \sim \text{Binom}(n,
p_t), t=1, \ldots, T$, with either $n=1$ or $n=20$ trials.  The setup is almost
identical for dynamic negative binomial regression except that we model the
log-mean as $\alpha + x_t' \beta_t$ and consider responses with $\alpha =
\log(10)$ or $\alpha = \log(100)$ corresponding to average count sizes of
roughly 10 or 100.  Further details may be found in Appendix
\ref{sec:benchmark-details}.

Some caution is warranted when comparing methods as the effective sampling rate
is sensitive to a procedure's implementation and the hardware on which it is
run. The routines are written primarily in R.  We use code from
\cite{binomlogit-2012} and \cite{fruhwirth-schnatter-book-2007} for the discrete
mixture of normals methods.  All benchmarks are carried out on an Ubuntu machine
with Intel Core i5-3570 3.4GHz processor and 8GB of RAM.  Some computations were
burdensome in R, and hence we wrote wrappers to C to speed up the MCMC
simulations.  In particular, both data augmentation methods implement forward
filtering and backward sampling using a C wrapper.  The \Polya-Gamma technique
calls C code to sample random variates using version 0.2-4 of the
\texttt{BayesLogit} package \citep{bayeslogit-2013}.  The conjugate updating and
backwards sampling of \cite{ravines-etal-2006} is done in C.  Having a C wrapper
to forward filter and backwards sample is particularly important, as our C
implementation is much faster than the corresponding R code.  Were we to use the
slower R version, our results would favor the \Polya-Gamma method, as it has
better effective sample sizes and would spend less time, proportionally,
sampling the latent random variates.

\cite{polson-etal-2013} outline the expected performance of the \Polya-Gamma
data augmentation technique, which depends heavily on how quickly one can
generate \Polya-Gamma random variates.  In their original algorithm, sampling
\Polya-Gamma random variates from $\PG(b,\psi)$ is fast when $b$ is a small
integer, but slow when $b$ is large. \cite{windle-thesis-2013} provides an
improved method for sampling $\PG(b,\psi)$; however sampling large $b$ is still
slower than sampling small $b$.  These differences in computational cost are
important, as one must sample $\omega_t \sim \PG(b_t, \psi_t), t=1,\ldots,T$ for
each MCMC iteration under \Polya-Gamma data augmentation.  In binomial logistic
regression, $b_t = n_t$ where $n_t$ is the number of trials at each response
$y_t$.  Hence, when $n_t$ is small the PG method will do well.  In negative
binomial regression, $b_t = y_t + d_t$ where $y_t$ is a response and $d_t$ is
the dispersion.  Thus, larger average responses should lead to longer MCMC
simulations.

In general, we find these principles to hold.  The \Polya-Gamma data
augmentation technique performs well for dynamic binomial logistic regression
when the number of trials of the response is small, showing a 25\% better ESR
for binary logistic regression than \cite{fussl-etal-2013}; however,
\cite{fussl-etal-2013} does better when the number of trials is large.
Similarly, the \Polya-Gamma technique outpaces
\cite{fruhwirth-schnatter-etal-2009} in negative binomial regression when the
average number of counts is small, but loses when the average number of counts
is large.  The Metropolis-Hastings approach of \cite{ravines-etal-2006} does the
worst in all of the tests.  Part of its poor performance is due to a non-linear
solve that must be made at each $t=1, \ldots, T$ when forward filtering.  We did
not attempt to optimize the performance of this non-linear solver, and hence
some improvement may be possible, though the disparity in ESRs suggests that any
improvement would not be enough to compete with either data augmentation
approach.  As a check upon \cite{ravines-etal-2006}, we also implemented a
Metropolis-Hastings sampler that draws blocks of disturbances using Laplace
approximations.  This fared worse still.

Of note, the \Polya-Gamma method almost always has superior effective sample
sizes.  Hence faster \Polya-Gamma samplers could push the \Polya-Gamma data
augmentation technique to the top for all of the models considered.  Table
\ref{tab:benchmark-summary} provides a summary of the benchmarks; Tables
\ref{tab:dynlogit-detail} and \ref{tab:dynnb-detail} in Appendix
\ref{sec:benchmark-details} provide a more detailed account of each benchmark.

\begin{table}
\small
\centering 

\begin{tabular}{l l c c}
\multicolumn{4}{c}{Dynamic Binomial Reg.} \\
\hline
& & Est.\ & ESR \\
n & f & AR & PG/dRUM \\
\hline
% Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-wout.ar
% PG / dRUM 
$1$ & low & no & 1.26 \\
% PG / CUBS 47.5399
% Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-with.ar
% PG / PG 1
% PG / dRUM 
$1$ & low & yes & 1.29 \\
% PG / CUBS 287.291

% Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-wout.ar
% PG / PG 1
% PG / dRUM 
$1$ & high & no & 1.25 \\
% PG / CUBS 36.9012
% Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-with.ar
% PG / PG 1
% PG / dRUM 
$1$ & high & yes & 1.26 \\
% PG / CUBS 175.964

% Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-wout.ar
% PG / PG 1
% PG / dRUM 
$20$ & low & no & 0.52 \\
% PG / CUBS 29.4906
% Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-with.ar
% PG / PG 1
% PG / dRUM 
$20$ & low & yes & 0.57 \\
% PG / CUBS 49.9306

% Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-wout.ar
% PG / PG 1
% PG / dRUM 
$20$ & high & no & 0.48 \\
% PG / CUBS 13.323
% Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-with.ar
% PG / PG 1
% PG / dRUM 
$20$ & high & yes & 0.58 
% PG / CUBS 38.9124
\end{tabular}
%
\hspace{12pt}
%
\begin{tabular}{l l c c}
\multicolumn{4}{c}{Dynamic Neg.\ Binomial Reg.} \\
\hline
& & Est.\ & ESR \\
$\mu$ & f & AR & PG/FS \\
\hline
% Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-wout.ar
% PG / PG 1
% PG / FS 
$10$ & low & no & 1.03 \\
% PG / CUBS 42.3582
% Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-with.ar
% PG / PG 1
% PG / FS 
$10$ & low & yes & 1.86 \\
% PG / CUBS 59.1934

% Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-wout.ar
% PG / PG 1
% PG / FS 
$10$ & high & no & 1.15 \\
% PG / CUBS 20.6336
% Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-with.ar
% PG / PG 1
% PG / FS 
$10$ & high & yes & 1.06 \\
% PG / CUBS 18.0189

% Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-wout.ar
% PG / PG 1
% PG / FS 
$100$ & low & no & 0.76 \\
% PG / CUBS 103.273
% Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-with.ar
% PG / PG 1
% PG / FS 
$100$ & low & yes & 0.82 \\
% PG / CUBS 102.229

% Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-wout.ar
% PG / PG 1
% PG / FS 
$100$ & high & no & 0.76 \\
% PG / CUBS 34.7778
% Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-with.ar
% PG / PG 1
% PG / FS
$100$ & high & yes & 0.78 \\
% PG / CUBS 69.6132

\end{tabular}

\caption{\label{tab:benchmark-summary} A summary of benchmarking results. ESR
  PG/dRUM 
  and ESR PG/FS report the ratio of
  the median effective sampling rate for the PG method compared to the best
  alternative, which in both cases are the discrete mixture of normal approaches
  of \cite{fussl-etal-2013} and \cite{fruhwirth-schnatter-etal-2009}.  A higher
  ratio corresponds to the PG method doing better than the best alternative.  The
  PG method always beats the CUBS approach and it does better than the discrete
  mixture of normals approach in dynamic binomial logistic regression when the number of
  trials is small and in dynamic negative binomial regression when the 
  mean is small.  $n$ corresponds to the number of trials for the binomial
  response while $\mu$ corresponds to the approximate mean for the negative binomial
  response.  $f$ determines whether there is a low or high amount of correlation
  between the covariates.  Est.\ AR indicates whether the parameters of the AR
  process were estimated or not.
}
\end{table}



\section{Conclusion}

As a general guideline, we suggest using the \Polya-Gamma method as the default
approach for dynamic binomial logistic regression and the method of
\cite{fruhwirth-schnatter-etal-2009} as the default method for dynamic negative
binomial regression.  There are two exceptions to this rule: (1) if the number
of trials in the dynamic binomial logistic regression is large, it may be better
to use the method of \cite{fussl-etal-2013} and (2) if the average count size in
the dynamic negative binomial regression is small, it may be better to use the
\Polya-Gamma data augmentation approach.  If an end-user finds that his or her
modeling scenario does not definitively favor either technique, we recommend
using the \Polya-Gamma approach since it is easily incorporated into bespoke
dynamic models thanks to its conditionally Gaussian form and single layer of
latent variables.  This form ensures that all of the machinery from Gaussian
models apply and that one only need a \Polya-Gamma sampler, as is provided by
the R Package \texttt{BayesLogit}, to generate posterior samples.  For all
models benchmarked herein, the \Polya-Gamma technique has superior effective
sampling size, so that more efficient \Polya-Gamma samplers will diminish any
gaps in performance.  Users of the \Polya-Gamma technique need not make any
changes to their code to take advantage of such improvements as they will be
quarantined within the \texttt{rpg} function of the \texttt{BayesLogit} package.

\appendix

% \section{Sampling $d$ in dynamic negative binomial regression}
% \label{sec:sample-d}

% We may sample $d$ by an independent Metropolis-Hastings or a random-walk
% Metropolis-Hastings.  In either case, we need the log-likelihood of $d$.

% The conditional density for a single term $y_t \sim N(d, p_t)$ is
% \[
% \frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} p_t^{y_t} (1-p_t)^{d} ,
% \]
% or 
% \[
% \frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} 
% \Big( \frac{\mu_t}{\mu_t + d} \Big)^{y_t} \Big( \frac{d}{\mu_t + d} \Big)^d
% \]
% in terms of the mean $\mu_t$.  Since we chose to model $\log \mu_t = x_t
% \beta_t$ it will be easier to work with the latter version.  In that case, the
% log-likelihood is
% \[
% \ell(d|\mu, y) = \sum_{t=1}^T [ \log \Gamma(y_t + d)  - \log \Gamma(d) ]  + 
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]

% If we assume that $d$ is a natural number then we can rid ourselves of $T$
% evaluations of the gamma function.  We may expand $\Gamma(y_t + d)$ and
% $\Gamma(d)$ as products, in which case the log-likelihood is
% \[
% \ell(d|\mu, y) = \sum_{t=1}^T \sum_{j=0}^{y_t-1} \log(d + j) + 
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]
% We can rewrite the first term of the sum as
% \[
% \sum_{t=1}^T \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{k-1} \log(d+j) \one \{y_t = k\}
% \]
% which becomes
% \[
% \sum_{k=1}^{\max(y_t)} n_k \sum_{j=0}^{k-1} \log(d+j)
% \]
% where $n_k = \{ \# y_t = k \}$.  We have thus
% \begin{align*}
% \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{\max(y_t)-1} n_k \log(d+j) \one\{j < k\}
% & = 
% \sum_{j=0}^{\max(y_t)-1} \log(d+j) \sum_{k=j+1}^{\max(y_t)} n_k \\
% & = \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j
% \end{align*}
% where $G_j = \{ \# y_t > j \}$.  Notice that $1-G_j = F(j) = \{\# y_t \leq j\}$
% and that $G$ may be pre-processed.  Hence the log-likelihood is
% \[
% \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j + 
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]
% Preprocessing $G$ saves us from having to compute a doubly indexed summation.

\section{Benchmarks}
\label{sec:benchmark-details}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                              %% 06 Benchmarks %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For both binomial responses and negative binomial responses, we create four
synthetic data sets whose log-odds and log-mean, respectively, take the form
\begin{equation}
\label{eqn:synth-psi}
\psi_t = \alpha + x_t' \beta_t, \; t = 1, \ldots, T
\end{equation}
with
\[
\beta_t = \Phi \beta_{t-1} + \omega_t, \; \omega_t \sim N(0, W)
\]
where $\Phi = 0.95 I_2$ and $W = 0.172 I_2$.  The covariates $x_t \in \bbR^2$
are 
\[
x_{tj} = \cos(\pi f_j s_t) / \sqrt{8}
\]
where $\{s_t\}_{t=1}^T$ is an equally spaced partition of $[-1,1]$ and $f$ is
either $f^{(low)} = (1,2)$ or $f^{(high)} = (1,1.1)$, corresponding to
covariates with a lower or higher degree of correlation.  The binomial response
is
\[
y_t \sim \text{Binom}(n, p_t)
\]
where $p_t = \frac{e^{\psi_t}}{(1+e^{\psi_t})}$, $n$ is either $1$ or $20$, and
$\alpha$ from (\ref{eqn:synth-psi}) set to 1.4.  The negative binomial response
is
\[
y_t \sim \text{Nbinom}(d, \mu_t)
\]
where $d=4$, $\mu_t = e^{\psi_t}$, and $\alpha$ from (\ref{eqn:synth-psi}) is
either $\log(10)$ or $\log(100)$.  Thus, for dynamic binomial logistic
regression, we create four data sets by varying $n$ and $f$, and for dynamic
negative binomial regression, we create four data sets by varying $f$ and
$\alpha$.

For each data set and each method, we run the MCMC simulations twice, once
assuming that the parameters of the AR model are known and again assuming that
they are unknown.  When the parameters are unknown, both $\Phi$ and $W$ are
assumed to be diagonal; the prior for $\Phi$ is $\Phi_{ii} \sim N(0.95, 0.1)
\bbI_{(0,1)}$ and the prior for $W$ is $W_{ii} \sim Ga(300/2, 30/2)$.  For all
simulations, the prior for $\alpha$ is $N(0, 100)$ and the prior for $\beta_0$
is $N(0, 100 I_2)$.  For the negative binomial benchmarks, $d$ is taken to be
known.
%% My note: $d$ unknown will ``hurt'' results in that it will induce extra
%% correlation, which presumably will hurt PG,mu=10 results.

Four data sets for each type of response, along with two prior structures yields
eight MCMC simulations for each method of posterior inference.  Tables
\ref{tab:dynlogit-detail} and \ref{tab:dynnb-detail} provide a complete summary
of the results.

%-------------------------------------------------------------------------------
% dyn logit

\begin{table}
\small
\centering
%Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-wout.ar
\begin{tabular}{l r r r r r r r r } 
         Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  & ESR.min  &  ESR.med  &  ESR.max  \\ 

\hline
\hline
 \multicolumn{9}{c}{$n=1$, $f = (1,2)$, known AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    28.66 &     1.00 &   8333.94 &   9388.18 &   9904.42 &    290.79 &    327.55 &    345.57 \\ 
           dRUM   &    29.13 &     1.00 &   5344.83 &   7550.10 &   9757.91 &    183.48 &    259.19 &    334.98 \\ 
           CUBS   &   612.14 &     0.71 &   2799.89 &   4219.25 &   5845.51 &      4.57 &      6.89 &      9.55
\\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-with.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
 \multicolumn{9}{c}{$n=1$, $f = (1,2)$, unknown AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    31.33 &     1.00 &   1855.82 &   7109.57 &   9739.31 &     59.24 &    226.96 &    310.89 \\ 
           dRUM   &    31.95 &     1.00 &   1535.90 &   5632.82 &   8921.27 &     48.07 &    176.30 &    279.22 \\ 
           CUBS   &   612.37 &     0.56 &    242.71 &    487.21 &    775.24 &      0.39 &      0.79 &      1.26
 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-wout.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{$n=1$, $f = (1,1.1)$, known AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    28.60 &     1.00 &   8596.25 &   9402.62 &   9933.72 &    300.60 &    328.79 &    347.36 \\ 
           dRUM   &    29.13 &     1.00 &   6095.80 &   7675.76 &   9634.57 &    209.27 &    263.51 &    330.77 \\ 
           CUBS   &   620.95 &     0.72 &   4120.25 &   5531.41 &   6817.09 &      6.64 &      8.91 &     10.98
 \\ %\end{tabular}


%Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-with.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{$n=1$, $f = (1,1.1)$, unknown AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    31.37 &     1.00 &    765.97 &   4636.70 &   9415.57 &     24.42 &    147.81 &    300.15 \\ 
           dRUM   &    31.96 &     1.00 &    656.58 &   3734.29 &   8079.58 &     20.55 &    116.85 &    252.82 \\ 
           CUBS   &   615.39 &     0.56 &    211.12 &    516.69 &    998.70 &      0.34 &      0.84 &      1.61
 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-wout.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{$n=20$, $f = (1,2)$, known AR parameters} \\
\hline
        %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    73.54 &     1.00 &   7161.18 &   9238.58 &   9923.40 &     97.38 &    125.63 &    134.95 \\ 
           dRUM   &    28.87 &     1.00 &   3528.34 &   6959.36 &   9893.99 &    122.21 &    241.04 &    342.69 \\ 
           CUBS   &   650.67 &     0.51 &    727.34 &   2770.01 &   3699.75 &      1.12 &      4.26 &      5.69
 \\ %\end{tabular}


%Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-with.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{$n=20$, $f = (1,2)$, unknown AR parameters} \\
\hline
        %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    76.26 &     1.00 &   1897.09 &   5482.92 &   9758.10 &     24.88 &     71.90 &    127.95 \\ 
           dRUM   &    31.69 &     1.00 &   1754.39 &   4003.81 &   9536.59 &     55.36 &    126.33 &    300.91 \\ 
           CUBS   &   662.42 &     0.47 &    498.89 &    953.03 &   1980.98 &      0.75 &      1.44 &      2.99
 \\ %\end{tabular}


%Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-wout.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{$n=20$, $f = (1,1.1)$, known AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    73.16 &     1.00 &   7918.31 &   9416.01 &   9810.23 &    108.23 &    128.70 &    134.09 \\ 
           dRUM   &    28.88 &     1.00 &   4654.77 &   7771.95 &   9041.13 &    161.17 &    269.10 &    313.05 \\ 
           CUBS   &   659.75 &     0.63 &   3813.36 &   6375.18 &   6741.68 &      5.78 &      9.66 &     10.22
 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-with.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{$n=20$, $f = (1,1.1)$, unknown AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    75.98 &     1.00 &    408.79 &   5735.76 &   9484.30 &      5.38 &     75.49 &    124.83 \\ 
           dRUM   &    31.71 &     1.00 &    418.48 &   4106.12 &   8269.33 &     13.20 &    129.48 &    260.76 \\ 
           CUBS   &   665.71 &     0.47 &    273.76 &   1290.78 &   1893.79 &      0.41 &      1.94 &      2.84
 \end{tabular}

 \caption{\label{tab:dynlogit-detail} Dynamic binomial logistic benchmarks.  $n$
   corresponds to the number of trials for each response; $f$ corresponds to the
   frequency used when constructing the covariates.  When $f = (1,2)$ the
   covariates are less correlated when $f=(1,1.1)$ the covariates are more
   correlated.  The PG method does well when $n=1$.  The FS method does well
   when $n=20$.  Time refers to the time taken to produce 10,000 post burn-in
   samples.  ARate refers to the Metroplis-Hastings acceptance rate.}
\end{table}

%-------------------------------------------------------------------------------
% dyn-nb

\begin{table}
\small
\centering
%Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-wout.ar
\begin{tabular}{l r r r r r r r r } 
Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  & ESR.med  &  ESR.max \\
\hline
\hline
 \multicolumn{9}{c}{$\mu=10$, $f = (1,2)$, known AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    42.50 &     1.00 &   7972.97 &   9649.48 &   9947.01 &    187.60 &    227.04 &    234.04 \\ 
             FS   &    29.54 &     1.00 &   1861.10 &   6530.79 &   9831.67 &     62.99 &    221.05 &    332.77 \\ 
           CUBS   &   655.71 &     0.61 &    800.95 &   3517.15 &   5603.81 &      1.22 &      5.36 &      8.55
 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-with.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
 \multicolumn{9}{c}{$\mu=10$, $f = (1,2)$, unknown AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    45.29 &     1.00 &    156.63 &   4852.66 &   9812.26 &      3.46 &    107.14 &    216.67 \\ 
             FS   &    32.31 &     1.00 &    135.06 &   1856.03 &   7837.19 &      4.18 &     57.45 &    242.58 \\ 
           CUBS   &   674.23 &     0.55 &    116.36 &   1223.44 &   2700.52 &      0.17 &      1.81 &      4.01
 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-wout.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
 \multicolumn{9}{c}{$\mu=10$, $f = (1,1.1)$, known AR parameters} \\
\hline
         %Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
            PG  &    42.39 &     1.00 &   9142.96 &   9692.30 &   9943.59 &    215.67 &    228.62 &    234.55 \\ 
            FS  &    29.55 &     1.00 &   2820.14 &   5898.99 &   9765.03 &     95.43 &    199.61 &    330.43 \\ 
          CUBS  &   656.58 &     0.73 &   4509.25 &   7276.52 &   7891.49 &      6.87 &     11.08 &     12.02

 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-with.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
 \multicolumn{9}{c}{$\mu=10$, $f = (1,1.1)$, unknown AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    45.16 &     1.00 &    276.72 &   1725.30 &   9555.14 &      6.13 &     38.20 &    211.58 \\ 
             FS   &    32.28 &     1.00 &    263.56 &   1164.83 &   5458.53 &      8.16 &     36.08 &    169.10 \\ 
           CUBS   &   677.20 &     0.62 &    405.83 &   1438.07 &   3812.09 &      0.60 &      2.12 &      5.63
 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-wout.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
 \multicolumn{9}{c}{$\mu=100$, $f = (1,2)$, known AR parameters} \\
\hline
         %Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
            PG  &    38.56 &     1.00 &   1860.03 &   7287.72 &   9927.30 &     48.23 &    188.99 &    257.43 \\ 
            FS  &    29.17 &     1.00 &   2352.61 &   7301.91 &   9827.50 &     80.65 &    250.31 &    336.89 \\ 
          CUBS  &   669.34 &     0.38 &    228.97 &   1226.28 &   2833.25 &      0.34 &      1.83 &      4.23

 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-with.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
 \multicolumn{9}{c}{$\mu=100$, $f = (1,2)$, unknown AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    41.33 &     1.00 &   1798.58 &   4986.44 &   9178.54 &     43.51 &    120.63 &    222.06 \\ 
             FS   &    32.28 &     1.00 &   2098.04 &   4727.49 &   9132.24 &     65.00 &    146.45 &    282.90 \\ 
           CUBS   &   677.94 &     0.32 &    459.22 &    799.84 &   1194.99 &      0.68 &      1.18 &      1.76
 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-wout.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
 \multicolumn{9}{c}{$\mu=100$, $f = (1,1.1)$, known AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    40.73 &     1.00 &   4061.25 &   7265.75 &   9856.66 &     99.72 &    178.41 &    242.03 \\ 
             FS   &    29.44 &     1.00 &   3396.35 &   6938.80 &   9867.25 &    115.36 &    235.66 &    335.13 \\ 
           CUBS   &   668.03 &     0.42 &   1676.35 &   3429.40 &   4146.91 &      2.51 &      5.13 &      6.21
 \\ %\end{tabular}

%Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-with.ar
%\begin{tabular}{l r r r r r r r r } 
\hline
 \multicolumn{9}{c}{$\mu=100$, $f = (1,1.1)$, unknown AR parameters} \\
\hline
         %Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    43.57 &     1.00 &   1186.07 &   3214.67 &   6880.24 &     27.22 &     73.79 &    157.93 \\ 
             FS   &    32.25 &     1.00 &   1115.12 &   3071.79 &   6879.66 &     34.58 &     95.25 &    213.31 \\ 
           CUBS   &   682.59 &     0.30 &    412.62 &    723.17 &   1028.33 &      0.60 &      1.06 &      1.51
 \end{tabular}
 \caption{\label{tab:dynnb-detail} dynamic negative binomial benchmarks. $\mu$
   corresponds to the mean of the response; $f$ corresponds to the frequency
   used when constructing the covariates.  When $f = (1,2)$ the covariates are
   less correlated when $f=(1,1.1)$ the covariates are more correlated.  The PG
   method does well when $\mu=10$.  The FS method does well when $\mu=100$. Time
   refers to the time taken to produce 10,000 post burn-in samples.  ARate
   refers to the Metroplis-Hastings acceptance rate.}
\end{table}

%-------------------------------------------------------------------------------

% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliography{bayeslogit}{}
\bibliographystyle{abbrvnat}

\end{document}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Tokyo Rainfall} \\
% \hline
%           Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
%             CUBS  &   233.98 &     1.00 &   652.86 &   761.98 &   910.62 &     2.79 &     3.26 &     3.89 \\ 
%               PG  &    21.72 &     1.00 &  4352.80 &  7735.08 & 10081.46 &   200.43 &   356.18 &   464.22 \\ 
%             dRUM  &    20.60 &     1.00 &  1627.61 &  3649.46 &  5428.93 &    79.03 &   177.19 &   263.60
%  \end{tabular}
% \end{table}

% \begin{table}
% \centering
% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta$.} \\
% \hline
%           Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%              PG  &    22.59 &     1.00 &   7367.68 &   9229.91 &  10189.37 &    326.10 &    408.52 &    450.97 \\ 
%            dRUM  &    23.19 &     1.00 &   2759.83 &   7260.46 &   9967.66 &    118.99 &    313.04 &    429.77
%          \end{tabular}

% \begin{tabular}{l r r r r r r r r }
% \hline
% \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta$.} \\
% \hline 
%           Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%              PG  &    22.52 &     1.00 &   8494.04 &   9662.06 &  10225.35 &    377.13 &    428.98 &    454.00 \\ 
%            dRUM  &    23.17 &     1.00 &   4963.68 &   8105.02 &   9936.46 &    214.26 &    349.86 &    428.91
% \end{tabular}

% \end{table}


% \begin{table}
% \centering
% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta, \phi, W$.} \\
% \hline
%           Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%             PG  &    25.04 &     1.00 &    643.23 &   6531.34 &  10420.88 &     25.69 &    260.82 &    416.13 \\ 
%           dRUM  &    25.73 &     1.00 &    430.79 &   4490.34 &  10124.47 &     16.74 &    174.55 &    393.48
%  \end{tabular}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta, \phi, W$.} \\
% \hline
%           Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%             PG  &    25.16 &     1.00 &    988.83 &   5654.23 &  10337.07 &     39.31 &    224.75 &    410.87 \\ 
%           dRUM  &    25.84 &     1.00 &   1040.12 &   5496.94 &   9760.64 &     40.25 &    212.74 &    377.75
%  \end{tabular}



