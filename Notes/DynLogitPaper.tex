\documentclass[11pt]{article}

\input{commands}

\usepackage{natbib}

\newcommand{\Polya}{P\'{o}lya}
\newcommand{\PG}{\text{PG}}
\newcommand{\point}{\noindent $\bullet$ }

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
%\parindent 0in
%\parskip 12pt

% Set Fancy Style
%\pagestyle{fancy}

%\lhead{Left Header}
%\rhead{Right Header}

\title{Dynamic Generalized Linear Models using \Polya-Gamma augmentation}
\author{Carlos M. Carvalho, James G. Scott, Liang Sun, Jesse Windle}

\begin{document}

\maketitle
\tableofcontents

\newpage

\section{Introduction}

\subsection{Dynamic Generalized Linear Models}

Generalized linear models (GLMs) arise when the response variable is categorical
or integral in nature as opposed to continuous.  For instance, one may be
interested in forecasting the political affiliation of citizens using their
habits of consumption, entertainment choices, etc. to get out the vote during an
election \citep{becker-12-0922}, or perhaps one wants to neuronal activity as a
function of some stimulus.  In either case, the response does not take on a
continuous range of values but rather takes on either on a finite number of
values, as in the former, or a whole number, as in the latter.

In GLMs, one models the distribution of the response $(y_i | \psi_i)$ by
assuming that $\psi_i$ can be written as a linear combination of some
predictors, $x_i \beta$.  The parameter $\psi_i$ may not be the most common way
to parameterize the distribution of the reponse $y_i$ and hence one often starts
with a representation $(y_i | \mu_i)$ and then transforms $\mu_i$ to $\psi_i$
under some function $g$ that maps to a coordinate system in which it is
reasonable to use the linear model $\psi_i = x_i \beta$.  Such models may be
summarized as
\[
\begin{cases}
y_i \sim \bbP(\mu_i), & i=1,\ldots, N \\
\psi_i = g(\mu_i) = x_i \beta.
\end{cases}
\]
One may proceed by the usual rules of Bayesian inference to calculate the
posterior distribution of $\beta$; however, the exact form of the this
distribution is often unrecognizable and not easy to simulate.  For instance,
when modeling binary data one may prefer to describe the distribution of $y_i$
by the probability $p_i$ of observing a certain outcome but to model $p_i$
linearly on the log-odds scale, $\psi_i = \log \frac{p_i}{1-p_i}$.  The model is
then
\[
\begin{cases}
y_i \sim \text{Binom}(1,p_i) \\
\psi_i = \log \frac{p_i}{1-p_i} = x_i \beta.
\end{cases}
\]
The posterior distribution for $\beta$ is not easy to simulate directly.

Dynamic GLMs (DGLMs) are useful when the underlying relationship between the
covariates and response changes slowly in time.  Following on the example above,
suppose the binary response denotes voter affiliation in the United States:
either a Repulican voter or a non-Republican voter.  It may be the case that the
characteristics that increase the likelihood of finding a Republican voter
change in time.  Perhaps the voting profile of white mothers that drive minivans
has shifted over the last two election cycles due to some factors not considered
by the model.  Instead of using the last election's regression coefficient one
might want to update to a new regression coefficient as information is aquired.
A DGLM would allow one to do just that.

Dynamic generalized linear models extend the static framework to the case that
the regression coefficient changes with the index of the observation, which we
will interpret as time.  In that case the model becomes
\[
\begin{cases}
  y_t \sim \bbP(\mu_t), & t=1, \ldots, T \\
  \psi_t = g(\mu_t) = x_t \beta_t \\
  \beta_t \sim \text{AR}(1).
\end{cases}
\]
We describe the evolution of $\beta_t$ above using an AR(1) process, but there
is nothing in what follows that limits one to that class of stochastic
processess.  So long as one is comfortable modeling $\beta$ using a Gaussian
process the methods suggested in this paper will hold.  The challenges
associated with exact posterior simulation are compounded in the dynamic case
since instead of $p$ covariates one must now handles $p \times T$.  

The main focus of this paper is exact posterior simulation when working within a
few specific class of DGLMs: dynamic binary and binomial logistic regression and
dynamic negative binomial regression for count data.  In general, the method we
outline will work whenever the response induces a logistic likelihood, which is
a likelihood in the transformed coordiantes $\psi = (\psi_i)$ of the form
\[
\ell(\psi | y) \propto \prod_{t=1}^T
\frac{(e^{\psi_i})^{a_i}}{(1+e^{\psi_i})^{b_i}}.
\]

\subsection{Previous Work}

\point Normal Mixture Models

One may do posterior inference by Metropolis-Hastings approach, data
augmentation for Gibbs sampling, or some combination thereof.  Early attempts,
like \cite{west-etal-1985} employed approximations to the true posterior to do
inference for dynamic linear models for exponential families.  Building upon
that work, \cite{ravines-etal-2006} use this approximation with a
Metropolis-Hastings step for exact posterior inference.  Fr\"{u}hwirth-Schnatter
and her various colaborators have a collection of papers using data augmentation
and a discrete mixture of normals approximation for posterior inference.
However, we find neither method completely satisfactory.

Previous work
includes the CUBS algorithm of \cite{ravines-etal-2006} which builds upon the
work of \cite{west-etal-1985}

Previous attempts for inference in dynamic generalized linear models 

\subsection{Contributions of the Paper}



Recently, \cite{polson-etal-2012} introduced a new data augmentation scheme for
posterior distributions derived from logisitic likelihoods.  Such models arise
when the distribution of the reponse variable $y_i$ depends upon a proportion
$p_i$, which occurs when working with catogorical or count data, and one models
this proportion on the log-odds scale, $\psi_i = \log \frac{p_i}{1-p_i}$, as the
linear combination $x_i \beta$.  In this case, the likelihood takes the form
\[
\prod_{i=1}^n \frac{ (e^{\psi_i})^{a_i} }{ (1 + e^{\psi_i})^{b_i} }
\]
where $a_i$ and $b_i$ are some function of the response, explanatory variables,
hyperparameters, or some combination thereof.  The \Polya-Gamma distribution
possesses an integral identity so that the posterior distribution $p(\beta |
y)$, derived from the logistic likelihodo above, may ``demarginalized'' to
produce an augmented posterior distribution $p(\beta, \omega | y)$ in which the
complete conditional $p(\omega | \beta, y)$ is the product of independent
\Polya-Gamma distributions and, if one choses a normal prior for $\beta$, the
complete conditional $p(\beta | \omega, y)$ is normal as well yielding a
convenient, efficient Gibbs sampler.  \cite{polson-etal-2012} show that this
method is superior to similar alternative methods, such as
\cite{holmes-held-2006} and \cite{fruhwirth-schnatter-fruhwirth-2007}, for
binary logistic regression and that it may be applied in a number of other
settings including contingency tables, multinomial logistic regression, and
negative binomial regression for count data.  However, they devote little
attention to dynamic models.  This paper fills that void.

% when $\omega \sim \PG(b, 0)$,
% \[
% \frac{(e^{\psi})^{a}}{(1+e^{\psi})^{b}} = 2^{-b} e^{\kappa \psi}
% \int_0^\infty e^{-\omega \psi_i^2 / 2} p(\omega) d \omega
% \]
% where $\kappa = a - b / 2$; thus,

\point Polya Gamma Augmentation for static models.

\point Background of dynamic GLM.

Prior models for dynamic generalized models include:

\begin{itemize}
\item \cite{west-etal-1985}

  For exponential families.

\item \cite{ravines-etal-2006}

  For exponential families.  Specific example for binomial, poisson, Gamma
  transfer function.

\item \cite{fruhwirth-schnatter-fruhwirth-2007}

  For binary logistic, multinomial logistic, though give binomial logistic example.

\item \cite{fruhwirth-schnatter-wagner-2006}

  For count data - Poisson regression.

\item \cite{fruhwirth-schnatter-etal-2009}

  Shows how to reduce the number of latents.  Can do dynamic Poisson regression.
  Can also do dynamic negative binomial regression.

  In both cases, she suggests using a normal mixture representation of the log
  gamma distribution.  But this turns out to be somewhat annoying to do since
  the shape parameter changes.  Hence you need an entire family of log-gamma
  distributions.

\end{itemize}

\section{Dynamic Binary Logistic Regression}

Suppose $\psi_{it}$ is the log-odds of success for observation $i$ at time $t$,
\[
P(y_{it}=1) = p_{it} \; \textmd{ where } \psi_{it} = \log \frac{p_{it}}{1-p_{it}}.
\]
Furthormore, suppose that the system is not changing in time.  In that case, the
we would model the log-odds using predictors we might find suitable yielding the
static regression
\[
\psi_{it} = x_{it} \beta.
\]
Given the logistic likelihood one can infer $\beta$ by his or her favorite
method, which in our case is the \Polya-Gamma auxiliary variable approach.

Now suppose that this system is evolving in time and that the statistical
manifestation of that evolution is a dynamic regression coefficients so that the
log odds are modeled by \( \psi_{it} = x_{it} \beta_t.  \) If we do not have a
strong sense of how the regression coefficient evolves, then a random walk for
$\beta_t$ is appropriate and we arrive at the dynamic generalized linear model
\[
\begin{cases}
\psi_{it} = x_{it} \beta_t \\
\beta_t = \beta_{t-1} + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]

To apply the \Polya-Gamma method, imagine fixing $t$ and looking at a single
term in the likelihood.  You would have something like (where $y_i$ is the
number of successes for covariate $i$--though for the PG code we have $y_i$ is
the proportion of successes!)
\[
\exp \Big[ (y_{i} - n_{i}/2) x_{i} \beta - \frac{1}{2} \omega_{i} \psi_i^2
\Big] 
p(\omega_i | n_i, 0). 
\]
Collecting all the observations this becomes
\[
\exp \Big[ \alpha' X \beta - \frac{1}{2} \beta' X' \Omega X \beta \Big]
p(\omega | n, 0) \;
\textmd{ where } \;
\alpha_i = (y_i - n_i/2) \; \textmd{ and } \; X = [x_i].
\]
Assuming that $\omega$ is fixed.  We want to find a quadratic form
\[
(z - X \beta)' \Omega (z - X \beta) = z' \Omega z - 2 z' \Omega X \beta + (X
\beta)' \Omega (X \beta).
\]
Since $\omega$ is fixed we can just multiply the likelihood by $\exp
\frac{-1}{2} z' \Omega z$ where $\Omega z = \alpha$ to generate this expression.
Thus, for fixed $\omega$ the likelihood of $\beta$ can be interpreted as coming from
\[
z = X \beta + \ep, \; \ep \sim N(0, \Omega^{-1})
\]
where
\[
\Omega z = \alpha.
\]
\begin{comment}
(Waving my hands: We have $\Omega z = \alpha = y - n/2$.  Thus we should have $y
= \Omega z + n/2$.  Use for generating synthetic data?  No, its pseudo-data.)
\end{comment}
Reincorporating the time index, we have
\[
\begin{cases}
z_t = X_t \beta_t + \ep_t, & \ep_t \sim N(0, \Omega_t^{-1}) \\
\beta_t = \beta_{t-1} + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]
It is easy to calculate $z_t$ since $\Omega_t$ is diagonal.  One still produce a
joint draw of $(\beta | W, D_T)$ when there are a different number of or no
observations at each time step.
\begin{comment}
  The one place that we need to be careful is that there may be a variable
  number of observations at each point in time.  Thus the number of rows in
  $z_t$ and $X_t$ is $m_t$, which may vary.  Also, it may be the case that there
  are no observations, in which case we have the equivalent of missing data.  I
  believe the dynamic linear model shouldn't have a problem with that.
\end{comment}
The sampling procedure is then
\begin{enumerate}
\item Set $\beta_t = 0$ for all $t$.  This is a reasonable seed since it
  represents even odds.
\item Calculate $\psi_t = X_t \beta_t$.
\item Sample $\omega_{it} \sim PG(n_{it}, \psi_{it})$.
\item Sample $\beta_t$ using the dynamic linear model above.
\item Repeat, starting at (2).
\end{enumerate}

\section{Dynamic Negative Binomial Regression}

\section{Comparison}

\section{Conclusion}

% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliography{bayeslogit}{}
\bibliographystyle{abbrvnat}

\end{document}