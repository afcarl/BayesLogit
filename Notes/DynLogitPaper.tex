% -*- TeX-master: "DynLogitPaperBA.tex"; -*-

\begin{abstract}
Dynamic linear models with Gaussian observations and Gaussian states lead to
closed-form formulas for posterior simulation.  However, these closed-form
formulas break down when the response or state evolution ceases to be Gaussian.
Dynamic, generalized linear models exemplify a class of models for which this is
the case, and include, amongst other models, dynamic binomial logistic
regression and dynamic negative binomial regression.  Finding and appraising
posterior simulation techniques for these models is important since modeling
temporally correlated categories or counts is useful in a variety of
disciplines, including ecology, economics, epidemiology, medicine, and
neuroscience.  In this paper, we present one such technique, \Polya-Gamma data
augmentation, and compare it against two competing methods.  We find that the
\Polya-Gamma approach works well for dynamic logistic regression and for dynamic
negative binomial regression when the count sizes are small.  Supplementary
files are provided for replicating the benchmarks.

\noindent %
{\it Keywords:} {Bayesian, Binomial, Logistic, Regression, Simulation}
\end{abstract}

% \tableofcontents
% \newpage

\doublespace

\section{Introduction}

\subsection{Bayesian inference for complex discrete data}

Bayesian inference for discrete-data models has long been recognized as a
challenging problem, due to the analytically inconvenient form of the likelihood
functions that arise.  This makes it much more difficult to fit discrete models
with rich stochastic structure.  This is in stark contrast to the case of
real-valued data, for which the Bayesian literature contains a tremendous
variety of highly structured models.  These tools allow us to handle data sets
that are not merely large, but also dense: varying in time or space, rich with
covariates, or layered with multi-level structure.  This level of sophistication
is made possible by the mathematical and computational simplicity of the
Gaussian likelihood, and the flexibility of the mixture-of-Gaussians class.

Given the widespread need for rich discrete-data models, much work has been done
on making them amenable to Bayesian inference.  The traditional approach is to
work directly with the discrete-data likelihood, whether via numerical
integration \citep{skene-wakefield-1990}, analytic approximations
\citep{carlin-1992,bradlow-etal-2002,gelman-etal-2008,forster-2011}, or the
Metropolis-Hastings algorithm \citep{dellaportas-forster-1999,
  dobra-tebaldi-west-2006}.  These methods are used in popular R packages that
implement MCMC for non-Gaussian models \citep{mcmcpack-2011}.  Unfortunately,
none lead to a fully automatic approach to posterior inference, as they require
either approximations (whose quality must be validated) or the choice of a
tuning constant (as in the Metropolis--Hastings sampler) that strongly affects
performance.  In practice, these difficulties limit users to simple models and
small data sets, especially in the case of non-i.i.d.~data.

This paper considers the particular situation of integer-valued outcomes
collected over time.  This kind of data may be found in, for example, ecology or
epidemiology when modeling populations of species or outbreaks of infections,
and in neuroscience when modeling brain activity as manifest in neural spike
trains.  Such data sets are archetypal of the wider pattern of computational
difficulty associated with discrete data sets: in addition to the non-Gaussian
likelihood, one must also account for the temporal correlation.
Autoregressive-like models are helpful in this regard, at the cost of making
posterior inference more difficult.

We directly address this problem, proposing an elegant and efficient data
augmentation technique for dynamic models with binomial likelihoods.  This
includes the logistic regression model and its variants (for binary and
categorical data), along with the negative-binomial model (for count data).  Our
approach involves a data-augmentation scheme based on the family of Polya-Gamma
distributions.  As we will show, this is the crucial step in allowing a wide
class of models to be handled using simple variations on established techniques
for Bayesian inference in linear Gaussian state-space models.
%%%%

\subsection{Challenges}

\npoint The origin of dynamic logit and negative-binomial models can be traced
to their static ancestors, which, in turn, are successors to the linear model.
We provide a brief overview of this evolution and the challenges that arise
along the way.

\npoint Generalized linear models (GLMs) 
% extend their linear counterparts by parameterizing
parameterize the expectation and variance of the response in terms of a linear
combination of the predictors, $\psi_t = \vx_t' \bbeta$, and lead to tractable
models for counts, categories, and other non-real valued data
\citep{wedderburn-1974, mccullagh-nelder-1989}.  Strictly speaking, one need not
specify the distribution of the response, just its first two moments, but since
we are interested in Bayesian inference we will restrict our attention to
situations in which the likelihood, $f(y_t | \psi_t)$, is specified.  When the
likelihood comes from the exponential family of distributions, one may easily
calculate the posterior mode and associated error estimates; however, most of
these likelihoods lead to posterior distributions which are difficult to sample,
requiring Metropolis-Hastings sampling, Gibbs sampling via data augmentation, or
some other MCMC method.

\npoint Dynamic generalized linear models (DGLMs) permit an evolving
relationship between the response and the covariates via time-varying regression
coefficients, $\{\bbeta_t\}_{t=1}^T$, so that $\psi_t = \vx_t' \bbeta_t$ for
$t=1, \ldots, T$.  One must specify a prior for $\{\bbeta_t\}_{t=1}^T$, which
controls how quickly $\{\bbeta_t\}_{t=1}^T$ may change; it is common to let
$\{\bbeta_t\}_{t=1}^T$ be a Gaussian AR(1) process or a Gaussian random walk.
In either case, one recovers a state-space model characterized by the
observation distribution $f(y_t | \psi_t)$ for the response and the Markovian
evolution distribution $g(\bbeta_t | \bbeta_{t-1})$ for the hidden states.
Transitioning from the static to the dynamic model is a small step conceptually
but a big step practically.  In particular, it becomes more difficult to
generate posterior samples.

\npoint To clarify the challenge of simulating $(\{\bbeta_t\}_{t=1}^T |
\{y_t\}_{t=1}^T)$ for dynamic generalized linear models, it is helpful to
consider more and less tractable state-space models.  
%%%%
\npoint State-space models with non-Gaussian, non-linear responses and
non-linear evolution equations are less tractable.  One may efficiently
approximate the filtered distributions of such models, $p(\bbeta_t |
\{y_s\}_{s=1}^t)$, using sequential methods, but the lack of structure makes
sampling from the posterior of $\{\bbeta_t\}_{t=1}^T$ difficult.  
%%%%
\npoint In contrast, dynamic linear models (DLMs) with Gaussian states and
Gaussian observations are more tractable: one may sample the posterior
distribution of the states using the forward filter backward sample (FFBS)
algorithm \citep{carter-kohn-1994, fruhwirth-schnatter-1994}, a procedure that
is linear in the number of observations and thus quite efficient.  However, the
FFBS algorithm breaks down outside of these vary narrow assumptions.
%%%%
\npoint Dynamic generalized linear models that have Gaussian, linear evolution
equations, but non-Gaussian, non-linear response distributions sit between these
extremes.  Within this class, the FFBS algorithm is not immediately applicable,
but the linear evolution equations give one hope of finding a clever way to
reintroduce it.  Many approaches follow this path, combining techniques used for
generalized linear models with the FFBS algorithm.  However, these amalgamations
are not always as effective as their constituent parts.  In particular,
Metropolis-Hastings based techniques for GLMs do not translate well to the
dynamic setting, as $\{\bbeta_t\}_{t=1}^T$ is of much higher dimension than
$\bbeta$.  
%%%%
\npoint Data augmentation techniques for GLMs that lead to a Gaussian complete
conditional for $\bbeta$ provide a preferable approach since the corresponding
complete conditional for $\{\bbeta_t\}_{t=1}^T$ will coincide with the
likelihood of a DLM and hence the FFBS algorithm.  (See section \ref{sec:pg} for
details.)  But such data augmentation tricks are difficult to find.  Recently,
\citet{polson-etal-2013} introduced a data augmentation trick for GLMs with
binomial likelihoods and that is what we exploit here.

\npoint This paper presents \Polya-Gamma data augmentation for dynamic
regression models with binomial likelihoods and compares it to a pair of other
approaches: conjugate updating and backward sampling with a Metroplis-Hastings
step \citep{migon-etal-2013} and data augmentation using a discrete mixture of
normals approximation \citep{fruhwirth-schnatter-etal-2009, fussl-etal-2013}.
Our method is pleasing because of its parsimony, ease of implementation, and
efficiency.

\npoint Dynamic models of counts or categories, like the ones studied herein,
may arise across a range of disciplines, as noted above.  Hence, having a guide
to the most efficient posterior simulation techniques for these models is
important for practitioners.  To that end, we benchmark two dynamic generalized
linear models and show that the \Polya-Gamma approach is the best choice for
dynamic logistic regression and dynamic negative binomial regression when the
count size is small.

\npoint The outline of the paper is as follows: Section 2 reviews other methods
of posterior simulation for dynamic models; Section 3 shows how one may use the
\Polya-Gamma data augmentation technique in the dynamic setting; Section 4
presents the benchmarking results; Section 5 concludes.

\section{Previous Efforts}
\label{sec:previous-efforts}

\npoint Let us still assume that $y_t$ is the response, generated by some member
of the exponential family according to $f(y_t | \psi_t)$ where $\psi_t = \vx_t'
\bbeta_t$ for $t=1, \ldots, T$, and that the hidden states
$\{\bbeta_t\}_{t=1}^T$ evolve according to an AR(1) or random walk process with
Gaussian disturbances described by the transition density $g(\bbeta_t |
\bbeta_{t-1})$.
%%%%
\npoint In the very specific case that $y_t \sim N(\psi_t, V_t)$, the forward
filter backwards sampler (FFBS) generates posterior samples of
$\{\beta_{t}\}_{t=1}^T$ in $\mcO(T)$ time as $T$ varies \citep{carter-kohn-1994,
  fruhwirth-schnatter-1994}.  When the response comes from some other
distribution, like the binomial or negative binomial distributions, the FFBS
breaks down.
%%%%
\npoint Thus, one must appeal to approximate distributions, Metropolis-Hastings
sampling, Gibbs sampling, or some other MCMC approach, like particle methods.

\npoint Alternative approaches to posterior simulation date back to at least
\citet{west-etal-1985}, who employ a conjugate updating, backward sampling (CUBS)
strategy to generate approximate posterior draws.  Their strategy is to use an
approximation of the filtered distributions $(\bbeta_t | \{y_s\}_{s=1}^t)$ when
backwards sampling.  The CUBS method is also $\mcO(T)$ as $T$ varies, but
requires solving $T$ non-linear equations, which is time consuming.

\npoint Metropolis-Hastings based approaches can take approximate draws and make
them exact by introducing an accept/reject step.  The challenge is to devise a
good approximation so that the probability of accepting a proposal is
reasonable.  As the dimension of the quantity one wants to sample increases,
this becomes more difficult.  A potential solution is to break the
high-dimensional quantity into pieces and sample the complete conditional of
these pieces sequentially, that is to do Metropolis-Hastings within Gibbs
sampling.  However, sampling the high-dimensional quantity in blocks tends to
increase the correlation between the successive samples within the Markov chain.
Thus, one must try to strike a balance between blocks that are too large,
leading to poor acceptance rates, and blocks that are too small, leading to
excessive autocorrelation.  One finds an extreme form of the blocking approach
in \citet{carlin-etal-1992}, prior to the advent of the FFBS.

\npoint Given this general strategy, one must still figure out how to pick a
good proposal distribution.  Since $f$ comes from the exponential family, it is
natural to use the Laplace approximation to arrive at a Gaussian proposal, as
this coincides with the iteratively reweighted least squares procedure for
generalized linear models \citep{gamerman-1997}.  One may follow a similar
strategy by first doing a change of coordinates to sample the disturbances
instead of the hidden states and then use a Laplace approximation
\citep{shephard-pitt-1997, gamerman-1998}.  Advocates of this strategy suggest
that the subsequent blocking possesses less intrinsic autocorrelation.
\citet{migon-etal-2013} show that the CUBS approximation of \citet{west-etal-1985}
is a good proposal, and based upon our own computational experiments we find
that, indeed, the approach of \citet{migon-etal-2013} is better than sampling
blocks of disturbances.  However, none of these Metropolis-Hastings proposals
are completely satisfactory.  In particular, \citet{migon-etal-2013} is still
time consuming, requiring $T$ non-linear solves for each MCMC iteration.

\npoint Data augmentation provides a preferable approach.  If one finds a data
augmentation scheme for the static problem, that is when $\psi_t = \vx_t'
\bbeta$, so that the complete conditional of $\bbeta$ is Gaussian, then the
corresponding complete conditional of $\{\bbeta_t\}_{t=1}^T$ for the dynamic
analog will coincide with a DLM and one may use the FFBS algorithm.
%%%%
\npoint But finding such a scheme is difficult since it requires auxiliary
variables that (1) yield a Gaussian complete conditional for $\bbeta$ and (2)
can be sampled efficiently.  Examples that almost meet both criterions include
\citet{mcfadden-1974}, where (1) is not met, and \citet{holmes-held-2006}, where
(2) can be significantly improved.

\npoint Fr\"{u}hwirth-Schnatter and Fr\"{u}hwirth and their colleagues Fussl,
Held, Rue, and Wagner have developed fixes for these shortcomings that rely upon
approximating distributions using discrete mixtures of Gaussians.  Their work
has lead to data augmentation schemes that satisfy (1) and (2), in an
approximate though accurate sense, for binomial and multinomial logistic
regression \citep{fruhwirth-schnatter-fruhwirth-2007,
  fruhwirth-schnatter-fruhwirth-2010, fussl-etal-2013}, Poisson regression
\citep{fruhwirth-schnatter-wagner-2006, fruhwirth-schnatter-etal-2009}, and
negative binomial regression \citep{fruhwirth-schnatter-etal-2009}.  (In the
sequel, for dynamic binomial logistic regression, we will limit our comparison
to \citet{fussl-etal-2013} since it appears to be the best choice within
Fr\"{u}hwirth-Schnatter et al.'s discrete mixture cornucopia.)
%%%%
\npoint While their methods work well, several rely upon precomputing large
tables of weights, means, and variances for the components of a collection of
mixtures that approximate an entire family of distributions.  Further all of the
discrete mixture of normal techniques make use of at least two layers of
auxiliary variables.  One would prefer to avoid many layers of latents since
this may inhibit traversing the posterior landscape and enlarges the memory
footprint when storing the latent states.

\npoint The situation is significantly harder once one abandons the structure we
have assumed to this point.  \citet{geweke-tanizaki-2001} have reviewed a
plethora of approaches within the more general setting of state-space models,
none of which work that well.  \citet{godsill-etal-2004} show how to sample
smoothed states, $(\{\bbeta_t\}_{t=1}^T | \{y_t\}_{t=1}^T)$, using particle
methods; however, this is relatively expensive in comparison to filtering
states.  Recently, \citet{geweke-etal-2013} leveraged the power of GPUs as a way
to significantly speed up sequential Monte Carlo, an interesting avenue not
considered herein.

\section{\Polya-Gamma Data Augmentation}
\label{sec:pg}

We consider DGLMs with binomial likelihoods: \( f(y_t | q_t) = c(y_t) q_t^{a_t}
(1-q_t)^{d_t}, \) where $a_t$ and $d_t$ may depend on $y_t$ but do not depend
upon $q_t$.  Logistic regression and negative binomial regression are two common
models that fit within this regime.  It is preferable to express the likelihood
$f$ in terms of the log odds, $\psi_t = \log \frac{q_t}{1-q_t}$, since this is
the scale on which we linearly model the covariates:
\[
f(y_t | \psi_t) = c(y_t) \, \frac{\exp(\psi_t)^{a_t}}{(1+\exp(\psi_t))^{b_t}},
\]
where $b_t = a_t + d_t$.  Given a collection of observations $\yy =
\{y_t\}_{t=1}^T$, the posterior distribution of the hidden states $\betap =
\{\bbeta_t\}_{t=1}^T$ is
\[
p(\betap | \yy) = c(\yy) \Big[ \prod_{t=1}^T
\frac{\exp({\psi_t})^{a_t}}{(1+\exp({\psi_t}))^{b_t}}
\Big] p(\betap),
\]
where $\psi_t = \vx_t' \bbeta_t$.  (We will use the generic function $p$ to
denote a probability density.)  Following \citet{polson-etal-2013}, one may
introduce a collection of independent \Polya-Gamma random variates $\oomega =
\{\omega_t\}_{t=1}^T$, $\omega_t \sim \PG(b_t, \psi_t)$ for $t=1, \ldots, T$, to
construct the joint distribution
\[
p(\betap, \oomega | \yy) = c(\yy) \Big[ \prod_{t=1}^T
\frac{\exp({\psi_t})^{a_t}}{(1+\exp({\psi_t}))^{b_t}} p(\omega_t | b_t, \psi_t)
\Big] p(\betap) \, .
\]
via the conditional structure $p(\betap, \oomega | \yy) = p(\oomega | \betap,
\yy) p(\betap | \yy)$.  The $\PG{}(b_t, \psi_t)$ density possesses the special
form
\[
p(\omega_t | b_t, \psi_t) = \cosh^{b_t}(\psi_t/2) \exp({- \omega_t \psi_t^2 / 2})
p(\omega_t),
\]
which is useful since the ratio
\begin{equation}
  \label{eqn:pg-cancellation}
  \cosh^{b_t}(\psi_t/2) / (1+\exp({\psi_t}))^{b_t}
  \propto
  \exp({-\psi_t b_t/2}) 
\end{equation}
so that, upon completing the square, the complete conditional of $\betap$ is
% \[
% p(\beta | y, \omega) \propto
% \Big[ \prod_{t=1}^n e^{\kappa_t \psi_t - \frac{\omega_t}{2} \psi_t^2}
% \bbI \{\psi_t = \vx_t' \beta\} \Big] p(\beta)
% \]
% where $\kappa_t = a_t - b_t / 2$.  A single term from the product has
% \[
% \exp \Big( \kappa_t \psi_t - \frac{\omega_t}{2} \psi_t^2 \Big)
% \propto
% \exp \Big(-\frac{1}{2} (\kappa_t / \omega_t - \psi_t)^2 \omega_t \Big),
% \]
% which is identical to the likelihood of an observation $z_t = \kappa_t /
% \omega_t$ drawn from
% \[
% z_t \sim N(\vx_t' \beta, \omega_{i}^{-1}).
% \]
\[
p(\betap | \oomega, \yy) \propto \Big[ \prod_{t=1}^T \exp \Big(-\frac{\omega_t}{2}
\Big(\frac{\kappa_t}{\omega_t} - \psi_t\Big)^2 \Big) \Big] p(\betap), \; \psi_t
= \vx_t' \bbeta_t,
\]
where $\kappa_t = a_t - b_t / 2$.  A single term from the product above is
identical to the likelihood of a pseudo-data point $z_t = \kappa_t / \omega_t$
drawn from $z_t \sim N(\psi_t, 1/\omega_t)$.  Thus, if $p(\betap)$ specifies
that $\betap$ is a Gaussian AR(1) process, then sampling the complete
conditional for $\betap$ is equivalent to sampling $(\betap | \{z_t\}_{t=1}^T)$
from the DLM
\[
\begin{cases}
  z_t = \psi_t + \nu_t, & \nu_t \sim N(0, 1/\omega_t) \\
  \psi_t = \vx_t' \bbeta_t \\
  \bbeta_t = \bmu + \bPhi (\bbeta_{t-1} - \bmu) + \vep_t, & \vep_t \sim N(0, \bW).
\end{cases}
\]

Collecting the complete conditional for $\betap$ and for $\oomega$ leads to
posterior simulation by Gibbs sampling: draw $(\betap|\oomega, \yy)$ using the
FFBS algorithm and draw $(\oomega | \betap, \yy)$ by taking independent samples
of $\omega_t \sim \PG(b_t, \psi_t)$ for $t=1, \ldots, T$.
\citet{polson-etal-2013} describe how to sample from $\PG$ distributions and
implement this sampler in the \texttt{R} package \texttt{BayesLogit}
\citep{bayeslogit-2013}.  Sampling any hyperparameters, like the autocorrelation
coefficient of the AR(1) process or the innovation variance, follows using
standard conjugate or MCMC techniques.

\begin{example}
  Suppose that one observes binomial outcomes $y_t \sim \text{Binom}(n_t, q_t)$
  for $t=1, \ldots, T$.  Letting $\psi_t$ be the log-odds, the data generating
  distribution is
  \[
  p(y_t | \psi_t) = c(y_t) \frac{\exp(\psi_t)^{y_t}}{(1+\exp({\psi_t}))^{n_t}}.
  \]
  Thus the complete conditional $(\betap | y, \omega)$ may be simulated by using
  forward filter backward sampling with pseudo-data $z_t = \kappa_t / \omega_t$
  where $\kappa_t = y_t - n_t / 2$.
\end{example}


\begin{example}
  Suppose that one observes counts according to $y_t \sim \NB(d, q_t)$ for
  $t=1, \ldots, T$, where $d$ is the number of failures before observing $y_t$
  successes, also interpreted as a positive real-valued dispersion coefficient,
  and $q_t$ is the probability of observing a success.  Letting $\psi_t$ be the
  log-odds, the data generating distribution is
  \[
  p(y_t | \psi_t) = c(y_t, d) \frac{\exp({\psi_t})^{y_t}}{(1+\exp({\psi_t}))^{y_t+d}}.
  \]
  In negative binomial regression, it is common to model the log-mean, $\lambda_t
  = \psi_t + \log(d) = \vx_t' \bbeta_t$, instead of the log-odds.  This requires only
  a slight modification.  Following the work above, the complete conditional
  $(\omega_t | \bbeta_t, d)$ is $\PG(b_t, \psi_t)$ where $b_t = y_t + d$.  However,
  the DLM used to estimate $\betap$ is now
  % \[
  % \begin{cases}
  %   z_t = \psi_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
  %   \psi_t = \vx_t \bbeta_t - \log(d) \\
  %   \bbeta_t = \mu + \bPhi (\bbeta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W)
  % \end{cases}
  % \]
  % where $z_t = \kappa_t / \omega_t$ and $\kappa_t = (y_t - d_t) / 2$.
  \[
  \begin{cases}
    z_t = \lambda_t + \nu_t, & \nu_t \sim N(0, 1/\omega_t) \\
    \lambda_t = \vx_t' \bbeta_t \\
    \bbeta_t = \bmu + \bPhi (\bbeta_{t-1} - \bmu) + \vep_t, & \vep_t \sim N(0, \bW)
  \end{cases}
  \]
  where $z_t = \kappa_t / \omega_t + \log(d)$ and $\kappa_t = (y_t - d) / 2$.
\end{example}

% To estimate $d$ in dynamic negative binomial regression one may use a
% Metropolis-Hastings step (Appendix \ref{sec:sample-d}).

% \subsection{AR(1) Example}

\begin{figure}
  \begin{center}
    \includegraphics[width=5.5in]{nb-ar1-tophalf.pdf}
    \caption{\label{fig:nb-ar1} Incidence of influenza-like illness in Texas,
      2008--11, together with the estimated mean from the negative-binomial AR(1)
      model.  The blanks in weeks 21-41 correspond to missing data.  The grey lines
      depict the upper and lower bounds of a 95$\%$ predictive interval.}
  \end{center}
\end{figure}

As an initial illustration, we fit a negative-binomial AR(1) model to four years
(2008--11) of weekly data on flu incidence in Texas, collected from the Texas
Department of State Health Services.  Let $y_t$ denote the number of reported
cases of influenza-like illness in week $t$.  We assume that these counts follow
a negative-binomial model, which will allow over-dispersion relative to the
Poisson.
% \begin{eqnarray*}
% %   y_t &\sim& \mbox{NB}(h, q_t) \; , \quad q_t= \frac{e^{\psi_t}}{1+e^{\psi_t}} \\
%   \psi_t &=& \alpha + \gamma_t  \; , \quad \gamma_t = \phi \gamma_{t-1} + \epsilon_t \\
%   \epsilon_t &\sim& N(0, \sigma^2) \, .
% \end{eqnarray*}
Figure \ref{fig:nb-ar1} shows the results of the fit. For simplicity, we assumed
an improper uniform prior on the dispersion parameter $d$, and fixed $\phi$ and
$\sigma^2$ to $0.98$ and 1, respectively, but it is straightforward to place
hyper-priors upon each parameter, and to sample them in a hierarchical fashion.
It is also straightforward to incorporate fixed effects in the form of
regressors.

% We emphasize that there are many ways to handle the over-dispersion present in
% this data set, and that we do not intend our model to be taken as a definitive
% analysis.  We merely intend it as a proof of concept, showing how various
% aspects of Bayesian hierarchical modeling---in this case, a simple AR(1)
% model---can be combined routinely with binomial likelihoods using the
% \Polya-Gamma scheme.

\section{Comparison}

% Dynamic versions of binomial logistic regression and negative binomial
% regression are common models.  Hence, it is important to understand which
% methods work best for each model.

Since Markov Chains generate correlated samples we compare methods by measuring
how fast each procedure produces \emph{nearly} independent samples, that is we
measure the effective sampling rate (ESR).  To that end, we employ the effective
sample size (ESS), which approximates the number of ``independent'' draws
produced by a sample of $M$ correlated draws.  One may view the ESS as an
estimate of the number of samples produced after having thinned the $M$
correlated samples so that remaining draws appear independent.  From
\citet{holmes-held-2006}, the effective sample size is
\[
ESS_{it} = M / \Big( 1 + 2 \sum_{k=1}^\ell \rho_k(\beta_{it}) \Big)
\]
where $\rho_k(\beta_{it})$ is the $k$th lagged autocorrelation of the chain
corresponding to the $i$th component of $\bbeta_t$.  The effective
sampling rate is the ratio of the effective sample size to the time taken to
generate the post-burn-in samples; thus, it measures the rate at which the
Markov Chain produces independent draws after initialization and burn-in.

To mitigate MCMC sample variation, 10 batches of 12,000 samples are taken and
the last 10,000 draws are recorded.  For batch $m$, we compute the
component-wise effective sample size $ESS_{it}^{m}$ corresponding to the
univariate chain for $\beta_{it}$.  Taking the mean over batches produces the
average, component-wise effective sample size $\overline{ESS}_{it}$ and, after
normalizing each batch by time, the average, component-wise effective sampling
rate $\overline{ESR}_{it}$.  Following,
\citet{fruhwirth-schnatter-fruhwirth-2010}, the primary metric of comparison is
the median effective sampling rate,
\[
\text{med} \; \Big\{ \overline{ESR}_{it} : i=1, \ldots, P; \;  t=1, \ldots, T \Big\}.
\]

We consider synthetic data sets with a variety of characteristics.
% since this is an empirical, as opposed to theoretical assessment.
For dynamic binomial logistic regression, we consider log odds of the form
$\alpha + \vx_t' \bbeta_t$ where $\alpha$ is a scalar and $\{\bbeta_t\}_{t=1}^T$
is a 2-dimensional AR(1) process with autocorrelation coefficient $\bPhi = 0.95
I_2$.  Four different synthetic data sets are constructed, allowing the
covariates $\{\vx_t\}_{t=1}^T$ to have lots or little of correlation and letting
the responses $y_t$ arise from $\text{Binom}(n, q_t), t=1, \ldots, T$ with
either $n=1$ or $n=20$ trials.  The setup is almost identical for dynamic
negative binomial regression except that we model the log-mean as $\alpha +
\vx_t' \bbeta_t$ and consider responses with $\alpha = \log(10)$ or $\alpha =
\log(100)$ corresponding to average count sizes of roughly 10 or 100.  Further
details may be found in the Appendix.

Some caution is warranted when comparing methods as the effective sampling rate
is sensitive to a procedure's implementation and the hardware on which it is
run.  (Supplementary files have been provided so that users may examine the
performance of these methods on their own.)  The routines are written primarily
in R.  We use code from \citet{binomlogit-2012} and
\citet{fruhwirth-schnatter-book-2007} for the discrete mixture of normals
methods.  All benchmarks are carried out on an Ubuntu machine with Intel Core
i5-3570 3.4GHz processor and 8GB of RAM.  Some computations were burdensome in
R, and hence we wrote wrappers to C to speed up the MCMC simulations.  In
particular, both data augmentation methods implement forward filtering and
backward sampling using a C wrapper.  The \Polya-Gamma technique calls C code to
sample random variates using version 0.2-4 of the \texttt{BayesLogit} package
\citep{bayeslogit-2013}.  The conjugate updating and backwards sampling of
\citet{migon-etal-2013} is done in C.  Having a C wrapper to forward filter and
backwards sample is particularly important, as our C implementation is much
faster than the corresponding R code.  Were we to use the slower R version, our
results would favor the \Polya-Gamma method, as it has better effective sample
sizes and would spend less time, proportionally, sampling the latent random
variables.

\citet{polson-etal-2013} outline the expected performance of the \Polya-Gamma
data augmentation technique, which depends heavily on how quickly one can
generate \Polya-Gamma random variates.  In their original algorithm, sampling
\Polya-Gamma random variates from $\PG(b,\psi)$ is fast when $b$ is a small
integer, but slow when $b$ is large. \citet{windle-thesis-2013} provides an
improved method for sampling $\PG(b,\psi)$; however sampling large $b$ is still
slower than sampling small $b$.  These differences in computational cost are
important, as one must sample $\omega_t \sim \PG(b_t, \psi_t), t=1,\ldots,T$,
for each MCMC iteration under \Polya-Gamma data augmentation.  In binomial
logistic regression, $b_t = n_t$ where $n_t$ is the number of trials at each
response $y_t$.  Hence, when there are few trials, as is usually the case, the
PG method will do well.  For negative binomial regression $b_t = y_t + d_t$
where $y_t$ is the response and $d_t$ is the dispersion.  Thus, larger average
counts sizes will lead to longer MCMC simulations.

In general, we find these principles to hold.  The \Polya-Gamma data
augmentation technique performs well for dynamic binomial logistic regression
when the number of trials of the response is small, showing a roughly 25\%
better ESR for binary logistic regression than \citet{fussl-etal-2013}; however,
\citet{fussl-etal-2013} does slightly better when the number of trials is large.
Similarly, the \Polya-Gamma technique outpaces
\citet{fruhwirth-schnatter-etal-2009} in negative binomial regression when the
average number of counts is small, but loses when the average number of counts
is large.  The Metropolis-Hastings approach of \citet{migon-etal-2013} does the
worst in all of the tests.  Part of its poor performance is due to a non-linear
solve that must be made $T$ times when forward filtering.  We did not attempt to
optimize the performance of this non-linear solver, and hence some improvement
may be possible, though the disparity in ESRs suggests that any improvement
would not be enough to compete with either data augmentation approach.  As a
check upon \citet{migon-etal-2013}, we also implemented a Metropolis-Hastings
sampler that draws blocks of disturbances using Laplace approximations.  This
fared worse still.

Of note, the \Polya-Gamma method almost always has superior effective sample
sizes.  Hence faster \Polya-Gamma samplers could push the \Polya-Gamma data
augmentation technique to the top for all of the models considered.  Table
\ref{tab:benchmark-summary} provides a summary of the benchmarks; details may be
found in the Appendix.

% Summary Table
% -------------------------------------------------------------------------------
\begin{table}

  \begin{center}
    \small
    \begin{tabular}{l l c c}
      \multicolumn{4}{c}{Dynamic Binom.\ Logistic Reg.} \\
      \hline
      & & Est.\ & ESR \\
      n & f & AR & PG/dRUM \\
      \hline
      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-wout.ar
      % PG / dRUM
      % $1$ & low & no & 1.26 \\
      $1$ & low & no & 1.26 \\
      % PG / CUBS 47.5399
      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-with.ar
      % PG / PG 1
      % PG / dRUM
      % $1$ & low & yes & 1.29 \\
      $1$ & low & yes & 1.30 \\
      % PG / CUBS 287.291

      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-wout.ar
      % PG / PG 1
      % PG / dRUM
      % $1$ & high & no & 1.25 \\
      $1$ & high & no & 1.23 \\
      % PG / CUBS 36.9012
      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-with.ar
      % PG / PG 1
      % PG / dRUM
      % $1$ & high & yes & 1.26 \\
      $1$ & high & yes & 1.23 \\
      % PG / CUBS 175.964

      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-wout.ar
      % PG / PG 1
      % PG / dRUM
      % $20$ & low & no & 0.52 \\
      $20$ & low & no & 0.91 \\
      % PG / CUBS 29.4906
      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-with.ar
      % PG / PG 1
      % PG / dRUM
      % $20$ & low & yes & 0.57 \\
      $20$ & low & yes & 0.98 \\
      % PG / CUBS 49.9306

      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-wout.ar
      % PG / PG 1
      % PG / dRUM
      % $20$ & high & no & 0.48 \\
      $20$ & high & no & 0.83 \\
      % PG / CUBS 13.323
      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-with.ar
      % PG / PG 1
      % PG / dRUM
      % $20$ & high & yes & 0.58
      $20$ & high & yes & 0.98
      % PG / CUBS 38.9124
    \end{tabular}
    % 
    \hspace{12pt}
    % 
    \begin{tabular}{l l c c}
      \multicolumn{4}{c}{Dynamic Neg.\ Binomial Reg.} \\
      \hline
      & & Est.\ & ESR \\
      $\mu$ & f & AR & PG/FS \\
      \hline
      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-wout.ar
      % PG / PG 1
      % PG / FS
      $10$ & low & no & 1.03 \\
      % PG / CUBS 42.3582
      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-with.ar
      % PG / PG 1
      % PG / FS
      $10$ & low & yes & 1.86 \\
      % PG / CUBS 59.1934

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-wout.ar
      % PG / PG 1
      % PG / FS
      $10$ & high & no & 1.15 \\
      % PG / CUBS 20.6336
      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-with.ar
      % PG / PG 1
      % PG / FS
      $10$ & high & yes & 1.06 \\
      % PG / CUBS 18.0189

      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-wout.ar
      % PG / PG 1
      % PG / FS
      $100$ & low & no & 0.76 \\
      % PG / CUBS 103.273
      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-with.ar
      % PG / PG 1
      % PG / FS
      $100$ & low & yes & 0.82 \\
      % PG / CUBS 102.229

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-wout.ar
      % PG / PG 1
      % PG / FS
      $100$ & high & no & 0.76 \\
      % PG / CUBS 34.7778
      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-with.ar
      % PG / PG 1
      % PG / FS
      $100$ & high & yes & 0.78 \\
      % PG / CUBS 69.6132

    \end{tabular}
  \end{center}

  \begin{center}
    \caption{\label{tab:benchmark-summary} A summary of the benchmarking
      results. ESR PG/dRUM and ESR PG/FS report the ratio of the median
      effective sampling rate for the PG method compared to the best
      alternative, which in both cases is the discrete mixture of normal
      approaches \citep{fussl-etal-2013, fruhwirth-schnatter-etal-2009}.  A
      higher ratio corresponds to the PG method doing better.  $n$ corresponds
      to the number of trials for the binomial response while $\mu$ corresponds
      to the approximate mean for the negative binomial response.  $f$
      determines whether there is a low or high amount of correlation between
      the covariates.  Est.\ AR indicates whether the parameters of the AR
      process were estimated or not.  }

  \end{center}


\end{table}

\section{Conclusion}

Considering efficiency, we suggest using the \Polya-Gamma method as the default
approach for dynamic binomial logistic regression and the method of
\citet{fruhwirth-schnatter-etal-2009} as the default method for dynamic negative
binomial regression.  There are two exceptions to this rule: (1) if the number
of trials in the dynamic binomial logistic regression is large, it may be better
to use the method of \citet{fussl-etal-2013} and (2) if the average count size in
the dynamic negative binomial regression is small, it may be better to use the
\Polya-Gamma data augmentation approach.

Usability is as important as efficiency, if not more so, and from that vantage
we recommend the \Polya-Gamma method in all cases.  The \Polya-Gamma approach,
adhering to the first principle of engineering, is simple, in both concept and
practice.  Conceptually, one need only understand an algebraic relationship
(Equation \ref{eqn:pg-cancellation}) to follow its derivation, unlike the
discrete mixture of normals approach, which is based upon the data generating
process and hence the mixing of distributions.  Practically, one need only be
comfortable with heteroscedastic Gaussian likelihoods---familiar territory for
most everyone---as the \Polya-Gamma auxiliary variables enter into the Gaussian
form in terms of the variance and do not require another latent quantity like
\citet{fruhwirth-schnatter-etal-2009} and \citet{fussl-etal-2013}.  These
assertions hold, not just for the AR(1) prior presented previously, but for
\emph{any} bespoke linear evolution equations, such as dynamic factor models.
Since the requisite Gaussian machinery is well established, the only challenge
is sampling the conditional distribution of the latent variables, but this
reduces to simulating \Polya-Gamma random variates, which can be done using the
\texttt{BayesLogit} R Package.  Thus, the only novel aspect of implementing the
\Polya-Gamma data augmentation approach is one line of code within an R script.
(The C code used by the \texttt{BayesLogit} package is freely available, so this
applies equally well to any scripting language that can make calls to external C
routines.)  Further, for all but one benchmark, the \Polya-Gamma technique has
superior effective sample size, so that more efficient \Polya-Gamma samplers
will diminish any gaps in performance.  When improved samplers arise, users of
the \Polya-Gamma technique need not make any changes to their code, they can
simply download the latest version of the R package.  The overwhelming
simplicity and adaptability of the method makes it a good choice regardless of
its efficiency.

% \begin{tabular}{l r r r r r r r r }
%   \hline
%   \multicolumn{9}{c}{Tokyo Rainfall} \\
%   \hline
%   Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\
%   CUBS  &   233.98 &     1.00 &   652.86 &   761.98 &   910.62 &     2.79 &     3.26 &     3.89 \\
%   PG  &    21.72 &     1.00 &  4352.80 &  7735.08 & 10081.46 &   200.43 &   356.18 &   464.22 \\
%   dRUM  &    20.60 &     1.00 &  1627.61 &  3649.46 &  5428.93 &    79.03 &   177.19 &   263.60
% \end{tabular}
% \end{table}

% \begin{table}
%   \centering
%   \begin{tabular}{l r r r r r r r r }
%     \hline
%     \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta$.} \\
%     \hline
%     Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
%     PG  &    22.59 &     1.00 &   7367.68 &   9229.91 &  10189.37 &    326.10 &    408.52 &    450.97 \\
%     dRUM  &    23.19 &     1.00 &   2759.83 &   7260.46 &   9967.66 &    118.99 &    313.04 &    429.77
%   \end{tabular}

%   \begin{tabular}{l r r r r r r r r }
%     \hline
%     \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta$.} \\
%     \hline
%     Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
%     PG  &    22.52 &     1.00 &   8494.04 &   9662.06 &  10225.35 &    377.13 &    428.98 &    454.00 \\
%     dRUM  &    23.17 &     1.00 &   4963.68 &   8105.02 &   9936.46 &    214.26 &    349.86 &    428.91
%   \end{tabular}

% \end{table}


% \begin{table}
%   \centering
%   \begin{tabular}{l r r r r r r r r }
%     \hline
%     \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta, \phi, W$.} \\
%     \hline
%     Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
%     PG  &    25.04 &     1.00 &    643.23 &   6531.34 &  10420.88 &     25.69 &    260.82 &    416.13 \\
%     dRUM  &    25.73 &     1.00 &    430.79 &   4490.34 &  10124.47 &     16.74 &    174.55 &    393.48
%   \end{tabular}

%   \begin{tabular}{l r r r r r r r r }
%     \hline
%     \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta, \phi, W$.} \\
%     \hline
%     Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
%     PG  &    25.16 &     1.00 &    988.83 &   5654.23 &  10337.07 &     39.31 &    224.75 &    410.87 \\
%     dRUM  &    25.84 &     1.00 &   1040.12 &   5496.94 &   9760.64 &     40.25 &    212.74 &    377.75
%   \end{tabular}

% \author{ %
%   Jesse Windle$^{a}$, %
%   Carlos M. Carvalho$^{b}$, %
%   James G. Scott$^{b}$, and %
%   Liang Sun$^{c}$ %
%   \footnote{ %
%     Jesse Windle, corresponding author, is a Post Doctoral Associate in the
%     Department of Statistical Science, Duke University, Box 90251, Durham NC
%     27708-0251 (E-mail: jesse.windle@stat.duke.edu).
%     % 
%     Carlos M. Carvalho is Associate Professor of Statistics, Department of
%     Information, Risk, and Operations Management, The University of Texas at Austin,
%     2110 Speedway Stop B6500 Austin, TX 78712 (E-mail:
%     carlos.carvalho@mccombs.utexas.edu).
%     % 
%     James G. Scott is Assistant Professor of Statistics, Department of Information,
%     Risk, and Operations Management, The University of Texas at Austin,
%     2110 Speedway Stop B6500 Austin, TX 78712 (E-mail:
%     james.scott@mccombs.utexas.edu).
%     % 
%     Liang Sun is a graduate student in the Department of Operations Research \&
%     Industrial Engineering, The University of Texas at Austin, 204 E. Dean Keeton
%     St.  Austin, Texas 78712 (E-mail: sallylia@gmail.com).
%     % 
%   }
%   \\ \\
%   $^{a}$ Department of Statistical Science, Duke University \\
%   $^{b}$ Department of IROM, The University of Texas at Austin \\
%   $^{c}$ Department of ORIE, The University of Texas at Austin %
%   % \\
% }

% Local Variables:
% TeX-master: "badriver.tex"
% End: