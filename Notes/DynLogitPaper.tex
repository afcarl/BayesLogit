\documentclass[11pt]{article}

\input{commands}

\usepackage{natbib}

\newcommand{\Polya}{P\'{o}lya}
\newcommand{\PG}{\text{PG}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Ga}{\text{Ga}}
\newcommand{\NB}{\text{NB}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\eqd}{\,{\buildrel d \over =}\,}
\newcommand{\KS}{\text{KS}}

\newcommand{\point}{\noindent $\bullet$ }

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
%\parindent 0in
%\parskip 12pt

% Set Fancy Style
%\pagestyle{fancy}

%\lhead{Left Header}
%\rhead{Right Header}

\title{Dynamic Generalized Linear Models using \Polya-Gamma augmentation}
\author{Carlos M. Carvalho, James G. Scott, Liang Sun, Jesse Windle}

\begin{document}

\maketitle
\tableofcontents

\newpage

\section{Introduction}

\subsection{Dynamic Generalized Linear Models}

This paper extends the work of \cite{polson-etal-2012} for static generalized
linear models with logistic likelihoods to dynamic generalized linear models
with logistic likelihoods.  

Generalized linear models (GLMs) arise when the response variable is categorical
or integral as opposed to continuous.  In that case, one models the distribution
of the response $(y_i | \psi_i)$ by assuming that $\psi_i$ can be written as a
linear combination of some predictors, $x_i \beta$.  The parameter $\psi_i$ may
not be the most common way to parameterize the distribution of the reponse $y_i$
and hence one often starts with a representation $(y_i | \mu_i)$ and then
transforms $\mu_i$ to $\psi_i$ under some function $g$ that maps to a coordinate
system in which it is reasonable to use the linear model $\psi_i = x_i \beta$.
Such models may be summarized as
\[
\begin{cases}
y_i \sim \bbP(\mu_i), & i=1,\ldots, N \\
\psi_i = g(\mu_i) = x_i \beta.
\end{cases}
\]
One may proceed by the usual rules of Bayesian inference to calculate the
posterior distribution of $\beta$; however, the exact form of the this
distribution is often unrecognizable and not easy to simulate.  Data
augmentation schemes are one way to remedy this problem.

Dynamic generalized linear models extend the static framework to the case that
the regression coefficient changes with the index of the observation, e.g. time.
In that case the model above becomes
\[
\begin{cases}
  y_t \sim \bbP(\mu_t), & t=1, \ldots, T \\
  \psi_t = g(\mu_t) = x_t \beta_t \\
  \beta_t \sim \text{AR}(1).
\end{cases}
\]
In general, $\beta_t$ may be any Gaussian time series.  Again, data augmetation
techniques allow one to simulate the posterior distribution of $\{\beta_t\}$;
however, the computational burdern is exacerbated in the dynamic case since
instead of simulating a $p$-dimensional regression coefficient one must now
simulate a $p \times T$-dimensional process.

The method we advocate applies whenever exact posterior simulation for dynamic
linear models when the conditional density $p(y_i | \psi_i)$ generates a
likelihood, in $\psi$, of the form
\[
\ell(\psi | y) \propto \prod_{t=1}^T
\frac{(e^{\psi_i})^{a_i}}{(1+e^{\psi_i})^{b_i}}.
\]
We focus specificlaly on dynamic binary and binomial logistic regression and
dynamic negative binomial regression for count data.

\subsection{Previous Work}

Mention work in static case? -- Data augmentation vs. MH in static vs. dynamic.

Bayesian inference for dynamic generalized linear models dates back to at least
\cite{west-etal-1985} who used conjugate updating and linear Bayes filtering to
filter the dynamic regression coefficients of dynamic generalized linear models
when the observation equations $(y_i | \mu_i)$ comes from an exponential family.
More recently, \cite{ravines-etal-2006} built upon that work by adding a
Metropolis-Hastings step to sample from the exact complete conditional
distribution of $\beta$ instead of using an approximation.

Fruhwirth-Schnatter and her collabarators use a discrete mixture of normals
approximation for posterior inference in a variety of static settings, including
binary regression, multinomial regression, Poisson regression, and negative
binomial regression for count data--all of which are or can be extended to the
dynamic setting \citep{fruhwirth-schnatter-fruhwirth-2007,
  fruhwirth-schnatter-fruhwirth-2010, fruhwirth-schnatter-wagner-2006,
  fruhwirth-schnatter-etal-2009}.

\begin{itemize}
\item \cite{west-etal-1985}

  For exponential families.

\item \cite{ravines-etal-2006}

  For exponential families.  Specific example for binomial, poisson, Gamma
  transfer function.

\item \cite{fruhwirth-schnatter-fruhwirth-2007}

  For binary logistic, multinomial logistic, though give binomial logistic example.

\item \cite{fruhwirth-schnatter-wagner-2006}

  For count data - Poisson regression.

\item \cite{fruhwirth-schnatter-etal-2009}

  Shows how to reduce the number of latents.  Can do dynamic Poisson regression.
  Can also do dynamic negative binomial regression.

  In both cases, she suggests using a normal mixture representation of the log
  gamma distribution.  But this turns out to be somewhat annoying to do since
  the shape parameter changes.  Hence you need an entire family of log-gamma
  distributions.

\end{itemize}

\subsection{Contributions of the Paper}

This paper examines the relative merits of the three approaches discussed
above--Linear Bayes with conjugate updating and Metroplis-Hastings, a discrete
mixture of normals approximation, and the \Polya-Gamma data augmentation
scheme--for dynamic generalized linear models.  We show that on an effective
sampling rate basis, the \Polya-Gamma data augmentation scheme is as good or
better than its competitors.

\subsection{Outline}

Section () shows how the \Polya-Gamma data augmentation trick applies to dynamic
binary logistic regression and dynamic negative binomail regression and their
corresponding Gibbs samplers.  Section () discusses the benchmarking methods and
results.  Section () concludes.

\section{\Polya-Gamma Data Augmentation}

Suppose that the data generating distribution $(y_t | \psi_t)$ has a conditional
distribution of the form
\[
p(y_t | \psi_t) = c(y_t) \frac{(e^{\psi_t})^{a_t}}{(1+e^{\psi_t})^{b_t}}
\]
and that $a_t$ and $b_t$ are functions of the data, but not $\beta_t$.  By
recognizing that
\[
\cosh^{-b_t}(\psi_t / 2) = \int_0^\infty e^{-\omega_t \psi_t^2 / 2} p(\omega_t |
b_t, 0) d \omega_t,
\]
where $p(\omega_t | b_t, 0)$ is the density of a $\PG(b_t, 0)$ random variate,
one may introduce the auxiliary variable $\omega_t$ so that
\[
p(y_t, \omega_t | \psi_t) = c(y_t) 2^{-b_t} e^{\kappa_t \psi_t -\omega_t
  \psi_t^2/2} p(\omega_t | b_t, 0)
\]
where $\kappa_t = a_t - b_t / 2$.  The likelihood for $(\psi_t | y_t, \omega_t)$
is proportional to the likelihood arising from the observation
\[
z_t = \psi_t + \ep_t, \; \ep_t \sim N(0, 1/\omega_t)
\]
where $z_t = \kappa_t / \omega_t$.  Thus the posterior distribution of $(\beta |
\omega)$ can be calculated using a filter forward backwards sampler under the
DLM
\begin{equation}
\label{eqn:dlm}
\begin{cases}
z_t = \psi_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
\psi_t = x_t \beta_t \\
\beta_t \sim \text{AR}(1).
\end{cases}
\end{equation}
The posterior distribution of $(\omega | \beta)$ is $\prod_{t=1}^T \PG(b_t,
\psi_t)$ as constructed by \cite{polson-etal-2012}.

\subsection{Example: Dynamic Binomial Logistic Regression}

Suppose one observes a binomial outcome $y_t$ where $P(y_t = 1) = p_t$ out of
$n_t$ trials at time $t$.  Letting $\psi_t$ be the log-odds, the data generating
distribution is
\[
p(y_t | \psi_t) = c(y_t) \frac{(e^\psi_t)^{y_t}}{(1+e^{\psi_t})^{n_t}}.
\]
Thus the complete conditional $(\{\beta_t\} | \omega)$ may be simulated by the
FFBS the DLM (\ref{eqn:dlm}) where $z_t = \kappa_t / \omega_t$ and $\kappa_t =
y_t - n_t / 2$ and the complete conditional $(\omega | \beta)$ may be simulated
by sampling $\PG(n_t, \psi_t)$ for $t=1, \ldots, T$.


\subsection{Example: Dynamic Negative Binomial Regression}

Suppose that one observes counts according to $y_t \sim \NB(d, p_t)$ where $d$
is the number of ``failures'' before observing $y_t$ successes and $p_t$ is the
probability of observing a success.  Letting $\psi_t$ be the log-odds, the data
generating distribution is
\[
p(y_t | \psi_t) = c(y_t, d) \frac{(e^{\psi_t})^{y_t}}{(1+e^{\psi_t})^{y_t+d}}.
\]
In negative binomial regression, it is common to model the log-mean, $\zeta_t =
\psi_t + \log(d) = x_t \beta_t$, instead of the log-odds.  This requires only a
slight modification.  Following the work above, the complete conditional
$(\omega_t | \beta_t, d)$ is $\PG(b_t, \psi_t)$ where $b_t = y_t + d_t$.
However, the DLM used to estimate $\{\beta_t\}$ is now
\[
\begin{cases}
z_t = \psi_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
\psi_t = x_t \beta_t - \log(d) \\
\beta_t = \text{AR}(1).
\end{cases}
\]
One may estimate $d$ as well using, for instance, a Metropolis-Hastings step.
We include a brief disucssion of that step in the appendix.

\section{Comparison}



\section{Conclusion}

\appendix

\section{Sampling $d$ in dynamic negative binomial regression}

We may sample $d$ by an indepedent Metropolis-Hastings or a random-walk
Metropolis-Hastings.  In either case, we need the log-likelihood of $d$.

The conditional density for a single term $y_t \sim N(d, p_t)$ is
\[
\frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} p_t^{y_t} (1-p_t)^{d} ,
\]
or 
\[
\frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} 
\Big( \frac{\mu_t}{\mu_t + d} \Big)^{y_t} \Big( \frac{d}{\mu_t + d} \Big)^d
\]
in terms of the mean $\mu_t$.  Since we chose to model $\log \mu_t = x_t
\beta_t$ it will be easier to work with the latter version.  In that case, the
log-likelihood is
\[
\ell(d|\mu, y) = \sum_{t=1}^T [ \log \Gamma(y_t + d)  - \log \Gamma(d) ]  + 
\sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
\]

If we assume that $d$ is a natural number then we can rid ourselves of $T$
evaluations of the gamma function.  We may expand $\Gamma(y_t + d)$ and
$\Gamma(d)$ as products, in which case the log-likelihood is
\[
\ell(d|\mu, y) = \sum_{t=1}^T \sum_{j=0}^{y_t-1} \log(d + j) + 
\sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
\]
We can rewrite the first term of the sum as
\[
\sum_{t=1}^T \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{k-1} \log(d+j) \one \{y_t = k\}
\]
which becomes
\[
\sum_{k=1}^{\max(y_t)} n_k \sum_{j=0}^{k-1} \log(d+j)
\]
where $n_k = \{ \# y_t = k \}$.  We have thus
\begin{align*}
\sum_{k=1}^{\max(y_t)} \sum_{j=0}^{\max(y_t)-1} n_k \log(d+j) \one\{j < k\}
& = 
\sum_{j=0}^{\max(y_t)-1} \log(d+j) \sum_{k=j+1}^{\max(y_t)} n_k \\
& = \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j
\end{align*}
where $G_j = \{ \# y_t > j \}$.  Notice that $1-G_j = F(j) = \{\# y_t \leq j\}$
and that $G$ may be pre-processed.  Hence the log-likelihood is
\[
\sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j + 
\sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
\]
Preprocessing $G$ saves us from having to compute a doubly indexed summation.


% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliography{bayeslogit}{}
\bibliographystyle{abbrvnat}

\end{document}