\documentclass[11pt]{article}

\input{commands}

\usepackage{natbib}

\newcommand{\Polya}{P\'{o}lya}
\newcommand{\PG}{\text{PG}}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Ga}{\text{Ga}}
\newcommand{\NB}{\text{NB}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\eqd}{\,{\buildrel d \over =}\,}
\newcommand{\KS}{\text{KS}}
\newcommand{\bbeta}{\boldsymbol{\beta}}

\newcommand{\point}{\noindent $\bullet$ }

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
%\parindent 0in
%\parskip 12pt

% Set Fancy Style
%\pagestyle{fancy}

%\lhead{Left Header}
%\rhead{Right Header}

\title{Dynamic Generalized Linear Models using \Polya-Gamma augmentation}
\author{Carlos M. Carvalho, James G. Scott, Liang Sun, Jesse Windle}

\begin{document}

\maketitle
\tableofcontents

\newpage

\section{Introduction}

In this paper, we show how the \Polya-Gamma data augmentation technique of
\cite{polson-etal-2012} may be used for posterior inference in dynamic
generalized linear models with logistic likelihoods.  Further, we perform an
suite of empirical benchmarks to show that the \Polya-Gamma data augmentation
technique out-performs other MCMC methods for binary logistic regression.

Generalized linear models consider responses $y_t \sim P(\psi_t)$ that depend on
the linear combination of predictors $\psi_t = x_t' \beta$ in a (possibly)
non-linear way \citep{mccullagh-nelder-1989}.  Extensions to the dynamic case
are straightforward via a dynamic regression coefficient: $\psi_t = x_t'
\beta_t$, where one must specify the dynamics of $\beta_t$ to complete the model
.  Unlike Gaussian, linear models, most likelihoods found in generalized linear
models do not lead to convenient posterior distributions for $\beta$ or
$\{\beta_t\}_{t=0}^T$.  Thus, when generating posterior samples one must appeal
to Metropolis-Hastings, Gibbs sampling with data augmentation, or some other
MCMC method.

The challenge of generating posterior samples is exacerbated for dynamic models
since there are many more unknown quantities to estimate than in the static
case, which makes, for instance, direct extensions of Metropolis-Hastings
samplers difficult.  This is the case for GLMs with logistic likelihoods, a
subclass of GLMs that have probability mass functions of the form
\[
p(y | \psi) \propto h(y) \prod_{i=1}^n
\frac{(e^{\psi_i})^{a_i(y)}}{(1+e^{\psi_i})^{b_i(y)}} \, ,
\]
and for which \Polya-Gamma data augmentation is applicable.  Such likelihoods
arise when modeling proportions on the log-odds scale, which is often convenient
for categorical or count data.  For instance, two common models, binomial
logistic regression and negative binomial regression, fit within this framework.
Dynamic versions of these models are used in economics [McFadden], epidemiology
[cite Lauren], ecology [cite Lauren], and neuroscience [cite Jonathan and Larry]
amongst others.  Hence, it is important to know the most efficient methods for
generating posterior samples for such models.

This paper examines the relative merits of the three approaches for posterior
inference in dynamic generalized linear models (GDLM) with logistic likelihoods:
conjugate updating and backward sampling with a Metroplis-Hastings step
\citep{ravines-etal-2006}, forward filter and backwards sampling using a
discrete mixture of normals approximation data augmentation scheme
\citep{fruhwirth-schnatter-etal-2009, fussl-etal-2013}, and forward filter and
backwards sampling using the \Polya-Gamma data augmentation scheme.  We show
that the \Polya-Gamma technique has superior effective sample sizes compared to
the other methods and superior effective sampling rates in the case of binary
logistic regression.  In general the \Polya-Gamma method works well for dyanmic
binomial logistic regression or dynamic negative binomial regression when the
responses are small integers.  This is due to the way in which \Polya-Gamma
random variates are generated, which slows in proportion to the size of the
response.

The outline of the paper is as follows: Section 2 reviews other methods of
inference for dynamic regression for exponential familes; Section 3 shows how
one may use the \Polya-Gamma data augmentation technique in the dynamic setting;
Section 4 presents the benchmarking results; Section 5 concludes.

\section{Previous Efforts}

Bayesian inference for dynamic generalized linear models dates back to at least
\cite{west-etal-1985} who used conjugate updating with backwards sampling by
linear Bayes (CUBS) to sample the dynamic regression coefficients of DGLMs when
the observation $(y_i | \psi_i)$ comes from an exponential family; but their
method is only approximate.  Much effort has been devoted to developing exact
posterior samplers, though none has proved to be completely satisfactory.  A
primary goal of any such sampler is to sample states jointly, like the filter
forward backwards sampler (FFBS) of \cite{fruhwirth-schnatter-1994} and
\cite{carter-kohn-1994}, since jointly sampling states tends to result in less
autocorrelation than sampling the states component-wise, an approach suggested
by \cite{carlin-etal-1992} prior to the advent of the advent of the FFBS.
However, the FFBS procedure requires Gaussian, linear state-space evolution
equations and observation equations.  Without these assumptions, as is the case
with exponential families in general, the machinery of the FFBS breaks down.  To
resurrect the FFBS, one may approximate the posterior with some convenient
proposal density and then accept or reject using Metropolis-Hastings, or one may
use data augmentation so that, conditinoally, the observations and states are
generated by a DLM.  Neither method is gaurenteed to work well.

\cite{gamerman-1998} discusses various Metropolis-Hastings based approaches, all
of which rely on some Laplace-type approximation (and hence all of which can be
phrased as iteratively reweighted least squares \citep{wedderburn-1974}) for
generating proposals.  None of the various approaches is completely
satisfactory: component-wise proposals have decent acceptance rates, though high
autocorrelation between consecutive samples, while joint proposals suffer from
unacceptably small acceptance rates.  Gamerman's solution is to transform the
problem so that one samples the innovation variances componenent-wise using a
Laplace approximation with Metropolis-Hastings update, arguing that the new
coordinate system possesses less intrincinc correlation.  But this approach is
more computationally intensize since one must transform the proposals back to
the original coordinate system at each iteration to evaluate Metropolis-Hastings
acceptance probability.

\cite{shephard-pitt-1997} attempt to strike a balance between the
autocorrelation of consecutive samples and the acceptance probabilities of
proposed samples by drawing blocks of states.  Sampling in blocks reduces
autocorrelation while restraining the size of the blocks ensures a reasonable
Metropolis-Hastings acceptance probability.  However, their method still suffers
from autocorrelation between consecutive draws for the hidden states.

More recently, techniques have emerged that do generate joint draws of the
states.  \cite{ravines-etal-2006} built upon \cite{west-etal-1985} by adding a
Metropolis-Hastings step to sample the states exactly.  Though this at first
would seem like a poor choice due to high-dimensionality often encountered in
time series, they find that, in fact, the technique results in reasonable
acceptance rates unlike a global Lapalce approximation.  The fact that conjugate
update improves the Metropoli-Hastings proposal enough to allow for
``efficient'' joint draws is somewhat surprising.

Fr\"{u}hwirth-Schnatter and her colleagues have explored data augmentation
techniques that rely upon discrete mixtures of normals to arrive at
conditionally Gaussian posteriors in a variety of settings including binomial
logistic regression and multinomial regression
\citep{fruhwirth-schnatter-fruhwirth-2007, fruhwirth-schnatter-fruhwirth-2010,
  fussl-etal-2013}, Poisson regression \citep{fruhwirth-schnatter-wagner-2006,
  fruhwirth-schnatter-etal-2009}, and negative binomial regression for count
data \citep{fruhwirth-schnatter-etal-2009}.  All of these techniques may be used
in static and dynamic regressions.  While these methods work well, several rely
upon precomputing large tables of weights, means, and variances for the
components of a collection normal mixtures that approximate an entire family of
distributions.  Further all of the discrete mixture of normal techniques make
use of at least two layers of auxiliary variables.  One would prefer to avoid
many layers of latents since this inhibits traversing the posterior landscape.

Much of Fru\"{u}hwirth-Schnatter et al.'s work has been devoted to dynamic
binomial logistic regression. Initially,
\cite{fruhwirth-schnatter-fruhwirth-2007} employed the latent utilities
interpretation of \cite{mcfadden-1974}.  Later,
\citep{fruhwirth-schnatter-fruhwirth-2010} found a discrete mixture of normals
approach based upon \cite{holmes-held-2006} to be superior. The 2010 paper
includes an extensive comparison of various data augmentation and
Metropolis-Hastings methods in the static case, including evidence that the
discrete mixture of normals approximation outperforms \cite{holmes-held-2006}
due to its computation efficiency.  Most recently, \cite{fussl-etal-2013} put
forth another posterior sampler for binomial logistic regression using discrete
mixtures of normals that improves inference when the number of trials at each
observation is greater than one.  Much of this work can be traced back the
\cite{albert-chib-1993}, which provides a data augmentation approach to probit
regression and which has a dynamic analog.  We limit our comparison below to
\cite{fussl-etal-2013} since it appears to be the best choice for dynamic
binomial logistic regression within Fr\"{u}wirth-Schnatter et al.'s cornicopia
of methods.

DGLMs can be cast within the more general framework of non-linear non-Gaussian
state-space models.  \cite{geweke-tanizaki-2001} highlight the various works of
Kitigawa, Tanizaki, and Mariano, amongst others, and to filter, smooth, or
simulate states within this context using numerical integration, resampling, or
rejection sampling.  However, the more general setting does not provide more
insight into how one may jointly samples states in DGLMs.  Each of the
approaches they review is flawed: numerical integration does not work well for
any but the simplest settings, sampling marginally smoothed states using
sequential methods is time consuming, and rejection sampling may have poor
acceptance probabilities.  None of the methods cited by Geweke and Tanizaki are
useful for generating posterior samples of the states jointly, an extremely
desirable property.  Their solution is to sample the states component-wise using
a Laplace approximation and a Metropolis-Hastings step, which in the case of
exponential families returns us to the methods dicussed by
\citep{gamerman-1998}.  \cite{godsill-etal-2004} show how one may jointly sample
states using particle filters, a potentially competitive approach that we do not
consider here.

\section{\Polya-Gamma Data Augmentation}

The \Polya-Gamma data augmentation technique has a single layer of latent
variables that yields conditionally Gaussian posterior distributions when
working with logistic likelihoods.  The two common models considered here are
the binomial logistic model with identical number of trials, whose observation
probability mass function is
\[
p(y_i | p_i) \propto p_i^{y_i} (1-p_i)^{n - y_i},
\]
and negative binomial models for count data, whose observation probability mass
function is \( p(y_i | p_i) \propto p_i^{y_i} (1-p_i)^{d}.  \) In either case,
one may use the log-odds scale, $\psi_i = \log \frac{p_i}{1-p_i}$, for
regression.  Under this coordinate system, the measurement density of the
binomial observations is
\[
p(y_i | \psi_i) \propto \frac{(e^{\psi_i})^{y_i}}{(1+e^{\psi_i})^{n_i}}.
\]
A similar form emerges in the case of negative binomial observations.  In
general, DGLMs with logistic likelihoods have densities of the form
\[
p(y, \bbeta) = c(y) \Big[ \prod_{i=1}^n
\frac{(e^{\psi_i})^{a_i}}{(1+e^{\psi_i})^{b_i}} \bbI \{\psi_i = x_i \beta_i\}
\Big] p(\bbeta)
\]
where $a_i$ and $b_i$ may be functions of $y$ or $\theta$, $\theta$ a set of
parameters that we suppress from the notation above and below; $\bbeta =
\{\beta_t\}$; and $p(\bbeta)$ governs the dynamics of $\bbeta$.  For instance,
$p(\bbeta)$ might be the density of an VAR(1) process.  As shown in
\cite{polson-etal-2012}, if $p(\bbeta)$ is a Gaussian ``prior'' and if one
augments this density with
\begin{equation}
\label{eqn:augmentation}
p(\omega | y, \bbeta) = \prod_{i=1}^n \Big[ p(\omega_i | b_i, \psi_i)
\Big]
\end{equation}
where $\omega_i \sim \PG(b_i, \psi_i)$, then the complete conditional posterior
distribution for $\bbeta$ will be Gaussian.  In particular, $p(\omega_i | b_i,
\psi_i) \propto \cosh^{b_i}(\psi_i/2) e^{-\omega_i \psi_i^2 / 2} p(\omega_i |
b_i, 0)$ so that
% \[
% p(\beta | y, \omega) \propto 
% \Big[ \prod_{i=1}^n e^{\kappa_i \psi_i - \frac{\omega_i}{2} \psi_i^2}
% \bbI \{\psi_i = x_i' \beta\} \Big] p(\beta)
% \]
% where $\kappa_i = a_i - b_i / 2$.  A single term from the product has
% \[
% \exp \Big( \kappa_i \psi_i - \frac{\omega_i}{2} \psi_i^2 \Big)
% \propto
% \exp \Big(-\frac{1}{2} (\kappa_i / \omega_i - \psi_i)^2 \omega_i \Big),
% \]
% which is identical to the likelihood of an observation $z_i = \kappa_i /
% \omega_i$ drawn from
% \[
% z_i \sim N(x_i' \beta, \omega_{i}^{-1}).
% \]
\[
p(\bbeta | y, \omega) 
\propto 
\Big[ \prod_{i=1}^n \exp 
\Big(-\frac{\omega_i}{2} \Big(\frac{\kappa_i}{\omega_i} - \psi_i\Big)^2 \Big)
\bbI \{\psi_i = x_i' \beta\} \Big] p(\bbeta)
\]
where $\kappa_i = a_i - b_i / 2$.  A single term from the product is identical
to the likelihood of a pseudo-data point $z_i = \kappa_i / \omega_i$ drawn from
$z_i \sim N(\psi_i, 1/\omega_i)$.  Thus, if $p(\bbeta)$ specifies that $\bbeta$
is a Gaussian VAR(1) process, then sampling complete conditional for $\bbeta$ is
equivalent to sampling $\bbeta$ from the DLM
\[
\begin{cases}
z_t = \psi_t + \nu_t, & \nu_t \sim N(0, 1/\omega_t) \\
\psi_t = x_t \beta_t \\
\beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]

\begin{example}
Suppose one observes a binomial outcome $y_t$ where $P(y_t = 1) = p_t$ out of
$n_t$ trials at time $t$.  Letting $\psi_t$ be the log-odds, the data generating
distribution is
\[
p(y_t | \psi_t) = c(y_t) \frac{(e^\psi_t)^{y_t}}{(1+e^{\psi_t})^{n_t}}.
\]
Thus the complete conditional $(\bbeta | y, \omega)$ may be simulated by using
filter forward backward sampling with pseudo-data $z_t = \kappa_t / \omega_t$
where $\kappa_t = y_t - n_t / 2$.
\end{example}


\begin{example}
Suppose that one observes counts according to $y_t \sim \NB(h, p_t)$ where $h$
is the number of ``failures'' before observing $y_t$ successes and $p_t$ is the
probability of observing a success.  Letting $\psi_t$ be the log-odds, the data
generating distribution is
\[
p(y_t | \psi_t) = c(y_t, h) \frac{(e^{\psi_t})^{y_t}}{(1+e^{\psi_t})^{y_t+h}}.
\]
In negative binomial regression, it is common to model the log-mean, $\lambda_t
= \psi_t + \log(h) = x_t \beta_t$, instead of the log-odds.  This requires only
a slight modification.  Following the work above, the complete conditional
$(\omega_t | \beta_t, h)$ is $\PG(b_t, \psi_t)$ where $b_t = y_t + h$.  However,
the DLM used to estimate $\bbeta$ is now
% \[
% \begin{cases}
% z_t = \psi_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
% \psi_t = x_t \beta_t - \log(d) \\
% \beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W)
% \end{cases}
% \]
% where $z_t = \kappa_t / \omega_t$ and $\kappa_t = (y_t - d_t) / 2$.
\[
\begin{cases}
z_t = \lambda_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
\lambda_t = x_t \beta_t \\
\beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_t \sim N(0, W)
\end{cases}
\]
where $z_t = \kappa_t / \omega_t + \log(d)$ and $\kappa_t = (y_t - d) / 2$.
\end{example}

For either model, posterior samples may be generated via Gibbs sampling.  One
can estimate the parameters of the VAR(1) process using standard conjugate
updating procedures and the complete conditional $(\omega | \bbeta, y)$, is
identical to (\ref{eqn:augmentation}).  \cite{polson-etal-2012} describe how to
sample from $\PG$ distributions and implement this sampler in the \texttt{R}
package \texttt{BayesLogit}.  

% To estimate $d$ in dynamic negative binomial regression one may use a
% Metropolis-Hastings step (Appendix \ref{sec:sample-d}).

%\subsection{AR(1) Example}

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{nb-ar1-tophalf.pdf}
\caption{\label{fig:nb-ar1} Incidence of influenza-like illness in Texas,
  2008--11, together with the estimated mean $\lambda_t$ from the
  negative-binomial AR(1) model.  The blanks in weeks 21-41 correspond to
  missing data.  The grey lines depict the upper and lower bounds of a 95$\%$
  predictive interval.}
\end{center}
\end{figure}

As an initial illustration of the point, we fit a negative-binomial AR(1) model
to four years (2008--11) of weekly data on flu incidence in Texas, collected
from the Texas Department of State Health Services.  Let $y_t$ denote the number
of reported cases of influenza-like illness (ILI) in week $t$.  We assume that
these counts follow a negative-binomial model, which will allow overdispersion
relative to the Poisson.
% \begin{eqnarray*}
% % y_t &\sim& \mbox{NB}(h, p_t) \; , \quad p_t= \frac{e^{\psi_t}}{1+e^{\psi_t}} \\
% \psi_t &=& \alpha + \gamma_t  \; , \quad \gamma_t = \phi \gamma_{t-1} + \epsilon_t \\
% \epsilon_t &\sim& N(0, \sigma^2) \, .
% \end{eqnarray*}
Figure \ref{fig:nb-ar1} shows the results of the fit.  One may sample from this
model by combining the \Polya-Gamma framework with forward filter backwards
sampling.  For simplicity, we assumed an improper uniform prior on the
dispersion parameter $h$, and fixed $\phi$ and $\sigma^2$ to $0.98$ and 1,
respectively, but it is straightforward to place hyper-priors upon each
parameter, and to sample them in a hierarchical fashion.  It is also
straightforward to incorporate fixed effects in the form of regressors.

% We emphasize that there are many ways to handle the overdispersion present in
% this data set, and that we do not intend our model to be taken as a definitive
% analysis.  We merely intend it as a proof of concept, showing how various
% aspects of Bayesian hierarchical modeling---in this case, a simple AR(1)
% model---can be combined routinely with binomial likelihoods using the
% \Polya-Gamma scheme.

\section{Comparison}

Dynamic versions of binomial logistic regression and negative binomial
regression are common models.  Hence, it is important to understand which
methods work best for each model.

We compare methods by median effective sample size (ESS) and median effective
sampling rate (ESR).  The effective sample size of $M$ correlated samples is an
estimate of the number of independent samples from the target distribution
needed to produce a collection of draws of equivalent quality.  The effective
sampling rate is the ratio of effective sample size to the time taken to
generate samples post burn-in.  Thus the time used to normalize the effective
sample size does not include burn-in or any preliminary computations.

For each data set, 10 batches of 10,000 samples are recorded after an initial
burn-in of 2,000 draws.  For batch $m$, the component-wise effective sample size
$ESS_{t}^{m}$ of each univariate chain corresponding to $\beta_t$ is calculated.
We take the mean over batches to produce the average, component-wise effective
sample size $\overline{ESS}_t$.  The minimum, median, and maximum average
effective sample sizes are reported in the tables herein.  Similarly, for each
run an averaage, component wise effective sampling rate is calculated,
$\overline{ESR}_t$.  The minimum, median, and maximum average effective sampling
rate is our primary measure of comparison because it measures how quickly each
method generates independent samples.

In terms of effective sampling rate, the \Polya-Gamma technique beats both
\cite{ravines-etal-2006} and \cite{fussl-etal-2013} for dynamic binary logistic
regression.  The method of \cite{fruhwirth-schnatter-etal-2009} works well for
dyanmic negative binomial regression.  This is due to the current inefficiency
in generating random variates from $\PG(n, \psi)$ when $n$ is large.  In terms
of effective sampling size, the \Polya-Gamma technique is superior to the other
methods in both settings.  Thus, if Polson et al. improve the speed of their
sampler it will likely surpass \cite{fussl-etal-2013} in terms of ESR for
negative binomial regression.

\section{Conclusion}

\appendix

% \section{Sampling $d$ in dynamic negative binomial regression}
% \label{sec:sample-d}

% We may sample $d$ by an indepedent Metropolis-Hastings or a random-walk
% Metropolis-Hastings.  In either case, we need the log-likelihood of $d$.

% The conditional density for a single term $y_t \sim N(d, p_t)$ is
% \[
% \frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} p_t^{y_t} (1-p_t)^{d} ,
% \]
% or 
% \[
% \frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} 
% \Big( \frac{\mu_t}{\mu_t + d} \Big)^{y_t} \Big( \frac{d}{\mu_t + d} \Big)^d
% \]
% in terms of the mean $\mu_t$.  Since we chose to model $\log \mu_t = x_t
% \beta_t$ it will be easier to work with the latter version.  In that case, the
% log-likelihood is
% \[
% \ell(d|\mu, y) = \sum_{t=1}^T [ \log \Gamma(y_t + d)  - \log \Gamma(d) ]  + 
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]

% If we assume that $d$ is a natural number then we can rid ourselves of $T$
% evaluations of the gamma function.  We may expand $\Gamma(y_t + d)$ and
% $\Gamma(d)$ as products, in which case the log-likelihood is
% \[
% \ell(d|\mu, y) = \sum_{t=1}^T \sum_{j=0}^{y_t-1} \log(d + j) + 
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]
% We can rewrite the first term of the sum as
% \[
% \sum_{t=1}^T \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{k-1} \log(d+j) \one \{y_t = k\}
% \]
% which becomes
% \[
% \sum_{k=1}^{\max(y_t)} n_k \sum_{j=0}^{k-1} \log(d+j)
% \]
% where $n_k = \{ \# y_t = k \}$.  We have thus
% \begin{align*}
% \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{\max(y_t)-1} n_k \log(d+j) \one\{j < k\}
% & = 
% \sum_{j=0}^{\max(y_t)-1} \log(d+j) \sum_{k=j+1}^{\max(y_t)} n_k \\
% & = \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j
% \end{align*}
% where $G_j = \{ \# y_t > j \}$.  Notice that $1-G_j = F(j) = \{\# y_t \leq j\}$
% and that $G$ may be pre-processed.  Hence the log-likelihood is
% \[
% \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j + 
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]
% Preprocessing $G$ saves us from having to compute a doubly indexed summation.

\section{Benchmarks}

\begin{table}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Dataset: Tokyo.  Prior: $\phi = 1, W \sim IGa(150, 15)$.  $T=366$.} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    21.72 &     1.00 &  4352.80 &  7735.08 & 10081.46 &   200.43 &   356.18 &   464.22 \\ 
            dRUM  &    20.60 &     1.00 &  1627.61 &  3649.46 &  5428.93 &    79.03 &   177.19 &   263.60 \\ 
            CUBS  &   233.98 &     0.47 &   652.86 &   761.98 &   910.62 &     2.79 &     3.26 &     3.89
 \end{tabular}

% ./table.bench-dynlogit-low-2-n-1-wout.ar
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Dataset: Synth Low.  Prior $\phi_i = 0.95, W_i=0.172$.  $T=500,
  P=2$.} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    28.34 &     1.00 &  8309.00 &  9395.80 &  9894.45 &   293.16 &   331.50 &   349.09 \\ 
            dRUM  &    29.08 &     1.00 &  5299.68 &  7646.07 &  9800.60 &   182.26 &   262.96 &   337.05 \\ 
            CUBS  &   609.04 &     0.68 &  2424.95 &  3930.01 &  5168.38 &     3.98 &     6.45 &     8.49
 \end{tabular}

% ./table.bench-dynlogit-high-2-n-1-wout.ar
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Dataset: Synth High.  Prior: $\phi_i = 0.95, W_i=0.172$.
  $T=500, P=2$.} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    28.46 &     1.00 &  8611.53 &  9441.44 &  9894.56 &   302.60 &   331.76 &   347.68 \\ 
            dRUM  &    29.19 &     1.00 &  6204.70 &  7879.60 &  9656.77 &   212.54 &   269.91 &   330.78 \\ 
            CUBS  &   610.43 &     0.61 &  2562.38 &  4291.96 &  6932.42 &     4.20 &     7.03 &    11.36
 \end{tabular}

% ./table.bench-dynlogit-low-2-n-1-with.ar
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{ Dataset: Synth Low. Prior $\phi_i \sim N(0.95, 0.01), W_i
  \sim IGa(10, 1)$.  $T=500, P=2$.} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    31.03 &     1.00 &  1348.88 &  7063.34 &  9914.37 &    43.46 &   227.65 &   319.53 \\ 
            dRUM  &    31.81 &     1.00 &  1213.67 &  5643.54 &  9060.83 &    38.15 &   177.40 &   284.82 \\ 
            CUBS  &   627.81 &     0.65 &   125.22 &   576.81 &  1695.55 &     0.20 &     0.92 &     2.70
 \end{tabular}

% ./table.bench-dynlogit-high-2-n-1-with.ar
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Dataset: Synth Low. Prior $\phi_i \sim N(0.95, 0.01), W_i \sim IGa(10, 1)$.  $T=500, P=2$.} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    31.02 &     1.00 &   686.61 &  4802.36 &  9523.97 &    22.13 &   154.81 &   307.01 \\ 
            dRUM  &    31.84 &     1.00 &   479.20 &  3774.41 &  8505.58 &    15.05 &   118.55 &   267.15 \\ 
            CUBS  &   618.25 &     0.55 &   181.61 &   541.04 &  1617.91 &     0.29 &     0.87 &     2.61
 \end{tabular}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{./table.bench-dynlogit-low-2-n-10-wout.ar} \\
% \hline
%           Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
%               PG  &    49.77 &     1.00 &  7116.22 &  9374.76 &  9911.13 &   142.98 &   188.36 &   199.13 \\ 
%             dRUM  &    28.98 &     1.00 &  3540.13 &  7482.71 &  9884.88 &   122.14 &   258.16 &   341.04 \\ 
%             CUBS  &   638.99 &     0.60 &  1233.54 &  3317.85 &  5278.43 &     1.93 &     5.19 &     8.26
%  \end{tabular}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{./table.bench-dynlogit-high-2-n-10-wout.ar} \\
% \hline
%           Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
%               PG  &    50.17 &     1.00 &  7327.10 &  9211.07 &  9826.71 &   146.05 &   183.61 &   195.88 \\ 
%             dRUM  &    28.97 &     1.00 &  3939.55 &  6967.89 &  9766.74 &   135.99 &   240.52 &   337.14 \\ 
%             CUBS  &   643.20 &     0.52 &  2370.69 &  4240.41 &  5878.98 &     3.69 &     6.59 &     9.14
%  \end{tabular}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{./table.bench-dynlogit-low-2-n-10-with.ar} \\
% \hline
%           Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
%               PG  &    52.54 &     1.00 &   626.75 &  5161.88 &  9855.10 &    11.93 &    98.24 &   187.56 \\ 
%             dRUM  &    31.84 &     1.00 &   674.18 &  3477.35 &  9119.24 &    21.17 &   109.21 &   286.40 \\ 
%             CUBS  &   661.84 &     0.56 &   364.16 &  1443.76 &  3065.32 &     0.55 &     2.18 &     4.63
%  \end{tabular}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{./table.bench-dynlogit-high-2-n-10-with.ar} \\
% \hline
%           Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
%               PG  &    52.99 &     1.00 &   174.62 &  1001.65 &  8922.27 &     3.29 &    18.91 &   168.37 \\ 
%             dRUM  &    31.79 &     1.00 &   151.89 &   867.68 &  7060.28 &     4.78 &    27.30 &   222.06 \\ 
%             CUBS  &   651.22 &     0.47 &   192.77 &   772.78 &  2680.11 &     0.30 &     1.19 &     4.12
%  \end{tabular}
\end{table}

\newpage

\begin{table}
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-low-2-mu-10-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    63.86 &     1.00 &  8305.72 &  9668.71 & 10000.00 &   130.07 &   151.41 &   156.60 \\ 
              FS  &    29.70 &     1.00 &  1755.48 &  6508.09 &  9870.64 &    59.11 &   219.11 &   332.32 \\ 
            CUBS  &   648.25 &     0.60 &   384.08 &  3407.23 &  5894.51 &     0.59 &     5.26 &     9.09
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-high-2-mu-10-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    59.63 &     1.00 &  8967.77 &  9684.87 & 10000.00 &   150.39 &   162.42 &   167.70 \\ 
              FS  &    29.65 &     1.00 &  2844.82 &  5812.03 &  9773.66 &    95.95 &   196.02 &   329.64 \\ 
            CUBS  &   656.19 &     0.74 &  4605.04 &  7412.28 &  8063.25 &     7.02 &    11.30 &    12.29
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-low-2-mu-10-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    66.80 &     1.00 &   149.03 &  4880.10 &  9833.67 &     2.23 &    73.06 &   147.21 \\ 
              FS  &    32.65 &     1.00 &   149.47 &  1793.39 &  7952.70 &     4.58 &    54.92 &   243.58 \\ 
            CUBS  &   667.39 &     0.55 &   135.02 &  1249.47 &  2792.73 &     0.20 &     1.87 &     4.18
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-high-2-mu-10-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &    62.28 &     1.00 &   244.92 &  2039.14 &  9767.92 &     3.93 &    32.73 &   156.83 \\ 
              FS  &    32.51 &     1.00 &   284.53 &  1219.91 &  5614.48 &     8.75 &    37.52 &   172.66 \\ 
            CUBS  &   669.68 &     0.62 &   363.77 &  1609.55 &  4586.74 &     0.54 &     2.40 &     6.85
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-low-2-mu-100-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &   393.08 &     1.00 &  1898.18 &  7298.22 &  9990.29 &     4.83 &    18.57 &    25.42 \\ 
              FS  &    29.68 &     1.00 &  2386.51 &  7229.31 & 10000.00 &    80.40 &   243.58 &   336.93 \\ 
            CUBS  &   661.62 &     0.37 &   198.71 &  1150.20 &  2867.16 &     0.30 &     1.74 &     4.33
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-high-2-mu-100-wout.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &   292.81 &     1.00 &  4090.32 &  7283.70 &  9697.62 &    13.97 &    24.87 &    33.12 \\ 
              FS  &    29.65 &     1.00 &  3417.13 &  6864.88 &  9566.54 &   115.26 &   231.58 &   322.68 \\ 
            CUBS  &   660.24 &     0.39 &  1065.38 &  3124.58 &  4242.90 &     1.61 &     4.73 &     6.43
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-low-2-mu-100-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &   395.37 &     1.00 &  1766.07 &  4793.62 &  9445.17 &     4.47 &    12.12 &    23.89 \\ 
              FS  &    32.61 &     1.00 &  2217.92 &  4814.16 &  9682.45 &    68.02 &   147.64 &   296.91 \\ 
            CUBS  &   670.26 &     0.32 &   347.70 &   783.63 &  1561.49 &     0.52 &     1.17 &     2.33
 \end{tabular}

\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{./table.bench-dynnb-high-2-mu-100-with.ar} \\
\hline
          Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
              PG  &   295.55 &     1.00 &  1170.90 &  3414.61 &  8363.79 &     3.96 &    11.55 &    28.30 \\ 
              FS  &    32.48 &     1.00 &  1139.88 &  3228.07 &  7966.98 &    35.10 &    99.39 &   245.29 \\ 
            CUBS  &   674.96 &     0.30 &   381.00 &   744.60 &  1115.24 &     0.56 &     1.10 &     1.65
 \end{tabular}

\caption{Using old non-hybrid sampler.}

\end{table}

\newpage

\begin{table}

% ./bench-dynnb-low-2-mu-10-wout.ar.table
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Low: $P=2$, $N=10$, $\Phi = 0.95I$, $W=0.172 I$} \\
\hline
          Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    42.19 &     1.00 &   7909.83 &   9605.44 &   9989.04 &    187.49 &    227.68 &    236.77 \\ 
             FS   &    29.49 &     1.00 &   1828.26 &   6526.06 &   9794.63 &     62.00 &    221.31 &    332.15
 \end{tabular}

%./bench-dynnb-low-2-mu-10-with.ar.table
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Low: $P=2$, $N=10$, $\phi \sim N(0.95, 0.1 I)$, $W_i \sim Ga(300/2, 30/2)$} \\
\hline
          Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    45.03 &     1.00 &    159.08 &   4846.11 &   9717.61 &      3.53 &    107.62 &    215.79 \\ 
             FS   &    32.36 &     1.00 &    140.04 &   1824.40 &   7436.81 &      4.33 &     56.37 &    229.79
 \end{tabular}

% ./bench-dynnb-high-2-mu-10-wout.ar.table
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{High: $P=2$, $N=10$, $\Phi = 0.95I$, $W=0.172 I$} \\
\hline
          Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    42.08 &     1.00 &   9092.48 &   9724.85 &   9968.42 &    216.06 &    231.09 &    236.88 \\ 
             FS   &    29.52 &     1.00 &   2915.06 &   5866.55 &   9705.36 &     98.74 &    198.71 &    328.74
 \end{tabular}

% ./bench-dynnb-high-2-mu-10-with.ar.table
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{High: $P=2$, $N=10$, $\phi \sim N(0.95, 0.1 I)$, $W_i \sim Ga(300/2, 30/2)$} \\
\hline
          Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    44.93 &     1.00 &    276.43 &   1969.84 &   9528.81 &      6.15 &     43.84 &    212.09 \\ 
             FS   &    32.39 &     1.00 &    239.37 &   1090.05 &   5350.24 &      7.39 &     33.65 &    165.19
 \end{tabular}

% ./bench-dynnb-low-2-mu-100-wout.ar.table
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Low: $P=2$, $N=100$, $\Phi = 0.95I$, $W=0.172 I$} \\
\hline
          Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    38.41 &     1.00 &   1886.58 &   7345.99 &   9855.94 &     49.12 &    191.26 &    256.61 \\ 
             FS   &    29.49 &     1.00 &   2447.72 &   7362.03 &   9927.30 &     83.00 &    249.64 &    336.62
 \end{tabular}

% ./bench-dynnb-low-2-mu-100-with.ar.table
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{Low: $P=2$, $N=100$, $\phi \sim N(0.95, 0.1 I)$, $W_i \sim Ga(300/2, 30/2)$} \\
\hline
          Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    41.25 &     1.00 &   1832.73 &   4804.28 &   9270.06 &     44.43 &    116.46 &    224.71 \\ 
             FS   &    32.34 &     1.00 &   2283.24 &   4706.06 &   9081.44 &     70.60 &    145.52 &    280.80
 \end{tabular}

% ./bench-dynnb-high-2-mu-100-wout.ar.table
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{High: $P=2$, $N=100$, $\Phi = 0.95I$, $W=0.172 I$} \\
\hline
          Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    40.60 &     1.00 &   4037.45 &   7248.51 &   9720.22 &     99.44 &    178.52 &    239.40 \\ 
             FS   &    29.49 &     1.00 &   3449.77 &   6959.22 &   9727.76 &    116.98 &    235.98 &    329.85
 \end{tabular}

% ./bench-dynnb-high-2-mu-100-with.ar.table
\begin{tabular}{l r r r r r r r r } 
\hline
\multicolumn{9}{c}{High: $P=2$, $N=100$, $\phi \sim N(0.95, 0.1 I)$, $W_i \sim Ga(300/2, 30/2)$} \\
\hline
          Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\ 
             PG   &    43.42 &     1.00 &   1308.62 &   3487.57 &   8016.66 &     30.13 &     80.31 &    184.62 \\ 
             FS   &    32.35 &     1.00 &   1199.19 &   3212.34 &   7733.74 &     37.07 &     99.29 &    239.05
 \end{tabular}

\caption{Using hybrid sampler.  $d$ set to 4.  $x_t^{(i)} \propto
  \cos( \pi \omega_i u_t)$.  $\beta_t = \phi \beta_{t-1} + \omega_t, \; \omega_t
  \sim N(0, W)$.  $\psi_t = \log(nb.mean) + x_t' \beta_t$.}

\end{table}



% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliography{bayeslogit}{}
\bibliographystyle{abbrvnat}

\end{document}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Tokyo Rainfall} \\
% \hline
%           Method  &     time &    ARate &  ESS.min &  ESS.med &  ESS.max &  ESR.min &  ESR.med &  ESR.max \\ 
%             CUBS  &   233.98 &     1.00 &   652.86 &   761.98 &   910.62 &     2.79 &     3.26 &     3.89 \\ 
%               PG  &    21.72 &     1.00 &  4352.80 &  7735.08 & 10081.46 &   200.43 &   356.18 &   464.22 \\ 
%             dRUM  &    20.60 &     1.00 &  1627.61 &  3649.46 &  5428.93 &    79.03 &   177.19 &   263.60
%  \end{tabular}
% \end{table}

% \begin{table}
% \centering
% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta$.} \\
% \hline
%           Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%              PG  &    22.59 &     1.00 &   7367.68 &   9229.91 &  10189.37 &    326.10 &    408.52 &    450.97 \\ 
%            dRUM  &    23.19 &     1.00 &   2759.83 &   7260.46 &   9967.66 &    118.99 &    313.04 &    429.77
%          \end{tabular}

% \begin{tabular}{l r r r r r r r r }
% \hline
% \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta$.} \\
% \hline 
%           Method  &    time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%              PG  &    22.52 &     1.00 &   8494.04 &   9662.06 &  10225.35 &    377.13 &    428.98 &    454.00 \\ 
%            dRUM  &    23.17 &     1.00 &   4963.68 &   8105.02 &   9936.46 &    214.26 &    349.86 &    428.91
% \end{tabular}

% \end{table}


% \begin{table}
% \centering
% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{Low corr X: $P$=2, $T$=400.  Estimate $\alpha, \beta, \phi, W$.} \\
% \hline
%           Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%             PG  &    25.04 &     1.00 &    643.23 &   6531.34 &  10420.88 &     25.69 &    260.82 &    416.13 \\ 
%           dRUM  &    25.73 &     1.00 &    430.79 &   4490.34 &  10124.47 &     16.74 &    174.55 &    393.48
%  \end{tabular}

% \begin{tabular}{l r r r r r r r r } 
% \hline
% \multicolumn{9}{c}{High corr X: $P$=2, $T$=400. Estimate $\alpha, \beta, \phi, W$.} \\
% \hline
%           Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\ 
%             PG  &    25.16 &     1.00 &    988.83 &   5654.23 &  10337.07 &     39.31 &    224.75 &    410.87 \\ 
%           dRUM  &    25.84 &     1.00 &   1040.12 &   5496.94 &   9760.64 &     40.25 &    212.74 &    377.75
%  \end{tabular}