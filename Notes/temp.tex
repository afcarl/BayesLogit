\documentclass{article}

\input{commands}

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
%\parindent 0in
%\parskip 12pt

% Set Fancy Style
%\pagestyle{fancy}

%\lhead{Left Header}
%\rhead{Right Header}

\begin{document}

% Change Font and Spacing
\large % change the font size to 12pt
%\linespread{1.1} % change the line spacing

% Set the counter
\setcounter{section}{0}

\section{Expected number of iterations}

Theorem 5.1 (p. 155) of Devroye can be used to calculate the expected number of
iterations for the algorithm.  It relies upon Wald's equation, which can be
found on p. 50 (Thm 3.5), i.e. section 3.4.  The introduction to that section
explains things well.  Imagine a rejection-like algorithm.  Each time you make a
proposal and then check it you generate a vector of random variables, $W_i =
(X_i, Y_i)$.  It may be the case that $W_i$ has more than two random variables.
The $W_i$ are iid.  Furthermore, there is some stopping rule $N$, which I assume
is the same thing as a stopping time and there is some function which maps $W_i$
to some quantity of interest $\psi(W_i)$, such as the number of floating point
operations, the number of if statements evaluated, or the number of terms
evaluted in a partial sum.  We know that $N$ is a stopping time because, as
pointed out by Devroye, since $P(N=n)$ is determined by $W_1, \ldots, W_n$
residing in some set $B_n$.  Thus $N = n$ is $\mcF_n$-measurable.  Wald's
theorem tells us how to calculate the \emph{total} expected number of whatever
we are interested in, i.e. the expected value of
\[
\sum_{i=1}^N \psi(W_i).
\]
Given the above assumptions, Wald's theorem says that
\[
\bbE \Big[ \sum_{i=1}^N \psi(W_i) \Big] = E(N) E(\psi(W_1)).
\]
The proof is succinct.  The key is to recognize that (let $Z_i = \psi(W_i)$)
$Z_i$ and $\bbI \{N \geq i\}$ are independent for all $i$ as $Z_i$ and $\bbI \{N
< i\}$ are independent.

Devroye's theorem 5.1 is a bit short on detail.  Since I am used to thinking in
terms of the partial sums without the proposal factored out, I will phrase
things in an alternate manner.  The theorems says: Suppose $f$ can be
represented as an alternating sum
\[
f(x) = \sum_{i=0}^\infty (-1)^i a_i(x)
\]
where $a_i(x)$ is positive and decreasing in $i$ for all $x$ and that the
proposal you will use is $g(x) \propto a_0(x)$.  Let $N$ be the number of
``outer-loop'' iterations in the alternating sum algorithm, that is the total
number of times you make a proposal, and let $Z_i = \psi(X_i, Y_i)$ be the total
number of inner-loop interations, that is the the total number of times you
compare $Y_i$ to $S_n(X_i)$.  Then
\[
\bbE(N) = \int_0^\infty \sum_{i=0}^\infty a_i(x) dx.
\]

One can see this by an application of Wald's equation.  Consider the following:
When following the alternating sum algorithm one proposes $X \sim g$, generates
$Y \sim \mcU(0, cg(X))$, and then check $Y \leq f(X)$ by iterating through the
partial sums.  We can calculate the probability that $Y$ will be
accpeted/rejected on the $i$th iteration by checking that $Y \in (S_{i-2}, S_i]$
if $i$ is odd (with the convention that $S_{-1} = 0$) and that $Y
\in (S_i, S_{i-2}]$ if $i$ is even.  Given $X$, this probability is just the
length of that interval divided by $S_0 = a_0$ as seen in the figure below.
\begin{center}
\includegraphics[scale=0.75]{expected_iterations.png}
\end{center}
Further more
\[
a_{i-1} - a_{i} = 
\begin{cases}
S_{i} - S_{i-2}, & \text{$i$ is even} \\
S_{i-2} - S_i, & \text{$i$ is odd}
\end{cases}
\]
Thus the probability on getting out of the inner loop on iteration $i$ given $X=x$
is
\[
\frac{a_{i-1} - a_i}{a_0}
\]
evaluated at $x$.  And hence the expected value of getting out of the inner loop
when $X=x$ is
\[
\frac{1}{a_0} \sum_{i=1}^\infty i (a_{i-1} - a_i) = \frac{1}{a_0(x)}
\sum_{i=0}^\infty a_i.
\]
Wald's theorem then tells us that
\begin{align*}
\bbE \Big[ \sum_{i=1}^N \psi(W_i) \Big] 
& = \bbE(N) \bbE(\psi(W_1)) \\
& = \bbE(N) \bbE( \bbE(\psi(W_1)|X_1) ).
\end{align*}
Since $\bbE(N) = c$ we have that this product is
\[
c \int_{0}^\infty \Big[ \frac{1}{a_0(x)} \sum_{i=0}^\infty a_i(x) \Big] g(x) dx
= \int_{0}^\infty \sum_{i=0}^\infty a_i(x) dx.
\]

We may interchange the sum and the integral by the MCT.  We can calculate each
integral analytically as the piecewise definition of $a_i$ is composed of the
exponential kernel and the inverse Gaussian kernel.  For the exponential kernel
we have
\[
r_n(x) = \cosh(z) \pi \Big(n + \frac{1}{2}\Big) 
\exp \Big\{ -\frac{x}{2} \Big( z^2 + (n+1/2)^2 \pi^2 \Big) \Big\}.
\]
And for the inverse Gaussian kernel we have... within the exponent
\begin{align*}
- \frac{z^2}{2x} - \frac{(2n + 1)^2}{2x}
& = -\frac{z^2}{2x} \Big( x^2 + \alpha_n^2), \; \alpha_n = (2n+1)^2 / z^2 \\
& = - \lambda_n - \frac{\lambda_n}{2x \alpha_n} (x - \alpha_n)^2, \; \lambda_n =
(2n+1)^2.
\end{align*}
Thus we are left with an inverse Gaussian kernel
\[
\ell_n(x) = 
\cosh(z) \pi (n + 1/2) \Big(\frac{2}{\pi}\Big)^{3/2} \exp(-\lambda_n)
\Big(\frac{1}{x}\Big)^{3/2}
\exp \Big\{ -\frac{\lambda_n}{2x\alpha_n}(x-\alpha_n)^2 \Big\}.
\]
Closed form antiderivatives exists for both components.  We can use that to
calculate the average number of iterations given $x$. 

\subsection{Distribution of inner-loop iterations}

The argument above shows how to compute $P(Z = j | X = x)$ thus one can also
calculate
\[
P(Z = j) = \int_{0}^\infty P(Z = j | X = x) g(x) dx,
\]
the expected probability that $Z = j$, which one can compute using the kernels
above.  (I think) We could also let $\Psi$ map $(X,Y)$ to the probability mass
function of $Z$ given $X$.  I need to think more about that.

% If you have a bibliography.
% The file withe bibliography is name.bib.
% \bibliography{name}{}
% \bibliographystyle{plain}

\end{document}
