\documentclass{article}

\input{commands}
\usepackage{outlines}
\usepackage{natbib}
\usepackage{parskip}

\newcommand{\Polya}{P\'{o}lya}
\newcommand{\Pois}{\text{Pois}}
\newcommand{\Ga}{\text{Ga}}
\newcommand{\NB}{\text{NB}}

%\newcommand{\vect}{\text{vec}}

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}
%\parindent 0in
%\parskip 12pt

% Set Fancy Style
%\pagestyle{fancy}

%\lhead{Left Header}
%\rhead{Right Header}

\begin{document}

% Change Font and Spacing
\large % change the font size to 12pt
\linespread{1.1} % change the line spacing

% Set the counter
\setcounter{section}{0}

\section{Dynamic / GP Logit}

Imagine a scenario where the coefficients in a regression may be slowly changing
in time.  The idea, I think, is that somehow the population is slowly evolving
via changes in the parameters of the regression.  For instance, we could regard
$\psi_{it}$ as the log-odds of success for observation $i$ at time $t$.  This
would be something like
\[
P(y_{it}=1) = p_{it} \; \textmd{ where } \psi_{it} = \log \frac{p_{it}}{1-p_{it}}.
\]
I am imagining a scenario where at each time $t$ we are doing a logistic
regression and these logistic regressions are tied together by some sort of
correlation / covariation structure.  This could be modeled by something like
\[
\begin{cases}
\psi_{it} = x_{it} \beta_t \\
\beta_t = \beta_{t-1} + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]
Above is the basic idea used by West et al. (1985) (or check out West and
Harrison 1997) for modeling dynamic generalized linear models.

Imagine fixing $t$ and looking at a single term in the likelihood using the
Polya-Gamma trick.  You would have something like (where $y_i$ is the number of
successes for covariate $i$--though for the PG code we have $y_i$ is the
proportion of successes!)
\[
\exp \Big[ (y_{i} - n_{i}/2) x_{i} \beta - \frac{1}{2} \omega_{i} \psi_i^2
\Big] 
p(\omega_i | n_i, 0). 
\]
Collecting all the observations this becomes
\[
\exp \Big[ \alpha' X \beta - \frac{1}{2} \beta' X' \Omega X \beta \Big]
p(\omega | n, 0) \;
\textmd{ where } \;
\alpha_i = (y_i - n_i/2) \; \textmd{ and } \; X = [x_i].
\]
Assuming that $\omega$ is fixed.  We want to find a quadratic form
\[
(z - X \beta)' \Omega (z - X \beta).
\]
Expanding the quadratic form we get
\[
z' \Omega z - 2 z' \Omega X \beta + (X \beta)' \Omega (X \beta).
\]
Since $\omega$ is fixed we can just multiply the likelihood by $\exp
\frac{-1}{2} z' \Omega z$ where $\Omega z = \alpha$ to generate this expression.
Thus, for fixed $\omega$ the likelihood of $\beta$ can be interpreted as coming from
\[
z = X \beta + \ep, \; \ep \sim N(0, \Omega^{-1})
\]
where
\[
\Omega z = \alpha.
\]
(Waving my hands: We have $\Omega z = \alpha = y - n/2$.  Thus we should
have $y = \Omega z + n/2$.  Use for generating synthetic data?)  Thus,
reincorporating the time index we have
\[
\begin{cases}
z_t = X_t \beta_t + \ep_t, & \ep_t \sim N(0, \Omega_t^{-1}) \\
\beta_t = \beta_{t-1} + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]
It is easy to calculate $z_t$ since $\Omega_t$ is diagonal.  The one place that
we need to be careful is that there may be a variable number of observations at
each point in time.  Thus the number of rows in $z_t$ and $X_t$ is $m_t$, which
may vary.  Also, it may be the case that there are no observations, in which
case we have the equivalent of missing data.  I believe the dynamic linear model
shouldn't have a problem with that.  The sampling procedure is then
\begin{enumerate}
\item Set $\beta_t = 0$ for all $t$.  This is a reasonable seed since it
  represents even odds.
\item Calculate $\psi_t = X_t \beta_t$.
\item Sample $\omega_{it} \sim PG(n_{it}, \psi_{it})$.
\item Sample $\beta_t$ using the dynamic linear model above.
\item Repeat, starting at (2).
\end{enumerate}

An equivalent, off-line, method would be to assume that $\beta(t)$ follows a
Gaussian process or some other prior that ties $\beta_t$ together.  Then we
would have something like
\[
\begin{cases}
z_{i} = x_{i} \beta(t_i) + \ep_{i}, & \ep_{i} \sim N(0, \omega_{i}^{-1}) \\
\beta \sim GP(0, K) \\
\end{cases}
\]
where $K(t_1, t_2)$ is the covariance function of the Gaussian Process.  We have
re-indexed so that each observation has its own index and time is a covariate.
In fact, this shows how you can take fancy linear model and apply it to
categorical data using the Polya-Gamma trick.

Maybe we should try to find categorical data that include spatial component.
One could then use try Gaussian Process regression to estimate with coefficients
changing smoothly in space.  In that case we would have,
\[
\begin{cases}
z_i = f(x_i) + \ep_i, \; \ep \sim N(0, \omega_i^{-1}) \\
f \sim GP(0, K).
\end{cases}
\]
This could be a computational burden because we will have to invert a relatively
large matrix.

Check out Chapter 3 of Williams and Rasmussen's book on Gaussin Processes.  You
must use approximation in that case because you do not have a conjugate
situation.  This is not the case with the Polya-Gamma method.  Though, again,
you may be burdened large amounts of data making it time consuming to invert
matrices.  They have an example using a database of hand written characters
provided by the USPS.  I wonder... instead of using a Gaussian process can you
use a Dirichlet process... You should have clusters, i.e. characters, or several
variants of characters and you want to assign each letter to a cluster.  Maybe
you can do a cluster of clusters.  I think Jordan had something similar to this.
This may relate to his beta process.  Basically, I am thinking of a Gaussian
process of normal mixtures, where the location of the normal mixtures is random.
This seems like it might be computationally intensive.

We can follow a program similar to above following Fr\"{u}hwirth-Schnatter.
This is kind of a hybrid between the Poisson and Logistic regression.  The
log-odds are still expressed as $\psi_{it} = x_i \beta_t$.  If, again, we think
of fixing $t$, then we sample using the latent utiltiy representation
\[
y_{i}^u = x_i \beta + \ep_{i}, \; \ep_i \sim N(m_{r_i}, s_{r_i}^2).
\]
Given $\beta$ we sample each $y_i^u$, $y_{i}^0$, and $r_i$ independently.
Further, we do not assume any of these have temporal dependence when
conditioning on $\beta_t$.  Thus the sampling procedure, I think, should be a
simple extentions where now we estimate $\beta_t$ using
\[
\begin{cases}
y_t = X_t \beta_t + \ep_t, & \ep_t \sim N(m_{r_t}, \diag(s_{r_t}^2)) \\
\beta_t = \beta_{t-1} + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]
Again, we must pay attention to the dimension of $y_t$, $X_t$, and $r_t$, which
changes in time.  (The number of columns in $X_t$ does not change in time.)  As
you can see, this is (essentially) identical to the model above.

In either case, we can write either model as
\[
\begin{cases}
y_t = X_t \beta_t + \ep_t, & \ep_t \sim N(b_t, V_t) \\
\beta_t = (I-\Phi)\mu + \Phi \beta_{t-1} + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]
For the moment let's restrict $\Phi$ to be diagonal.  I am writing this slightly
differently than Fr\"{u}wirth Schnatter because she includes the mean term in
$\beta_t$ (essentially).  I suppose I could do that here as well by accepting
singular $W$.  The mean and variance, $b_t$ and $V_t$ are known while $W$ is not
known.  (Aside: we could store $y_t$ and $X_t$ as lists.  But we could also
store them as vectors or matrices and keep track of the indexing using another
variable.)

\section{DLM}

It turns out that we can represent both the PG DLMs and the FS DLMs using the
same model.  FS includes a static term, which we can do and which we discuss
below, though not much changes when that is the case.  \textbf{What we discuss
  here informs our implementation of the FFBS in the BayesLogit package.}  The
basic model is
\[
\begin{cases}
z_t = x_{s,t} \alpha + x_{d,t} \beta_t + \ep_t, & \ep_t \sim N(0, V_t) \\
\beta_t = (1-\phi) \mu + \Phi \beta_t + \omega_t, & \omega_t \sim N(0, w).
\end{cases}
\]
Thus we have covariates that are associated with a static coefficient and
covariates that are associated with a dynamic coefficient. 

This encapsulates all of the models we encounter.  In the PG case, $V_t =
1/\omega_t$.  In the FS logistic case, $V_t = V_{r_t}$.  In the FS NB case $V_t
= V_{r_t}$ and we must use $z_t = \log(\lambda_t) + \log(d) + m_{r_t}$. where
$m_{r_t}$ and $V_{r_t}$ in this case from the representation of $-\log \Ga$.

We may further clarify what is going on by rewriting the system letting $x_t =
[x_{s,t}, x_{d,t}]$ (row vector) and $\theta_t = [\alpha_t; \beta_t]$ (col
vector) and then using the DLM
\[
\begin{cases}
z_t = x_t \theta_t + \ep_t, & \ep_t \sim N(0, V_t) \\
\theta_t = (1-\Phi) \mu + \Phi \theta_t + \omega_t, & \omega_t \sim N(0, W) \\
\Phi = 
\begin{pmatrix}
I & 0 \\
0 & \phi
\end{pmatrix}
\text{ and }
W = 
\begin{pmatrix}
0 & 0 \\
0 & w
\end{pmatrix}.
\end{cases}
\]
When there is no $\alpha$, then this just reduces to $\Phi = \phi$ and $W = w$.
We can filter forward as normal using $\Phi$ and $W$.  To backwards sample we
draw $\Theta_T | D_T$ and let $\alpha = \Theta_{0:P_a-1}$ if $P_a > 0$ (the
dimension of $\alpha$) and we let $\beta_T = \theta[P_a:P_b-1,T]$ where $P_b$ is
the dimension of $\beta_T$.  Then proceed using $p(\theta_{t-1} | D_{t-1},
\theta_t)$.  The joint distribution $p(\theta_t, \theta_{t-1}, D_{t-1})$ has a
singular variance.  Thus, there is not even a density for us to work with when
doing the regression.  However, it seems that we should be able to regress
$\beta_{t-1}$ on $(\alpha_{t-1}, \beta_t)$ or on $(\alpha_t, \beta_t)$.



When backwards
sampling we just need to look at the joint distribution of $(\beta_t,
\beta_{t-1})$, which we can calculate using the submatrices of $a_t, R_t, m_t,
C_t$ from when we filtered forward.  In particular, we just let (\texttt{il}
stands for $i$ lag)
\begin{itemize}
\item \texttt{a.i  = a[b.idc,i]}
\item \texttt{m.il = m[b.idc,i-1]}
\item \texttt{R.i  = R[b.idc, b.idc, i]}
\item \texttt{C.il = C[b.idc, C.idc, i-1]}.
\end{itemize}

Thus, given these small caveats, we just need the FFBS described below with the
adjustments discussed above.  Think of $\beta_t$ as $\theta_t$ when including
$\alpha$.
\begin{outline}
\1 Time $t-1$ posterior:
\[
\beta_{t-1} \mid D_{t-1} \sim N(m_{t-1}, C_{t-1}).
\]
\1 Evolution / Prior:
\begin{align*}
a_t & = (I-\Phi) \mu + \Phi m_{t-1} \\
R_t & = \Phi C_{t-1} \Phi' + W \\
\beta_t & \mid D_{t-1} = N(a_t, R_t).
\end{align*}
\1 Observation:
\begin{align*}
f_t & = X_t a_t + b_t \\
Q_t & = X_t R_t X_t' + V_t \\
y_t & \mid D_{t-1} \sim N(f_t, Q_t).
\end{align*}
\1 Covariance:
\[
\cov(y_t, \beta_t | D_{t-1}) =: \rho_t = X_t R_t.
\]
\1 Joint:
\[
\begin{bmatrix}
y_t \\ \beta_t
\end{bmatrix}
\mid D_{t-1}
\sim
N
\Big(
\begin{bmatrix}
f_t \\ a_t
\end{bmatrix},
\begin{bmatrix}
Q_t & \rho_t \\
\rho_t' & R_t
\end{bmatrix}
\Big)
\]
\1 Regression matrix, error:
\begin{align*}
A_t & = \rho_t' Q_{t}^{-1} \\
e_t & = y_t - f_t.
\end{align*}
\1 Posterior
\begin{align*}
m_t & = a_t + A_t e_t \\
C_t & = R_t - \rho_t' Q_t^{-1} \rho_t \\
\beta_t & \mid D_t \sim N(m_t, C_t).
\end{align*}

\end{outline}

When calculating $Q_t^{-1}$ you want to consider the dimension of $X_t$, which
we will call $N \times P$.  In the PG/FS Logit/NB case, $X_t$ is $1 \times P$
and $Q_t$ is $1 \times 1$!  Thus just do $1/Q_t$ if in \textbf{PG/FS Logit/NB}
case.  If $N > P$ then you might want to use SWM, i.e.
\[
(V + XRX')^{-1} = V^{-1} + V^{-1} X(R^{-1} + X'V^{-1}X)^{-1}X' V^{-1}.
\]
If $P > N$ then you might just want to invert $Q$ directly.  It is possible that
$P > N$ since we may have only an observation or two for a given time point.
NOTICE that in former case, you do not need to calculate $Q$.  Thus $Q$ is $N
\times N$ ($N$ is the number of ``data points'' and $P$ is the dimension of
$\beta_t$) and
\begin{outline}
\1 $P \geq N$ and $P > 1$

  \2 Note: $A_t' = Q_t^{-1} \rho_t \iff Q_t A_t' = \rho_t$.
  \2 $L L' = Q_t$ so $LL' A_t' = \rho_t$.
  \2 $\xi = L^{-1} \rho_t \iff L \xi = \rho_t$.
  \2 $A_t' = L^{'-1} \xi \iff L'A_t' = \xi$.
  \2 $m_t = a_t + A_t e_t$.
  \2 $C_t = R_t - \xi' \xi$.

\1 $N > P$ (Maybe this should be something like $N > c P$ where $c > 1$.)

  \2 $T = R_t^{-1} + X_t'V_t^{-1}X_t$
  \2 $LL' = T$
  \2 $L^{'-1} X'V^{-1} = \xi \iff X'V^{-1} = L' \xi$.
  \2 $Q_t^{-1} = V_t^{-1} + \xi' \xi$.
  \2 $A_t = \rho_t' Q_t^{-1}$.
  \2 $m_t = a_t + A_t e_t$
  \2 $C_t = R_t - \rho_t' Q_t^{-1} \rho_t$.

\end{outline}

To backward sample we need $p(\beta_{t-1} \mid \beta_t, D_{t-1})$.
\begin{outline}
\1 Covariance:
\[
\cov(\beta_t, \beta_{t-1} \mid D_{t-1}) = \Phi C_{t-1}
\]
\1 Joint:
\[
\begin{bmatrix}
\beta_t \\ \beta_{t-1}
\end{bmatrix}
\mid D_{t-1}
\sim
N
\Big(
\begin{bmatrix}
a_t \\ m_{t-1}
\end{bmatrix},
\begin{bmatrix}
R_t & \Phi C_{t-1} \\
(\Phi C_{t-1})' & C_{t-1}
\end{bmatrix}
\Big)
\]
\1 Regression matrix, error:
\begin{align*}
B_t & = (\Phi C_{t-1})' R_t^{-1} \\
e_t & = \beta_t - a_t
\end{align*}
\1 Draw:
\begin{align*}
\ell & = m_{t-1} + B_t e_t \\
U & = C_{t-1} - (\Phi C_{t-1})' R_t^{-1} (\Phi C_{t-1}) \\
\beta_{t-1} & \mid \beta_t, D_{t-1} \sim N(\ell, U).
\end{align*}
\end{outline}

If you have missing data then
\[
\begin{cases}
m_t = a_t = \Phi m_{t-1} \\
C_t = R_t = \Phi C_{t-1} \Phi' + W.
\end{cases}
\]
This won't (I don't think) affect backwards sampling.  Whereas before you get a
reduction in variance in $C_t$ compared to $R_t$, now you do not.

\subsection{More models}

I had made a mistake in how I went about jointly sampling $(\alpha,
\{\beta_t\})$; in particular, my backwards sampling was incorrect I believe.
But it may not matter in any event.  Suppose we have the following model:
\[
\begin{cases}
y_t = x_t' \alpha + z_t' \beta_t + \nu_t, & \nu_t \sim N(0, V_t) \\
\beta_t = \mu + \Phi (\beta_{t-1} - \mu) + \omega_t, & \omega_T \sim N(0, W).
\end{cases}
\]
(Note: you cannot just marginalize $\alpha$.  That induces correlation of the
form $\cov(y_t, y_{t-1} | \{\beta_t\})$.)  

\section{Static Probit vs. Static Logit}

Effective sample size.  These are the averages for 10 simulations.  Each
simulation was run 10000 time and the first 1000 samples were discarded.  For
both the logit and probit models a non-informative $N(0, 100)$ prior was used.
The simulations were run on a workstation with an Intel Xeon 2.40 GHz CPU and 4
GB of RAM.  Below, $N$ is the number of observations and $P$ is the number of
covariates.

You will notice that the running times for the PG method are drastically lower
than found in the PG paper.  This is because the code used in that paper drew
each $\omega_i$ one at a time.  It is much faster to use the \texttt{rpg}
functions ability to draw the entire vector $\omega$ at once.

I believe the reason the Probit model does better than the Logit model when
$\beta$ has many dimensions is because you can precompute the posterior variance
(and Cholesky factor) of $p(\beta | y, z)$ before Gibbs sampling.  You cannot do
that in the Logit case.

We also calculate the average ESS over observations and simulations of the
posterior probability of each success probability.  The posterior probability
can be calculated from the log odds $\psi_i = x_i \beta$ via $p_i = \exp(\psi_i)
/ (1 + \exp(\psi_i))$ for the logit model and from $p_i = \Phi(x_i \beta)$ for
the probit model.  We may encounter values of $x_i \beta$ in the probit model
that result in $p_1 = 1$ or $0$.  In that case the ESS calculation is degenerate
and we remove it from the average.

\begin{table}
\centering
\begin{tabular}{l c c c c c c}
          & $N$  & $P$ & $P^*$ & PG.ESS   &  Pro.ESS  & BL.to.Pro \\
\hline
Diabetes  & 768  & 8   & 9  & 4849.392 & 2547.9075 & 1.903284  \\
Heart     & 270  & 13  & 19 & 3203.718 & 1470.3308 & 2.178910  \\
Australia & 690  & 14  & 35 & 3218.647 &  995.6811 & 3.232609  \\
Germany   & 1000 & 20  & 49 & 4959.180 & 2485.7040 & 1.995081  \\
\end{tabular}
\caption{The average effective sample size of $\beta$ over components of $\beta$ and 
  simulations.  $N$ is the number of observations, $P$ is the
  number of covariates, and $P^*$ is the dimension of $\beta$.}
\end{table}

\begin{table}
\centering
\begin{tabular}{l c c c c c}
           & PG.Time & Pro.Time & PG.ESS.per.Time & Pro.ESS.per.Time &
           PG.Pro.ratio \\
\hline
Diabetes   & 12.290  &    8.435 &        394.5803 &        302.06372 & 1.3062816    \\
Heart      &  7.571  &    4.547 &        423.1565 &        323.36283 & 1.3086121    \\
Australia  & 24.039  &   10.217 &        133.8927 &         97.45338 & 1.3739158    \\
Germany    & 47.668  &   16.813 &        104.0358 &        147.84417 & 0.7036857      
\end{tabular}
\caption{The average effective sample sizes per execution time.}
\end{table}

\begin{table}
\centering
\begin{tabular}{l c c}
          & PG       & Pro      \\
\hline
Diabetes  & 4719.281 & 2499.716 \\
Heart     & 3109.270 & 1568.471 \\
Australia & 2681.208 & 1363.054 \\
Germany   & 4736.249 & 2401.344
\end{tabular}
\caption{Average effective sample size of the posterior probability of success
  over observations and simulations.  Degenerate ESS removed.}
\end{table}

\section*{FS\&F for NB}

See FS 2009.

The distribution for $k$ is
\[
{k+d-1 \choose k} (1-p)^d p^k.
\]
We could think of $p$ as the probability of success and $d$ as the number of
failures, in which case $k$ is the number of successes before there are $d$
failures; but that is not the best interpretation for our purposes, which we
will get to in a moment.  You can write $k \sim NB(d, p)$ as a Poisson-Gamma
mixture
\begin{align*}
& k \sim \text{Pois}(\lambda) \\
& \lambda \sim \text{Ga}(d, \text{scale}=\alpha) \; \text{ where } \alpha = \frac{p}{1-p}.
\end{align*}
This is all from Wikipedia.  

This arises because one wants to use a Poisson like model, but to have greater
variance in the number of counts.  The mean and variance of $X \sim
\text{Pois}(\lambda)$ is $\lambda$.  The mean of the negative binomial is
\[
\bbE[k] = \bbE[\bbE[k|\lambda]] = \bbE[\lambda] = d \alpha
\]
and the variance is
\begin{align*}
\Var(k) & = \bbE[\Var(k|\lambda)] + \Var(\bbE[k|\lambda]) \\
& = \bbE[\lambda] + \Var(\lambda) \\
& = d \alpha + d \alpha^2.
\end{align*}
Letting $\mu = d \alpha$ we have
\[
\bbE[k] = \mu \; \text{ and } \; \Var(k) = \mu + \frac{\mu^2}{d}.
\]
This is like the Poisson distribution, but with greater dispersion.

All of this is great since we have a scale-mixture of exponential-like things,
which is the key to FS\&F.  In particular,
\[
\lambda \sim \alpha \text{Ga}(d, 1)
\]
so that taking the $\log$ of both sides you have
\[
\log(\lambda) = \psi + \log(\text{Ga}(d,1))
\]
(or alternatively $\log (\lambda) = \psi - - \log(\text{Ga}(d,1))$) where
\[
\psi = \log \alpha, \; \text{ the ``log odds.''}
\]
I put this in quotes, because it isn't exactly the quantity we desire, but this
is what we model when using the \Polya-Gamma trick.  We can also write
\[
\psi = \log \mu - \log d,
\]
which shows that are choice of $\psi$ above is just a scaled version of the log
mean counts.  So long as we include an intercept in our regression we will have
the same posteriors for all predictors and produce the same forecasts.

Now we can use the normal mixture trick.  When $d=1$, we have an exponential, so
we can use FS\&F mixture where the mixture is $N(-m_r, \sigma_r^2)$ since she
looks at $-\log(\text{Ex}(1))$.  Everything else is conjugate.  In particular,
\[
p(k | \alpha, \lambda) = \frac{e^\lambda \lambda^k}{k!} 
\frac{\alpha^{-d} \lambda^{d-1}}{\Gamma(d)} e^{-\lambda / \alpha}.
\]
where $\alpha = \mu / d = e^\psi / d$.  To sample $\lambda$, which is just an
intermediate auxilliary, we have
\[
p(\lambda | \alpha, k) \propto \lambda^{k+d-1} e^{-\lambda (1+1/\alpha)} \propto
\text{Ga}(k+d, \text{rate}=1+1/\alpha).
\]
To sample $r$ we have a discrete mixture,
\[
p(r|\lambda, \psi) \propto \sigma_{r}^{-1} \exp 
\Big[ \frac{-\sigma_r^{-2}}{2} \Big(\log(\lambda) - \psi + m_r \Big)^2 \Big].
\]
This seems way too easy.  This would show that it is sometime easier to sample
what appears to be a more complicated distribution (a mixture vs. a non-mixture).

\subsection*{Estimating $d$}

From above it seems like we might be able to estimate $d$.  But this does not
work for FS\&F, since we need to know $d$ for the normal mixture.  However, I
think we might be able to use the Polya-Gamma trick. 

We know a few things.  First we know that $p(k | \alpha, d)$ is negative
binomial, which means that we can use the PG trick.  We also know that we can
sample $p(\lambda | \alpha, k, d)$.  Thus we should be able to sample
\[
p(\alpha, \lambda | k, d) =  p(\alpha | k, d) p(\lambda | \alpha, k, d)
\]
Furthermore, it looks to me like we can sample
\[
p(d | \alpha, \lambda).
\]
Thus we could maybe Gibbs sample so that
\begin{enumerate}
\item Sample $(\omega | \alpha, k, d)$.
\item Sample $(\alpha | \omega, k, d)$.
\item Sample $(\lambda | \alpha, k, d)$.
\end{enumerate}
Actually, \textbf{THIS IS WRONG}.  We need to sample
\[
(\lambda | \alpha, k, d, \omega).
\]
We could sample $\alpha$, approximately, be doing several PG steps.  What
happens when you do the above.  Can you take only one auxiliary step and still
guarentee covergence of the chain?  If you are going to do several auxiliary
steps, you might as well just look at the likelihood for $d$
\[
\frac{\Gamma(k+d)}{\Gamma(d)} (1-p)^d.
\]
I think it is possible to do something with this, but it is going to get ugly.
It is like a sum of gamma densities or something.

\section{Dynamic Principle Componenent Regression}

See Christiansen p. 385 for a more thorough discussion of principle component
regression.  Principle component regression is different than principle
component analysis.

With PCA you find orthogonal latent factors.  Given knowledge of the population
\[
x \sim N(0, \Sigma)
\]
then we know that
\[
x = Vy, \; y \sim N(0, D^2)
\]
where $\Sigma = VD^2V'$.  When working with a population $X \sim MN(0, I,
\Sigma)$ (where we have i.i.d rows) then we can take the SVD $X = \hat U \hat D
\hat V'$ so that $\hat V$ is an estimate of $V$ and $\hat D^2$ is an estimate of
$n D^2$ (or something like that).

In PC regression we take the design matrix $X = U D V'$ and then transform
\[
y = X \beta + \ep
\]
into
\[
U_*' y = 
\begin{bmatrix}
D \\ 0
\end{bmatrix}
\gamma + \ep.
\]
where $U_*$ has been made full rank by adding additional orthonormal columns.
The variance of $\gamma_i$ is $\sigma^2 / d_i^2$ thus if some of the $d_i$ are
small then it may be reasonable to drop the corresponding variables $\gamma_i$
from the regression.

James wants to use princple component regression in the dynamic case.  Suppose
that $X = UDV$ and that $x_t$ is a row of $X$ and $\beta_t$ as a column of
$\beta$.  Then our dynamic regression involves
\[
y_t = x_t \cdot \beta_t + \ep_t.
\]
Specifically
\[
y_t = \sum_{i} X_{ti} \beta_{it} + \ep_t
\]
The SVD of $X$ can be written as
\[
X_{ti} = \sum_{j} d_j u_{tj} v_{ij}.
\]
Thus we have
\[
y_t = \sum_i \sum_j d_j u_{tj} v_{ij} \beta_{it} + \ep_t.
\]
Letting
\[
\gamma_{jt} = \sum_i v_{ij} \beta_{it}
\]
that is
\[
\gamma_t = V' \beta_t.
\]
Thus we have
\[
y_t = \sum_{j} d_j u_{tj} \gamma_{jt} + \ep_t
\]
or
\[
y_t = z_t \cdot \gamma_t + \ep_t
\]
where
\[
z_{tj} = d_j u_{tj} \text{ or } z_t = u_t D \text{ or } Z = UD.
\]
Just assuming that the logic carries over, if $d_j$ is small, then remove it
from the regression to reduce the dimensionality of the problem.

Actually, this may be slightly different than princple components regression.
It appears that principal components removes the mean in each column of $X$.
This would align with the initial assumptions we mentioned.

I think the same is going to hold here.  The only thing that matters is the
column space of $X$ so we can do a change of variables so that
\[
y = \alpha 1 + X \beta + \ep
\]
produces the same fitted values and residuals as
\[
y = \kappa 1 + (X - 1 m_x') \gamma + \ep.
\]
The same argument holds as above, but now we do the SVD on $X- 1 m_x'$.  But now
we ensure that we can have an intercept.  In that case we have
\[
y_t = \kappa + z_t \gamma_t + \ep_t.
\]
After $X$ has been normalized, take the SVD to get $UDV'$.  $Z=UD$ is called the
scores I believe.

\section{Dynamic Negative Binomial}

Unfortunately, it appears that FS may be competitive.  I have yet to do the
actually simulations; but it appears that \cite{fruhwirth-schnatter-etal-2009}
may have a better ESR than the PG method.  This is due to the relative
inefficieny when calculating $PG(y+d, \psi)$ random variables.  Whenver the
count $y$ is high enough it is going to take a while to simulate all of the
random variables we need.  Yet more motivation for finding a better $PG(n, z)$
sampler.

See \texttt{notes.tex} for more discussion of negative binomial.

In those models we have
\[
\begin{cases}
y_t \sim \Pois(\lambda_t) \\
\lambda_t \sim \mu_t \Ga(d, \text{scale}=1/d) \\
\log(\mu_t) = x_t \beta.
\end{cases}
\]
We are modeling the log-mean (or log-odds).  We can make this dynamic by In
those models we have
\[
\begin{cases}
y_t \sim \Pois(\lambda_t) \\
\lambda_t \sim \mu_t \Ga(d, \text{scale}=1/d) \\
\lambda_t \sim \alpha_t \Ga(d, 1) \\
\log(\mu_t) = x_t \beta_t \\
\log(\alpha_t) = x_t \beta_t - \log(d) \\
\beta_t = \beta_{t-1} + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]
Or if we are following the principle components regression above we have
\[
\begin{cases}
y_t \sim \Pois(\lambda_t) \\
\lambda_t \sim \mu_t \Ga(d, \text{scale}=1/d) \\
\lambda_t \sim \alpha_t \Ga(d, 1) \\
\log(\mu_t) = \hat \kappa + z_t \beta_t \\
\log(\alpha_t) = \kappa + z_t \beta_t, & \kappa = \hat \kappa - \log(d) \\
\beta_t = \beta_{t-1} + \omega_t, & \omega_t \sim N(0, W).
\end{cases}
\]
where $z_t$ now does not include an intercept.  It is convenient to have several
quantities to work with so 
\[
\begin{cases}
\zeta_t = x_t \beta_t \\
\psi_t = \zeta_t - \log(d) \\
\psi_t = \log(\alpha_t) \\
\alpha_t = \frac{p_t}{1-p_t}.
\end{cases}
\]
We could model the log-odds directly.  There is an R-script
\texttt{NBPG-logodds.R} that does that in the non-dynamic case.  But it is
traditional to model the log-mean.

For FSF, I think pretty much nothing changes between the static and dynamic
case:

\begin{enumerate}

\item Sample $(d, r, \lambda | \{\beta_t\})$ via

\begin{enumerate}
\item $p(d | \mu, y)$ where $\mu = \{\mu_t\}$
\item $p(\lambda | \alpha(d), d, y)$ where $\alpha = \alpha_t(d)$
\item $p(r | \lambda, \alpha, d, y)$.
\end{enumerate}

\item Sample $( \{\beta_t\} | r, \lambda, d, y )$.

  We need to FFBS now instead of LS using our normal-mixture term for the
  observation.

  Also we need to calculate $\mu_t$ differently.

  We also must deal with sampling the intercept.

\end{enumerate}

For the PG method we again sample $p(d, \omega | y, \beta)$ by $p(d | y, \beta)
p(\omega | d, y, \beta)$.  The likelihood for $\{\beta_t\}, \omega$ is
\[
\propto \prod_{t=1}^T e^{\psi_i \kappa_i} e^{-\omega_i \psi_i^2 / 2} p(\omega_i | y_i + d_i).
\]
where $\psi_t = \hat \iota + x_t \beta_t - \log(d)$.  In this case we see that
it really isn't a big deal to model the log-odds instead of the log-mean.  If we
model $\psi_t = \iota + x_t \beta_t$ where $\iota = \hat \iota - \log d$ then we
can recover $\mu_t = \psi_i + \log d$.  In either case though it doesn't really
matter.  We can always chose to roll $\log d$ into $z$.

Following the identical logic as above if we multiply the likelihood by
$\exp - \frac{1}{2} z \Omega z$ where $\Omega z = \kappa$ then we have
\[
\exp \Big( -\frac{1}{2} (z - \psi)' \Omega (z- \psi) \Big).
\]
Thus we have
\[
\begin{cases}
z = \psi + \ep, \; \ep \sim N(0, \Omega^{-1}) \\
\Omega z = \kappa
\end{cases}
\]
which becomes
\[
\begin{cases}
z_t = \iota + x_t \beta_t + \ep_t, & \ep_t \sim N(0, 1/\omega_t) \\
z_t = \kappa_t / \omega_t \\
\beta_t = \phi \beta_{t-1} + \nu_t, & \nu_t \sim N(0, W).
\end{cases}
\]
Or in the case of log-mean we have
\[
\begin{cases}
\hat z_t = z_t + \log(d) = \hat \iota + x_t \beta_t + \ep_t, 
  & \ep_t \sim N(0, 1/\omega_t) \\
z_t = \kappa_t / \omega_t \\
\beta_t = \phi \beta_{t-1} + \nu_t, & \nu_t \sim N(0, W).
\end{cases}
\]
I don't think it makes sense to have an intercept $\iota$ when we are using a
random walk.  If we have some covariates then I think it is okay.  But if we
just had
\[
\begin{cases}
z_t = \iota + \beta_t + \ep_t \\
\beta_t = \beta_{t-1} + \nu_t
\end{cases}
\]
then $\beta_t$ is not identified.  (Recall however, that Scott 2011 mentions
something about working parameters.  Evidently it is sometime okay to let
parameters not be identified and then to later identify them.)

I believe in either case we have
\[
\begin{cases}
z_t = \iota + x_t \beta_t + \ep_t, \; \ep_t N(0, V_t) \\
\beta_t \sim GP
\end{cases}
\]
where GP refers to Gaussian process, in place of AR(1) or a random walk or
whatever it is that we have.

We need to be somewhat careful about whether we are using covariates or not.
When we have
\[
\begin{cases}
z_t = \iota + x_t \beta_t + \ep_t \\
\beta_t \sim AR(1)
\end{cases}
\]
it is okay where $\beta_t$ has a non-zero mean.
When we have
\[
\begin{cases}
z_t = \iota + \beta_t + \ep_t \\
\beta_t \sim AR(1)
\end{cases}
\]
then $\beta_t$ should not have a mean.  When $\beta_t$ is a random walk then
there should be no mean level $\iota$.

\section{CUBS with MH}

\subsection{CUBS}

CUBS: conjugate update backward sample.  \cite{west-etal-1985} used conjugate
updating to generate a statistical model for forward filtering and backwards
sampling for generalized linear models.  The basic idea is to use linear Bayes.
They take an approximate backwards sample using the normal DLM recursions.
\cite{ravines-etal-2006} suggested using a Metropolis step to make this exact.
This works for exponential families, but I focus on the dynamic binary logistic
model since that is what we will work with.  There is a relatively small change
to go between models, related to the conjugte updating step and the densities
for the MH step.

The basic model is
\[
\begin{cases}
y_t \sim f(\psi_t) \\
\psi_t = x_t \beta_t \\
\beta_t = \mu + \Phi( \beta_{t-1} - \mu ) + \omega_t, & \omega_t \sim [0, W].
\end{cases}
\]
To remind myself.  Things are not completely specified here since we haven't
defined the distribution of $\omega_t$.  In general, we can think of this as the
likelihood $y_t \sim f(\psi_t), \psi_t = x_t \beta_t$ and a ``prior'' on $\beta$
which controls how the $\beta_t$ are correlated in $t$, which will be defined by
the above recursion and $\omega_t \sim N(0, W)$.

We can filter forward using linear Bayes without specifying the distribution of
$\omega_t$.  This will give us a bunch of approximate moments.  We can then use
those moments to backwards sample.  This is then the proposal we use for a
Ind.\ Metrpolis step.

We forward filter using a conjugate updating step as follows.  Assume everything
is conditioned upon $D_{t-1}$.

\begin{itemize}

\item \textbf{Prior}: $\beta_{t-1} \sim [m_{t-1}, C_{t-1}]$.

\item \textbf{Predictive}: $\beta_{t-1} \sim [a_t, R_t]$ where
\[
\begin{cases}
a_t = \Phi m_{t-1} + (1 - \Phi) \mu \\
R_t = \Phi C_{t-1} \Phi + W. \\
\end{cases}
\]

\item \textbf{Predictive}: $\psi_t \sim [f_t, q_t]$ where
\[
\begin{cases}
f_t = x_t a_t \\
q_t = x_t R_t x_t'.
\end{cases}
\]

\item \textbf{Conjugate Updating}: Knowing the moments of $\psi_t$, we assume
  $\psi_t$ is distributed with something conjugate to the likelihood, whose
  parameters are matched to the moments of $\psi_t$.  We then update and go
  backwards, calculating the moments of the posterior.  That is given $\psi_i
  \sim [f_t, q_t]$, we can take the moments and map them to parameters $r_t,
  s_t$ so that $\psi_t \sim p(r_t, s_t)$.  Then update so that $(\psi_t | y_t)
  \sim p(r_t^*, s_t^*)$.  Now map back to means and variances so that $(\psi_t |
  y_t) \sim [f_t^*, q_t^*]$.

\item \textbf{Posterior}: Perhaps not being completely scrupulous, (since $p(\psi_t | \theta_t)$ 
  is not a density), we have (from \cite{west-etal-1985})
  \begin{align*}
    p(\beta_t, \psi_t, y_t | D_{t-1}) 
    & \propto p(y_t | \psi_t, D_{t-1}) p(\psi_t, \beta_t | D_{t-1}) \\
    & \propto p(y_t | \psi_t, D_{t-1}) p(\psi_t | D_{t-1}) p(\beta_t | \psi_t,
    D_{t-1}) \\
    & \propto p(\psi_t | y_t, D_{t-1}) p(\beta_t | \psi_t, D_{t-1}).
  \end{align*}
  So we can marginalize over $(\psi_t | D_t)$ to get $p(\beta_t | D_{t-1})$.
  Now we can use what we know about moments to update.  We know that $(\beta_t |
  \psi_t, D_{t-1})$ has a distribution with mean
  \[
  a_t + A_t (\psi_t - f_t)
  \]
  and variance, take $\rho_t = R_t x_t'$
  \[
  R_t - \rho_t \rho_t' / q_t.
  \]
  Thus the mean is
  \[
  m_t = \bbE[\beta_t | D_{t-1}] = \bbE[ a_t + A_t (\psi_t - f_t) | D_{t-1}] = a_t + A_t (f_t^* - f_t)
  \]
  and the variance is
  \[
  \Var[\beta_t | D_{t-1}] = \bbE[ \Var(\beta_t | \psi_t, D_{t-1}) | D_{t-1}) ] +
  \Var( \bbE[\beta_t | \psi_t, D_{t-1}] | D_{t-1})
  \]
  which is
  \[
  C_t = A_t q_t^* A_t' + (R_t - \rho_t \rho_t' / q_t) = R_t - \rho_t \rho_t' (1/q_t -
  q_t^* / q_t^2).
  \]

\end{itemize}

To backwards sample we proceed assuming that all of the moments we calculated
correspond to normal distributions.  See above.

At each step we need to transform to a conjugate prior and update.  At the end
of the Ravines paper they have a list of common conjugate priors and how to
update.

\begin{itemize}

\item Binary Logit: the likelihood determined by $p(y_t = 1) = p_t$ has a beta
  conjugate prior.  The distribution $p \sim beta(\alpha, \beta)$ and $\psi =
  \log (p / (1-p))$ determine the distribution of $\psi$.  From Ravines, when
  $\psi_t \sim [f_t, q_t]$ we have
  \[
  p_t \sim Beta(r_t, s_t) 
  \]
  where
  \[
  \begin{cases}
    f_t = \gamma(r_t) - \gamma(s_t) \\
    q_t = \gamma'(r_t) + \gamma'(s_t),
  \end{cases}
  \]
  $\gamma$ is the digamma function and $\gamma'$ is the trigamma function.  We
  need to solve for $r_t$ and $s_t$ numerically when we know $f_t, q_t$ and want
  to find $r_t, s_t$.  They list approximations in the Ravines paper, which are
  $f_t \sim \log(r_t) - \log(s_t)$ and $q_t = 1/r_t + 1/s_t$ respectively.  When
  updating you go from
  \[
  \begin{cases}
    r_t \ra r_t^* = y_t + r_t \\
    s_t \ra s_t^* = n_t - y_t + s_t.
  \end{cases}
  \]
  Then we go back to $f_t^*$ and $q_t^*$ by above.

\item Binary Logit: the beta distribution is conjugate to the binomial
  distribution.  Consider the beta distribution $beta(r, s)$, which is
  \[
  f(p) = p^{r-1} (1-p)^{s-1} / \beta(r, s).
  \]
  Now do a change of variables from the probability scale to the log-odds
  scale.  In particular $p = (1 + e^{-\psi})^{-1}$ so that
  \[
  dp = (-1) (1 + e^{-\psi})^{-2} (-1) e^{-\psi} d \psi = e^{\psi} /
  (1+e^{\psi})^2 d \psi
  \]
  Thus
  \begin{align*}
  \frac{(e^{\psi})^{r-1}}{(1+e^\psi)^{r-1}}  \frac{1}{(1+e^\psi)^{s-1}} d p
  & = 
  \frac{(e^{\psi})^{r-1}}{(1+e^{\psi})^{s+r-2}} \frac{e^{\psi}}{(1+e^{\psi})^2} d
  \psi \\
  & = \frac{(e^{\psi})^{r}}{(1+e^{\psi})^{s+r}} d \psi.
  \end{align*}
  Thus we know that
  \[
  \int_\bbR  \frac{(e^{\psi})^{r}}{(1+e^{\psi})^{s+r}} d \psi = \beta(r, s).
  \]
  Thus the moment generating function of $\psi$ is
  \[
  \beta(r,s)^{-1} \int_{\bbR} \frac{(e^{\psi})^{r}e^{\psi
      t}}{(1+e^{\psi})^{s+r}} d \psi 
  = \beta(r+t, s-t) / \beta(r,s).
  \]
  The $\beta$ function is
  \[
  \beta(r,s) = \frac{\Gamma(r) \Gamma(s)}{\Gamma(r+s)}.
  \]
  It will be easier to write it in the following way:
  \[
  \beta(r+t, s) = \exp \Big( \ln \Gamma(r+t) + \ln \Gamma(s-t) - \ln \Gamma(r+s) \Big).
  \]
  Letting $'$ denote differentiation with respect to $t$ we have
  \[
  \beta'(r+t, s-t) = \beta(r+t, s-t) \Big[ \gamma(r+t) - \gamma(s-t) \Big]
  \]
  and
  \[
  \beta''(r+t, s-t) = \beta(r+t, s-t) \Big[ \gamma(r+t) - \gamma(s-t) \Big]^2
  + \beta(r+t, s-t) \Big[ \gamma_1(r+t) + \gamma_1(s-t) \Big]
  \]
  where $\gamma$ is the digamma function and $\gamma_1$ is the trigamma
  function.  Evaluating these at zero and dividing by $\beta(r,s)$ we find that
  the first and second moments are
  \[
  \bbE[\psi] = \gamma(r) - \gamma(s)
  \]
  and
  \[
  \bbE[\psi^2] = \Big[ \gamma(r) - \gamma(s) \Big]^2
  + \Big[ \gamma_1(r) + \gamma_1(s) \Big].
  \]
  Thus
  \[
  \Var(\psi) = \gamma_1(r) + \gamma_1(s).
  \]



  

\end{itemize}

\subsection{Correction (MH)}

We need to consider the posterior distribution, or a proportional quantity,
which amounts to the conditional density multiplied by the prior.  In the case
of the target density we have
\[
\Big[ \prod_{i=1}^T p(y_i | \beta_i) p(\beta_i | \beta_{i-1}) \Big] p(\beta_0 |
m_0, C_0),
\]
which is a product of observation, conditional (AR(1)) densities, and the prior.
In the case of the proposal density, we sampled according to
\[
p(\beta_t | D_T) \Big[ \prod_{i=T}^1 p(\beta_{t-1} | \beta_t, D_{t-1}) \Big]
\]
where $(\beta_t | D_T) \sim N(m_T, C_T)$ and $(\beta_{t-1} | \beta_t, D_{t-1})$
is normal with conditional mean and variance
\[
\begin{cases}
\bbE[\beta_{t-1} | \beta_t, D_{t-1}] = m_{t-1} + A_{bs,t} (\beta_t - a_t) \\
\Var[\beta_{t-1} | \beta_t, D_{t-1}] = C_{t-1} + C_{t-1} \Phi R_t^{-1} \Phi C_{t-1}
\end{cases}
\]
where
\[
A_{bs,t} = C_{t-1} \Phi R_t^{-1}.
\]

Recall that for MH the acceptance probability is determined by the quantity
\[
\frac
{f(x^*) / q(x^* | x)}
{f(x  ) / q(x | x^*)}
\]
where $x^*$ is the proposal, $f$ is the target density, and $q$ is the proposal
density.  In the case of independence Metropolis, the proposal does not depend
on the prevous state so we can calculate $f(x) / q(x)$ and then record that for
the next iteration.

\begin{itemize}

\item \textbf{binomial logistic}: In the binomial case, the target observation
  density is
  \[
  p(y_i | \beta_i) = {n_i \choose y_i} \frac{(e^{\psi_i})^{y_i}}{(1+e^{\psi_i})^{n_i}}.
  \]
  where $\psi_i = x_i \beta_i$.

\end{itemize}

\section{Dynamic Binary Logistic Regression}

\section{Likelihood of DGLM:}

My sampler is having problems.  It seems like it may have to do with some sort
of ill-posedness when allowing one to estimate $\phi$ and $W$.  It also appears
when I have a multidimensional $\beta$.  So what is the Hessian?  Also, there
are more degrees of freedom than there are observations, though $\beta$ is
correlated so those aren't completely free parameters.  How does that affect
things?  I will break things up into two pieces.  The piece I get from the
evolution of $\beta$ and the piece I get from the observation.

Let us consider the observation equation first.
\[
\log \prod_{i=1}^T |W|^{-1/2} \exp \Big[ - \frac{1}{2} (\beta_t - \Phi
\beta_{t-1})' W^{-1} (\beta_t - \Phi \beta_{t-1}) \Big].
\]
and for the sake of simplicity, let's replace the variance with the precision:
\[
\log \prod_{t=1}^T |\Xi|^{1/2} \exp \Big[ - \frac{1}{2} (\beta_t - \Phi
\beta_{t-1})' \Xi (\beta_t - \Phi \beta_{t-1}) \Big].
\]
This is
\[
\frac{T}{2} \log |\Xi| - \frac{1}{2} \sum_{t=1}^T (\beta_t - \Phi
\beta_{t-1})' \Xi (\beta_t - \Phi \beta_{t-1}).
\]
A catch is that we are not just interested in $\phi$ and $\Xi$, we eventually
must estimate $\beta$ as well.  For now I will ignore the prior, though later
maybe we will need to add that back in.  Let's start taking directional
derivatives.  

Let's just consider a single term in the sum for now:
\[
\frac{1}{2} (\beta_t - \Phi \beta_{t-1})' \Xi (\beta_t - \Phi \beta_{t-1}).
\]
\begin{description}
\item[$\beta_t$:] \hfill
\[
\frac{\del}{\del \beta_t} (\cdot) = (\beta_t - \Phi \beta_{t-1})' \Xi h^{\beta_t}
\]

\item[$\beta_{t-1}$:] \hfill
\[
\frac{\del}{\del \beta_{t-1}} (\cdot) = -(\beta_t - \Phi \beta_{t-1})' \Xi \Phi h^{\beta_{t-1}}.
\]

\item[$\phi$:] \hfill
\[
\frac{\del}{\del \phi} (\cdot) = - (\beta_t - \Phi \beta_{t-1})' \Xi
\text{d}(\beta_{t-1}) h^{\phi}
\]
where $d(\cdot)$ takes a vector and sends it to a diagonal matrix.  We can do
this because $d(\phi) \beta_{t-1} = d(\beta_{t-1}) \phi$.

\item[$\Xi$:] \hfill
\[
\frac{\del}{\del \Xi} (\cdot) = \frac{1}{2} (\beta_t - \Phi \beta_{t-1})' H^{\Xi} (\beta_t - \Phi \beta_{t-1}).
\]

\item[$\beta_t$ and $\beta_t$:] \hfill
\[
\frac{\del^2}{\del \beta_t \del \beta_t} \cdot = {h^{\beta_t}_2}' \Xi h^{\beta_t}_1.
\]

\item[$\beta_t$ and $\beta_{t-1}$:] \hfill
\[
\frac{\del^2}{\del \beta_t \del \beta_{t-1}} (\cdot) = - {h^{\beta_t}_2}' \Xi h^{\beta_{t-1}}_1.
\]

\item[$\beta_t$ and $\phi$:] \hfill
\[
\frac{\del^2}{\del \beta_t \del \phi} (\cdot) = - {h^{\beta_t}_2}' \Xi d(\beta_{t-1}) h^{\phi}_1.
\]

\item[$\beta_t$ and $\Xi$:] \hfill
\[
\frac{\del^2}{\del \beta_t \del \Xi} (\cdot) = (\beta_t - \Phi \beta_{t-1})'
H^{\Xi}_1 h^{\beta_t}_2.
\]

\item[$\beta_{t-1}$ and $\beta_{t-1}$:]
\[
\frac{\del^2}{\del \beta_{t-1} \del \beta_{t-1}} (\cdot) = h^{\beta_{t-1}}_2
\Phi \Xi \Phi h^{\beta_{t-1}}_1.
\]

\item[$\beta_{t-1}$ and $\phi$:]
\[
\frac{\del^2}{\del \beta_{t-1} \del \phi} (\cdot) = 
 {h^{\beta_{t-1}}_2}' \Phi \Xi d(\beta_{t-1}) h^{\phi}_1
- (\beta_t - \Phi \beta_{t-1})' \Xi d(h^{\phi}_1) h^{\beta_{t-1}}_2.
\]

\item[$\beta_{t-1}$ and $\Xi$:]
\[
\frac{\del^2}{\del \beta_{t-1} \del \Xi}(\cdot) = 
-(\beta_{t-1} - \Phi \beta_{t-1})' H^{\Xi}_1 h^{\beta_{t-1}}_2.
\]

\item[$\phi$ and $\phi$:]
\[
\frac{\del^2}{\del \phi \del \phi}(\cdot) = {h^{\phi}_2}' d(\beta_{t-1}) \Xi
d(\beta_{t-1}) h^{\phi}_1.
\]

\item[$\phi$ and $\Xi$:]
\[
\frac{\del^2}{\del \phi \del \Xi} = - (\beta_t - \Phi \beta_{t-1})' H^{\Xi}_1
d(\beta_{t-1}) h^{\phi}_2.
\]

\item[$\Xi$ and $\Xi$:]
\[
\frac{\del^2}{\del \Xi \del \Xi} (\cdot) = 0.
\]

\end{description}

And now let's consider $\log |\Xi|$:
\begin{description}

\item[$\Xi$:]

\item[$\Xi$ and $\Xi$:]

\end{description}

The observation equation is only going to help us with the $\beta$ terms.

\subsection{Stochastic Volatility}

A single term from a stochastic volatility observation is
\[
e^{-x_t \beta_t/2} e^{-y_t^2 e^{-x_t \beta_t} / 2}
\]
so the log of that is
\[
- \frac{x_t \beta_t}{2} - \frac{y_t^2}{2} e^{-x_t \beta_t}.
\]
Taking derivatives we have
\begin{description}

\item[$\beta_t$:] \hfill
\[
\frac{\del}{\del \beta_{t}}(\cdot) = - \frac{x_t h^{\beta_t}}{2} 
+ \frac{y_t^2}{2} e^{-x_t \beta_t} x_t h^{\beta_t}.
\]

\item[$\beta_t$ and $\beta_t$:]
\[
\frac{\del^2}{\del \beta_t \del \beta_{t}}(\cdot) = 
-\frac{y_t^2}{2} e^{-x_t \beta_t} x_t h^{\beta_t}_2 x_t h^{\beta_t}_1.
\]

\end{description}

\section{Dynamic Regression (Again)}

I believe \cite{shephard-pitt-1997} relies on quasi-likelihoods.  Recall, for
the moment, that in general, we have
\[
p(y|\theta) p(\theta) = p(y,\theta).
\]
Hence, 
\[
p(\theta | y) = p(y,\theta) / p(y).
\]
Or we could be estimating some of the $\theta$'s, call the subset of interest
$\theta_1$ and the remaining, $\theta_2$.  Then we have
\[
p(\theta_1 | y, \theta_2) = p(y,\theta) / p(y, \theta_2).
\]
When finding a Laplace approximation in either case, the normalizing constant
does not matter, since it is not a function of $\theta$ or $\theta_1$,
respectively.  We just need the joint distribution, i.e. the data-generating
process with the prior, to calculate the appropriate gradient and Hessian.
Also, I believe we could do a single Netwon set to update $\theta$ or we could
combine two Newton steps to update $\theta_1$ and $\theta_2$, which would
essentially be coordinate ascent and Newton.  The same applies to MH sampling.
We can come up with the Laplace approximation and then using it to sample in
block-like fashion.

In the dynamic case, Shephard and Pitt suggest using a transformation from
$\theta$ to $\omega$, which corresponds to the innovation variance.  In the case
that
\[
\theta_t = \Phi \theta_{t-1} + \omega_t
\]
this corresponds to $A \theta = \omega$, where $A$ is the lower triangular
matrix
\[
A =
\begin{pmatrix}
I & 0 & 0 & 0 & \ldots \\
-\Phi & I & 0 & 0 & \ldots \\
0 & -\Phi & I & 0 & \ldots \\
0 & 0 & -\Phi & I & \ddots \\
\vdots & \vdots & \vdots & \ddots & \ddots
\end{pmatrix}
\]
and $\theta = \{\theta_0, \ldots, \theta_T\}$.  Sovling $Ax = b$ is fast.  In
fact, we know that $\theta_0 = \omega_0$ and $\theta_t = \omega_t + \Phi
\theta_{t-1}$, we can solve this inductively (forward solve).  We can also
invert $A$ easily, it is lower triangular.  We know that $\theta$ has a nice
Markovian factorization and that this can be used for forward filtering and
backwards sampling.  However, it isn't obvious to me that has nice structure
when conditioning on ``future'' values of $\theta_t$.  Though, as the
calculation in the previous section shows, it seems like that factorization
should still be useful when finding a Laplace approximation.  It seems like we
should still get a digonal Hessian.

Let $\ell(\theta)$ denote the likelihood.  In the case of exponential studies,
this likelihood has been studied extensively.  I should be able to find the
closed form solution to $\nabla_\theta \ell$.  From that, one should get
something like
\[
\nabla_\omega \ell (\theta(\omega)) = \nabla_\theta \ell(\theta) A^{-1} h^{\omega}.
\]
A similar calculation is available for the Hessian:
\[
D^2_\omega \ell (\theta(\omega)) = (h_2^{\omega})' (A^{-1})' D^2_\theta \ell(\theta)
(A^{-1}) h_1^{\omega}.
\]

For our problems, we have the log-odds, $\psi_t = x_t \beta_t$, the states
$\beta_t$, and the innovations $\omega_t$.  One minor concern I have is
incorporating an intercept, which previously I did using FFBS.  Though, in the
MCMC case it may not matter.

So we should have $\ell(\psi) = \sum_{i=1}^T \ell(\psi_i)$, where I am abusing
notation by using two $\ell$'s.  In the dynamic case $\psi = X \theta$ where $X
= \diag\{x_t'\}$.  Then
\[
\frac{\del^2 \ell}{\del \theta_j \del \theta_j} = x_j \ell''(x_t \theta_t) x_j'
\]
and
\[
\frac{\del^2 \ell}{\del \theta_i \del \theta_j} = 0 \, , \; i \neq j. 
\]
So the Hessian for the LLH in $\theta$ is block diabonal with rank-1 blocks.  We
regularize this using the prior.  We can transform this using $A$.  Or, we can
use a submatrix of $A$ is we only care about updating a certain portion of
$\omega$.

In particular, let $\theta^{(t)}_{i} = \theta_{t+i}$.  Then given,
$\theta^{(t)}_{0:k+1}$ we can map $\theta^{(t)}_{1:k}$ to $\omega^{(t)}_{1:k}$.

There are a couple of options I need to think about here.  We can draw $\theta$
in blocks.  We can draw $\omega$ in blocks.  We can draw $\theta$ in blocks by
drawing $p(\omega_1 | y, \theta_2)$ and then transforming $\omega_1$ to
$\theta_1$.  It seems to me that this last option is what Shephard and Pitt are
suggesting.  Note that $A'A = P$ where $P$ is the precision matrix of the prior.
$A$ is lower bidiagonal and $P$ is tridiagonal.  We know that $\omega = A^{-1}
\theta$ will lead to a term of the form $\omega' \omega$ in the log-posterior,
effecitively making the ``prior'' of $\omega$ diagonal.  (Since we are just
scaling things, this does, in fact, correspond to the true prior.)  This is
telling us that we know how to factor tridiagonal matrices.  Now imagine that we
want to sample $\theta_1$ given $\theta_2$.  Let $H$ be the Hessian of the
log-likelihood of $\theta$ and $P$ be the precision of the prior of $\theta$, where
the prior is assumed to be Gaussian.  Then we should have something like
\[
\theta_1' (H_{11} + P_{11}) \theta_{1} - 2 \theta_1' (H_{12} + P_{12}) \theta_2
- 2 \theta_1' G_1
\]
where $G_1$ is the gradient of the log-likelihood.  (I have assumed we are
centering around 0.)  We know that $H_{11}$ is block diagonal and $P_{11}$ is
tridiagonal for our purposes.  We also that we can transform $\theta_1$ so that
$P_{11}$ will turn into a diagonal matrix.  (When transforming we need to be
careful.  I believe we want to think about getting the Laplace approximation to
$p(\theta_1 | y, \theta_2)$ and then transforming $\theta_1$ to $\omega_1$,
which alters the distribution.  Since we are working with a Gaussian
approximation, everything will work out nicely.)  It is also useful to think
carefully about $H_{12}$ and $P_{12}$.  $H_{12}$ should be zero because $H$ is
block diagonal.  $P_{12}$ should mostly be zero, since $P$ is tridiagonal.  Thus
only the $\theta$ values on the border should matter.  (Since $H$ is block
diagonal, we should only have to update that in blocks as a function of
$\theta$.  $P$ is fixed as a function of $\theta$.  We will need to recalculate
both when we change the parameters of the AR model.)

\section{Dynamic MH (III)}

(See \citep{wedderburn-1974}.)  Consider the static case.  Suppose we
have log-likelihood $\sum_{i=1}^N \ell(\psi_i)$ and log prior $\rho(\beta)$.
Assume the prior is Gaussian.  Then the second order approximation to the
log-posterior is completely determined by the log-likelihood.  In that case, we
have
\[
\frac{\del}{\del \beta} \sum_{i=1}^N \ell(\psi_i) = 
\sum_{i=1}^N \ell'(\psi_i) x_i'
\]
as $\psi_i = x_i' \beta$.  The second derivative is
\[
\frac{\del}{\del \beta} \sum_{i=1}^N \ell'(\psi_i) x_i'
= \sum_{i=1}^N \ell'(\psi_i) x_i x_i'.
\]
Suppose we center this about the previous $\beta$, $\beta^{(p)}$.  Then we have
\[
\frac{1}{2} (\beta - \beta^{(p)})' \Big[  \sum_{i=1}^N \ell''(\psi_i^{(p)}) x_i x_i'
\Big]  (\beta - \beta^{(p)})'  + \Big[ \sum_{i=1}^N \ell'(\psi_i^{(p)}) x_i' \Big]
(\beta - \beta^{(p)}).
\]
To maximize, we could consider $\delta \beta = \beta - \beta^{(p)}$, or we can
consider the expression above directly.  We do the latter.  Notice that at this
point, we can follow a Newton scheme and we would be done.  But we can also
reinterpret this as a least squares problem.  To do that, recall that a LS
solution has the form $(X'DX)^{-1} X'Dy$.  Finding the quadratic form in just
$\beta$ produces
\[
\frac{1}{2} \beta' \Big[  \sum_{i=1}^N \ell''(\psi_i^{(p)}) x_i x_i'
\Big] \beta + \Big[ \sum_{i=1}^N \ell'(\psi_i^{(p)}) x' - 
\sum_{i=1}^N (\beta^{(p)})' \ell'(\psi_i^{(p)}) x_i x_i' \Big] \beta
\]
which is
\[
\frac{1}{2} \beta' \Big[  \sum_{i=1}^N \ell''(\psi_i^{(p)}) x_i x_i'
\Big] \beta 
+ \Big[ \sum_{i=1}^N \Big(\ell'(\psi_i^{(p)}) - \psi_i^{(p)} \ell''(\psi_i^{(p)}\Big)
x_i' \Big] \beta.
\]
or
\[
\frac{1}{2} \beta' \Big[  \sum_{i=1}^N \ell''(\psi_i^{(p)}) x_i x_i'
\Big] \beta 
+ \Big[ \sum_{i=1}^N \Big(\frac{\ell'(\psi_i^{(p)})}{\ell''(\psi_i^{(p)})} - \psi_i^{(p)} \Big)
\ell''(\psi_i^{(p)}) x_i' \Big] \beta.
\]
Thus the quadratic form for $\beta$ is identical to that produced solving the
least squares problem
\[
y_i^* = x_i \beta + \nu_i, \; \nu_i \sim N(0,1/w_i^{(p)})
\]
where
\[
w^{(p)}_i = -\ell''(\psi_i^{(p)})
\]
and
\[
y_i^* = \psi_i^{(p)} - \frac{\ell'(\psi_i^{(p)})}{\ell''(\psi_i^{(p)})}.
\]
To double check this, consider the case of a normal likelihood.  Then
$\ell'(\psi) = -(y-\psi) / V$ and $\ell''(\psi) = 1/V$, which makes $y_i^* =
y_i$.  So we can iteratively solve
\[
\begin{cases}
y_i^*(\beta^{(p)}) = x_i \beta + \nu_i, \; \nu_i \sim N(0, 1/w_i(\beta^{(p)}))
\\
\beta \sim N(m_0, C_0)
\end{cases}
\]
where $y_i^*$ and $w_i$ are as above.

There is no difference between following Newton's method or following
iteratively weighted least squares.  They are the same.  It is just easy for
statisticians to think about iteratively weighted least squares.  This may come
in handy below.

Now consider the case of a dynamic regression.  Again let $\sum_{i=1}^N
\ell(\psi_i)$ be the log-likelihood.  The prior is absolutely essential, and
takes for the form of the state-space equation.  Thing proceed very similarly.
We have
\[
\frac{\del}{\del \beta_i} \sum_{i=1}^N \ell(\psi_i)
= \ell'(\psi_i) x_i'
\]
and
\[
\frac{\del}{\del \beta_j} \ell'(\psi_i) x_i = 
\begin{cases}
0, & j \neq i , \\
\ell''(\psi_i) x_i x_i' , & j = i.
\end{cases}
\]
Thus we can see that the Hessian with respect to stack $\{\beta_i\}$ will be
block-diagonal, with rank-1 blocks.  This is, essentially, all we need if we
want to do a massive Netwon step.  Also, note that the complete gradient and the
complete Hessian give us the information we need to update a block of $\beta$'s
using the Laplace approximation---we just select the correct blocks.  After
updating a block of $\beta$'s we would just need to update the correct blocks of
the Hessian and gradient to reflect the new $\psi$'s.  But we want to see how
this is converted into a LS's problem.  To that end, suppose we take the
approximation around $\beta^{(p)}$, then we have
\[
\frac{1}{2} \sum_{i=1}^N \beta_i' \ell''(\psi_i^{(p)}) x_i x_i' \beta_i
+ \sum_{i=1}^N \Big[ \Big( \ell'(\psi_i^{(p)}) - (\beta_i^{(p)})' \ell''(\psi_i^{(p)})
x_i \Big) x_i' \beta_i \Big]
\]
which is
\[
\frac{1}{2} \sum_{i=1}^N \beta_i' \ell''(\psi_i^{(p)}) x_i x_i' \beta_i
+ \sum_{i=1}^N \Big[ \Big( \frac{\ell'(\psi_i^{(p)})}{\ell''(\psi_i^{(p)})} -
\psi_i^{(p)}\Big)  \ell(\psi_i^{(p)})  x_i' \beta_i \Big].
\]
As you can see, very little has changed to the approximation of the liklihood.
This is the same approximation we would get by
\[
y_i^* = x_i \beta_i + \nu_i, \; \nu_i \sim N(0, 1/w_i^{(p)})
\]
where
\[
w^{(p)}_i = -\ell''(\psi_i^{(p)})
\]
and
\[
y_i^* = \psi_i^{(p)} - \frac{\ell'(\psi_i^{(p)})}{\ell''(\psi_i^{(p)})},
\]
just like above, with the only difference being that $\psi_i = x_i' \beta_i$.
As mentioned above, the Hessian of the likelihood is block diagonal, so the
likelihood is unaffected when we consider sampling $\beta$ in blocks.  There are
no interacting $\beta$'s there.  However, there are interacting $\beta$'s in the
\emph{prior}.  Since the posterior is always proportional to the joint, which is
the data generating density times the prior, the conditioning in blocks will
just impact the prior in this case.  Thus we can derive the blocking posterior
by blocking the prior.  For an AR(1) process the prior precision is nice: it is
tridiagonal.  This leads to a blocking scheme with tridiagonal precisions!
Which means the prior will still be an AR(1) process!  (However, the parameters
of the AR process will be different.  To work with Cholesky decomps., you want
to order your observations from largest to smallest.  In that case we can write
the likelihood for $\beta$ as $(A \beta)' D (A \beta)$ where $A$ is upper
bidiagonal and hence (assuming $D=I$) $P = A'A$ reveals the Cholesky
decomposition.)

In particular, recall that, given a precision matrix $\Omega$ and $x \sim N(m,
\Omega^{-1})$ that we have
\[
(x_2 | x_1) \sim N(m_{2|1}, V_{2|1})
\]
where
\[
m_{2|1} = -\Omega_{22}^{-1} \Omega_{21} (x_1 - m_1) + m_2.
\]
and
\[
\Omega_{2|1} = \Omega_{22}
\]
and $V_{2|1} = \Omega_{2|1}^{-1}$.  Since $\Omega$ is (block) tridiagonal,
$\Omega_{22}$ is (block) tridiagonal, and corresponds to the precision from an
AR(1) process.  If one wanted to be fast, they could use the tridiagonality to
invert $\Omega_{22}$ quickly.  Further, $\Omega_{21}$ is almost all zeros, so
one could be clever about that product as well---in the case above the only
values of $\beta$ that matter are those ``on the border'' of the values that you
will sample.  While $\Omega_{22}$ will look very similar to $\Omega$, it will
differ in the $(0,0)$th element (or the $(N,N)$th element depending how you
arrange things).

(My aside: the above work shows why the FFBS works.  The hessian of the log
likelihood from the DLM is block diagonal.  The prior is block tridiagonal.
Hence the posterior precistion, or an approximation to the posterior precision
will be block tridiagonal.  I think you can calculate the Cholesky decomposition
of a symmetric, tridiagonal matrix in linear time, and further that
decomposition will be lower bidiagonal.  Solving a bidiagonal linear system is
also linear in time.  Hence the whole thing is linear in time.  So, to do a
Newton step with the same efficiency as the FFBS, we just need to take advantage
of the tridiagonal structure of the posterior precision.)

There are three possibilities: (1) that the block includes $\beta_1$, (2) that
the block does not include $\beta_1$ or $\beta_T$ and (3) that the block
includes $\beta_T$.  (I hadn't contemplated this before, but drawing $\beta_0$
is somewhat superfluous.  We could just as easitly start by setting up the time
$1$ prior and then forward filtering and backwards sampling, stopping at
$\beta_1$.  I suppose, including the prior $\beta_0 \sim N(m_0, C_0)$ in a DLM
let's you adjust things manually.  It may also affect the selction of
parameters?)  In cases (1) and (2) we have to use the modified AR(1) process.
In case (3), we can use the AR(1) process using the known value on the left.

It seems to me that it may just be easier to use some block tridiagonal routine
to draw from the block.  Otherwise we need a block Choleksy routine and a block
forward or backward solve routine for tri/bi diagonal matries.  I suppose it
boils down to essentially the same thing.

If we let $H(\psi^{(p)}$ denote the Hessian and $G(\psi^{(p)})$ denote the
gradient then the log-likelihood and log-prior yield
\[
\frac{1}{2} (\beta - \beta^{(p)})' H (\beta - \beta^{(p)}) + G \beta -
\frac{1}{2} \beta' P \beta,
\]
which is
\[
-\frac{1}{2} \beta' (P - H) \beta + (G - (\beta^{(p)})' H) \beta.
\]
We can use the latter expression to get the conditional draw of $\beta_1$.  We
could also just focus on $\beta_{1}$.  Recall that $H$ is block diagonal and let
$\Omega = P - H$ and $b' = (G - \beta^{(p)} H)$, then we have
\[
-\frac{1}{2} \beta_1' \Omega_{11} \beta_1 - \beta_2' P_{21} \beta_1 + b_1' \beta_1,
\]
that is
\[
-\frac{1}{2} \beta_1' \Omega_{11} \beta_1 + (b_1' - \beta_2' P_{21}) \beta_1.
\]
Thus we have
\[
\begin{cases}
\bbE[\beta_1 | \beta_2] = - \Omega_{11}^{-1} ( P_{12} \beta_2 - b_1) \\
V(\beta_1 | \beta_2) = \Omega_{11}^{-1}.
\end{cases}
\]
Note that $P_{12}$ is sparse.

Shephard and Pitt and Gamerman suggest using a transformed coordinate system.
First, note that if one has a MH sampling in one coordinate system and then
does a change of variable to that routine for both the target and proposal that
the acceptance rate doesn't change:
\[
\frac {\pi(x^*)}{\pi(x)} \frac {q(x|x^*)}{q(x^*|x)}
= \frac {\pi(x^*(w^*))  |\frac{dx}{dw}(w^*)|}{\pi(x(w)) |\frac{dx}{dw}(w)|} 
\frac{q(x(w)|x^*(w^*)) |\frac{dx}{dw}(w)|}{q(x^*(w^*)|x(w))  |\frac{dx}{dw}(w^*)|}.
\]
Of course, it may be easier to transform the space to pick a better proposal.
But that is not what Shepard etc.\ propose.  Instead, I believe this should be
seen as a way to improve Gibbs sampling a Gaussian distribution.  You want to
Gibbs sample along the principle components to get the best mixing.  In fact,
Gibbs sampling along the pricinple components means you are taking independent
samples, so it is the same as taking a joint draw.  We won't be doing that
exactly here; first, because things aren't Gaussian, and second, because the
draws aren't independent.  But the idea, I expect, is the same, in that you want
to try to make ``orthogonal'' moves.

Consider the transformation $\omega = A \beta$ where $\beta$ is ordered in a
decreasing fashion and $\omega$ are the innovation variances, with the convation
that $\omega_1 = \beta_1$.  $A$ is upper diagonal.  Then the log-prior has the
term $(A \theta)' D (A \theta)$ where $D$ is diagonal corresponding to the
variance of the innovations and the variance of $\beta_1$.  The prior presion,
thus, will be diagonal if we work on the $\omega$ scale.  However, I don't see
how one can easily write down a pseudo-likelihood.  On the $\omega$ scale, you
don't have any block diagonal structure.

Earlier, we noted that if you update $\beta_{i:i+k}$ then you only need to
update the block diagonal Hessian $H$ where $H_{ii} = \ell''(\psi_i) x_ix_i'$
and gradient $G$ where $G_i = \ell'(\psi_i) x_i'$ for the corresponding blocks
$H_{jj}$ and $G_j$ where $j \in i:i+k$.  When updating $\omega$, things change.
Since $\omega_i$ affects $\beta_{j}$ for $j \geq i$, when we update the block
$\omega_{i:i+k}$ we must update $H_{jj}$ and $G_j$ for $j \geq i$.  This is
because $\psi_i = \psi(\beta_i)$ but $\psi_i = \psi(\omega_1, \ldots,
\omega_i)$.  That is, of course, if we assume that we always will take the
Taylor expansion about the current value of $\omega$.

Given the above two observations it seems like it might be easiest to just do a
blocked Newton scheme to do blocked MH within Gibbs.  When calculating the
likelihood, I think we need to include all $\psi_i$ values that include the
updated $\theta_i$ values.

So the procedure will be something like: we have $H$ and $G$.  Use
transformation to get $\tilde H = \tilde H(\omega)$ and $\tilde G = \tilde
G(\omega)$.  The log-posterior approximation is something like
\[
\frac{1}{2} (\omega - \omega^{(p)})' \tilde H (\omega - \omega^{(p)}) + \tilde G (\omega -
\omega^{(p)})
+ \omega' D \omega.
\]
We need to rewrite this in terms of $\omega = (\omega_1, \omega_2)$ to get the
quadratic form in terms of $(\omega_1 | \omega_2)$.  Or rather, I could rephrase
this as a Gaussian posterior, and then use the conditional rules.  Let $-\Omega =
H + D$ and $b' = \tilde G - (\omega^{(p)})' \tilde H$.  Then we have
\[
-\frac{1}{2} \omega' \Omega \omega + b' \omega.
\]
The joint is then $\omega \sim N(m, \Omega^{-1})$ where $\Omega m = b$.  We can
then use our rules of conditonal expectation to get
\[
\begin{cases}
\bbE[\omega_1|\omega_2] = -\Omega_{11}^{-1} \Omega_{12} (\omega_2 - m_2) + m_1 \\
V(\omega_1 | \omega_2) = \Omega_{11}^{-1}.
\end{cases}
\]
If we just consider $\omega_1$, then we have
\[
-\frac{1}{2} \omega_{1}' \Omega_{11} \omega_1 +  -\omega_2 \Omega_{21} \omega_1 +
-\frac{1}{2} \omega_2' \Omega_{22} \omega_2 + b_1' \omega_1,
\]
which becomes
\[
-\frac{1}{2} \omega_{1} ' \Omega_{11} \omega_1 + [b_1' - \omega_2 \Omega_{21}] \omega_1.
\]
So we have 
\[
\begin{cases}
\bbE[\omega_1 | \omega_2] = - \Omega_{11}^{-1} [\Omega_{12} \omega_2 - b_1] \\
V(\omega_1 | \omega_2) = \Omega_{11}^{-1}.
\end{cases}
\]
These reconcile because $\Omega_{11}^{-1} \Omega_{12} m_2 + m_1 =
\Omega_{11}^{-1} b_1$ which one can see by multiplying through by $\Omega_{11}$.

\subsection{$\beta$ Algorithm}

Let $\psi_i = x_i' \beta_i$.  Let $H$ be the Hessian of the log-likelihood in
terms of $\beta$, i.e. the block diagonal matrix with diagonal elements
\[
H(\psi)_{ii} = \ell''(\psi_i) x_i x_i'.
\]
Let $G$ be the gradient of the log-likelihood in terms of $\beta$, i.e. the
block row vector with elements
\[
G(\psi)_i = \ell'(\psi_i) x_i'.
\]
Let $F(\psi) = \beta' H(\psi)$, which is a block row vector with
\[
F(\psi)_i = \psi_i \ell''(\psi_i) x_i'.
\]
Note that when we update $\beta_i$ we just need to update the corresponding
portions of $H$, $G$, and $F$.  Also note, that we can think of, for instance,
$G$, as a product of the vectorized version of $\{x_i\}$.  Let $b' = G - F$.
Assume that the prior for $\beta$ is
\[
\begin{cases}
\beta_t = \Phi \beta_{t-1} + \omega_t, \; \omega_t \sim N(0, W) \\
\beta_0 \sim N(0, C_0).
\end{cases}
\]
I have written this in a way that ignores the mean.  We can incorporate a mean
via $\psi_i = x_i' (\mu + \beta_i)$ or $\psi_i = \alpha + x_i' \beta_i$ and then
estimate each accordingly.  Assume we are ordering things in an ascending
manner.  (It doesn't matter, since the prior precision will be tridiagonal
either way, though it may be convenient to think in ascending or descending
order; here I think that ascending will be useful for computational purposes.)
Let $A$ be
\[
A =
\begin{pmatrix}
I & 0 & 0 & 0 & \ldots \\
-\Phi & I & 0 & 0 & \ldots \\
0 & -\Phi & I & 0 & \ldots \\
0 & 0 & -\Phi & I & \ddots \\
\vdots & \vdots & \vdots & \ddots & \ddots
\end{pmatrix}.
\]
Let $D$ be the block diagonal matrix with entries $D_{ii} = W$ for $i=2..T$ and
$D_{11} = \Phi C_0 \Phi' + W$.  Then the prior log-likelihood is
\[
(A \beta)' D (A \beta) = \beta' A'DA \beta,
\]
so the prior precision is the block tridiagonal matrix $P = A'DA$.  Let $\Omega
= H + P$.  From above, we know that to sample in blocks we can use
\[
\begin{cases}
\bbE[\beta_1 | \beta_2] = - \Omega_{11}^{-1} ( P_{12} \beta_2 - b_1) \\
V(\beta_1 | \beta_2) = \Omega_{11}^{-1}.
\end{cases}
\]
where $P_{12}$ is sparse.  If $i_\ell$ and $i_r$ are the right and left end
points of $\beta_1$, then we have that
\[
P_{12} \beta_2 = P_{i_\ell, i_\ell-1} \beta_{i_\ell-1}
+ P_{i_r,i_r+1} \beta_{i_r+1}.
\]
We eliminate a term if there is not a left or right end point.

So the algorithm is:
\begin{itemize}
\item Set aside space for $P$.
\item Set up $P$.
\item Determine partition $\{k_i\}$.
\item For each block:
  \begin{itemize}
  \item Set $\Omega_{11}$ using $P$.
  \item Update $\Omega_{11}$ using $H$, via $\Omega = P-H$.
  \item Calculate $b_1$ using $G$ and $F$.
  \item Calculate $P_{12} \beta_2$ using $P$ and $\beta$.
  \item Sample $\beta_1^*$.  Calc $\psi_1^*$.
  \end{itemize}
\end{itemize}
We need to have $\ell$, $\ell'$, and $\ell''$ as functions available.  For the
MH step, we need repeat the procedure, in the sense that we must do
\begin{itemize}
\item Using $\beta^*$, the proposal from a specific block:
\item Calculate $\Omega_{11}^*$.
\item Calculate $b_1^*$.
\item Use that to get log likelihood $\log p_N(\beta^{(p)} ; m^*, C^*)$.
\end{itemize}
For the target distribution, which we get using the likelihood, we only need to
use those terms that are potentially changing, i.e the $1$ terms.

\subsection{$\omega$ algorithm}

Let $H$, $G$, $A$, $D$, and $P$ be defined as above.  Let $L = A^{-1}$.  We know
the form of $L$:
\[
L = 
\begin{pmatrix}
I & 0 & 0 & 0 & \ldots \\
\Phi & I & 0 & 0 & \ldots \\
\Phi^2 & \Phi & I & 0 & \ldots \\
\Phi^3 & \Phi^2 & \Phi & I & \ddots \\
\vdots & \vdots & \vdots & \ddots & \ddots
\end{pmatrix}.
\]
Do a change of variables to $\omega = A \beta$.  The change of
variables doesn't affect the likelihood.  Since this is a linear transformation
and $A$ doesn't depend on anything we are sampling, the log-prior doesn't need
to consider the Jacobian, that is the log-prior to consider is
\[
\omega' D \omega.  
\]
Let $\tilde H(\psi) = L' H(\psi) L$ and $\tilde G(\psi) = G(\psi) L$.  We can
see that
\[
(LA(\beta-\beta^{(p)}))' H (LA(\beta-\beta^{(p)})) = (\omega - \omega^{(p)})' \tilde H
(\omega - \omega^{(p)})
\]
and
\[
G (LA(\beta-\beta^{(p)}) = \tilde G (\omega - \omega^{(p)}).
\]
Above both $H$ and $G$ are evaluated at $\psi^{(p)}$.  Note that $\tilde H$ no longer
has any special structure!  Let $\tilde \Omega = D - \tilde H$.  Let $\tilde
F(\psi) = \omega' \tilde H = (L \omega)' H L = \beta' H L = F(\psi) L$.  Then
$\tilde b(\psi) = \tilde G(\psi) - \tilde F(\psi)$.  Thus, the Taylor
approximation of the likelihood is
\begin{align*}
\frac{1}{2} (\omega - \omega^{(p)})' \tilde H  (\omega - \omega^{(p)}) + \tilde G (\omega
- \omega^{(p)})' 
& = \frac{1}{2} \omega' \tilde H \omega - 
(\omega^{(p)})' \tilde H \omega + \tilde G \omega \\
& = \frac{1}{2} \omega' \tilde H \omega + (\tilde G - \tilde F) \omega.
\end{align*}

Then we know that
\[
\begin{cases}
  \bbE[\omega_1 | \omega_2] =
  - \tilde \Omega_{11}^{-1} [\tilde \Omega_{12} \omega_2 - \tilde b_1] \\
  V(\omega_1 | \omega_2) = \tilde \Omega_{11}^{-1}.
\end{cases}
\]
Remember that when updating $\omega_1$ we need to update all $\beta_i$ that are
above the first entry in $\omega_1$.  This will be implemented like $\beta_{1+}
= L_{1+,\cdot} \omega$.  Though we know that $\beta_i = \Phi \beta_{i-1} +
\omega_i$.  So if $i_\ell$ is the first entry of $\beta_1$, then we can
recursively calculate $\beta_i = \Phi \beta_{i-1} + \omega_i$ for $i \geq
i_\ell$.

To repeat: suppose $i_\ell$ is the first entry of the block.  Then we need to
use all entries $i \geq i_\ell$ in $H$, $G$, and $F$ to form $\tilde H$, $\tilde
G$, and $\tilde F$.  Thus we need to update $H_{ii}$, $G_i$, and $F_i$ for $i
\geq i_\ell$.  For instance, $\tilde G_1 = G_{i_\ell:T} L_{i_\ell:T,1}$.  Let $U
= L'$.  For $\tilde H$ we have that
\[
\tilde H_{kj} = \sum_{i=1}^T U_{ki} H_{ii} U_{ji}'.
\]
Since $U_{ki} = 0$ for $k > i$ we have
\[
\tilde H_{kj} = \sum_{i=\max\{k,j\}}^T U_{ki} H_{ii} U_{ji}'.
\]
So a compromise will be
\[
\tilde H_{i_\ell:i_r,i_\ell:i_r} = 
\sum_{i=i_\ell}^T U_{i_\ell:i_r,i} H_{ii} U_{i_\ell:i_r,i}'.
\]
Actually, we can also consider $X = \diag(x_t)$ where $x_t$ is a column vector
and then note that $H = X \Lambda X'$ where $\lambda_{i} = \ell''(\psi_i)$ and
$\Lambda_{ii} = \lambda_i$.  Then $\tilde H = (X'L)' \Lambda (X'L)$.  Then $\Xi'
= X'L$ is lower block triangular.  Then we can reprhase the above product as
\[
\tilde H_{kj} = \sum_{i=1}^T \Xi_{ki} \Lambda_{ii} \Xi_{ji}'
\]
where $\Xi_{ki}$ is a column block for the row index.  Since $\Xi$ is upper
triangular, we can apply the same reasoning as above to get
\[
\tilde H_{i_\ell:i_r,i_\ell:i_r} = \sum_{i=i_\ell}^T \Xi_{i_\ell:i_r,i} \Lambda_{ii} \Xi_{i_\ell:i_r,i}'.
\]
Note, $\Lambda_{ii}$ is not a block so we can adapt this to the usual routines
for matrix multiplication.

Let $\eta = \ell'(\psi)$.  Similarly, $G_i = \eta_i x_i'$.  So 
\[
\tilde G_j = \sum_j \eta_i x_i' L_{ij}.
\]
If we let $\vec{x}$ be the vectorized version of $x$, then we have that
\[
\tilde G_j = \sum_i \eta_i (\Xi')_{ij}.
\]
Similarly,
\[
\tilde F_j = \sum_{i} (\psi_i \lambda_i) (\Xi')_{ij}.
\]
Again, we know that $(\Xi')_{ij}$ is lower triangular, so this sum can be taken
over $i \geq j$, where $j$ indicates the row-block.

It will be convenient to have two functions, one that takes the block of
$\beta$'s and maps to either the starting block-index of that block or mapping
to the ending block-index of that block and another that maps the block-index to
the starting and ending indices.  For instance, let $\sigma$ map the block of
beta's to the start of the block-index.  And let $\tau$ map the block of beta's
to the end of the block-index.  Let $s$ map the block index to the start of the
matrix index.  Let $t$ map the block index to the end of the matrix index.  Then
if we have something like
\[
G_{j:j+k} = \sum_{i=j}^T \eta_i (\Xi')_{i,j:j+k}
\]
we can write
\[
\tilde G_{s(j):t(j+k)} = \sum_{i=j}^T \eta_i (\Xi')_{i,s(j):t(j+k)}
\]
where we are implicitly understanding that we are swithcing from block-index
notation to matrix-index notation.  Similarly,
\[
\tilde H_{s(j_1):t(k_1),s(j_2):t(k_2)} 
= \sum_{i=\max\{j_1, j_2\}}^T \Xi_{s(j_1):t(k_1),i} \lambda_{ii} (\Xi_{s(j_2):t(k_2),i})'.
\]
or
\[
\tilde H_{s(j_1):t(k_1),s(j_2):t(k_2)} 
= \sum_{i=\max\{j_1, j_2\}}^T \Xi_{s(j_1):t(k_1),i} \lambda_{ii} (\Xi')_{i,s(j_2):t(k_2)}.
\]
It seems to me that it will be easiest to just update the vectors $\lambda_i$,
$\eta_i$, and $\psi_i \lambda_i$ and then the appropriate $\tilde H$, $\tilde
G$, etc.

Let $\lambda(\psi) = \ell''(\psi)$, $\eta(\psi) = \ell'(\psi)$, and $\phi(\psi)
= \psi \lambda(\psi)$.  Let $\Xi^L = X'L$.  Let $\Xi^U = L'X$.  Let $j_i$ be the
start of block $i$ and let $k_i$ be the end of block $i$.  Then we can do the
following:
\begin{itemize}
\item You have $\lambda$, $\eta$, and $\phi$.
\item Let $I_i = j_i:k_i$.  Let $I_{-i} = -(j_i:k_i)$.
\item Let $\tilde G_{I_i} = \eta' \Xi^L_{\cdot, I_i}$.
\item Let $\tilde H_{I_i,\cdot} = \Xi^U_{I_i,\cdot} \Lambda \Xi^L_{\cdot,
    \cdot}$.
\item Let $\tilde F_{I_i} = \phi' \Xi^L_{\cdot, I_i}$.
\item Set $\tilde H_{I_i, I_i}$ by above.
\item Set $\tilde H_{I_i, I_{-i}}$ by above.
\item Use that to get $\tilde \Omega_{I_i, I_i}$, $\tilde \Omega_{I_i, I_{-i}}$,
  and $\tilde b_{I_i}$.
\item From this we can sample $\omega_{I_i}^*$.
\item Us $\omega_{I_i}^*$ to calculate a new $\psi^*$ vector.
\item We use this new $\psi^*$ vector to generate $m^*, C^*$ to calculate the
  likelihood of $p_N(\beta^{(p)}; m^*, C^*)$.
\item In the likelihood of the MH check, we need to use $\psi_{j_i:T}$.
\end{itemize}

\subsection{LLH}

\[
\ell(\psi) = \psi y - n \log(1 + e^{\psi}).
\]
So
\[
\ell'(\psi) = y - n \frac{e^{\psi}}{1+e^{\psi}}
\]
and
\[
\ell''(\psi) = - n \frac{e^{\psi}}{1+e^{\psi}} + n \Big(\frac{e^{\psi}}{1+e^\psi}\Big)^2.
\]
If we let $p = e^{\psi} / (1 + e^{\psi})$ then
\[
\ell'(\psi) = y - n p
\]
and
\[
\ell''(\psi) = - n p + n p^2.
\]

\section{Question}

In normal mixture case, we can look at things marginally, i.e. we can
marginalize out the states of $\beta$ and then get an estimate of $\phi$ and
$W$.  We can then use those to draw $\beta$, yielding a joint draw.  Further, by
marginalizing $\beta$, it would see that we might have a nicer looking
likelihood, though maybe that is fantasy?  Actually, that must be fantasy.  We
have
\[
p(y, \beta | \Theta) = p(y | \beta) p(\beta | \Theta)
\]
and
\[
p(y, \beta | \Theta) = p(\beta | y, \Theta) p(y | \Theta).
\]
The latter is like generating $y$ from, for instance an ARMA(1,1) in the DLM
case, and then using the DLM to sample $\beta$.  In either case, we have the
same likelihood.  However, that doesn't change the fact that it may be better to
do sampling in the latter case.

\section{Data Generation}

I am worried about values of $\psi$ that are too large, inducing probabilities
that are 0 or 1 on the computer.

% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliography{bayeslogit}{}
\bibliographystyle{abbrvnat}

\end{document}

% Inefficient derivation.
%   so we have
%   \[
%   \beta(r+t, s) = \frac{\Gamma(r + t) \Gamma(s)}{\Gamma(r+t+s)},
%   \]
%   in which case
%   \[
%   \del_t \beta(r+t, s) = \Gamma'(r+t) \frac{\Gamma(s)}{\Gamma(r+t+s)}
%   + (-1) \frac{\Gamma(r+t) \Gamma(s)}{\Gamma(r+t+s)^2} \Gamma'(r+t+s).
%   \]
%   Further
%   \[
%   \del_t^2 \beta(r+t,s) = \Gamma''(r+t) \frac{\Gamma(s)}{\Gamma(r+t+s)}
%   + (-2) \frac{\Gamma'(r+t) \Gamma(s)}{\Gamma(r+t+s)^2} \Gamma'(r+t+s)
%   + 2 \frac{\Gamma(r+t) \Gamma(s)}{\Gamma(r+t+s)^3} \Gamma'(r+t+s)^2.
%   \]
%   Evaluating these quantites at zero we have
%   \[
%   \Gamma'(r) \frac{\Gamma(s)}{\Gamma(r+s)}
%   - \frac{\Gamma(r) \Gamma(s)}{\Gamma(r+s)^2} \Gamma'(r+s)
%   \]
%   and
%   \[
%   \Gamma''(r) \frac{\Gamma(s)}{\Gamma(r+s)}
%   - 2 \frac{\Gamma'(r) \Gamma(s)}{\Gamma(r+s)^2} \Gamma'(r+s)
%   + 2 \frac{\Gamma(r) \Gamma(s)}{\Gamma(r+s)^3} \Gamma'(r+s)^2.
%   \]
%   These become
%   \[
%   \frac{\Gamma'(r)}{\Gamma(r)} \frac{\Gamma(r) \Gamma(s)}{\Gamma(r+s)}
%   - \frac{\Gamma(r) \Gamma(s)}{\Gamma(r+s)} \frac{\Gamma'(r+s)}{\Gamma(r+s)}
%   \]
%   and
%   \[
%   \frac{\Gamma''(r)}{\Gamma(r)} \frac{\Gamma(r) \Gamma(s)}{\Gamma(r+s)}
%   - 2 \frac{\Gamma(r) \Gamma(s)}{\Gamma(r+s)} \frac{\Gamma'(r)}{\Gamma(r)} \frac{\Gamma'(r+s)}{\Gamma(r+s)}
%   + 2 \frac{\Gamma(r) \Gamma(s)}{\Gamma(r+s)} \frac{\Gamma'(r+s)^2}{\Gamma(r+s)^2}.
%   \]