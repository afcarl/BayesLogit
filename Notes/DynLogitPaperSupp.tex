\documentclass[12pt]{article}
%\documentclass[bib]{ba}

\input{DynLogitPaperCommands}

% Page Layout
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}

\begin{document}

\title{\Polya-Gamma Data Augmentation for Dynamic Models: Supplementary Material }

\begin{center}
\Large \Polya-Gamma Data Augmentation for Dynamic Models: Supplementary Material
\end{center}

% \title[Supplementary Material]{Supplementary Material for \Polya-Gamma Data
%   Augmentation for Dynamic Models}
%\inserttype[ba0001]{article}

%\maketitle

% \appendix
\section{Benchmark Data Sets}
\label{sec:benchmark-details}

For both binomial responses and negative binomial responses, we create four
synthetic data sets whose log-odds and log-mean, respectively, take the form
\begin{equation}
  \label{eqn:synth-psi}
  \psi_t = \alpha + \vx_t' \bbeta_t, \; \; t = 1, \ldots, T,
\end{equation}
with
\[
\bbeta_t = \bPhi \bbeta_{t-1} + \vep_t, \; \; \vep_t \sim N(0, \bW)
\]
where $\bPhi = 0.95 \bI_2$ and $\bW = 0.172 \bI_2$.  The covariates $\vx_t \in
\bbR^2$ are
\[
x_{tj} = \cos(\pi \vf_j s_t) / \sqrt{8}
\]
where $\{s_t\}_{t=1}^T$ is an equally spaced partition of $[-1,1]$ and $\vf$ is
either $\vf^{(low)} = (1,2)$ or $\vf^{(high)} = (1,1.1)$, corresponding to
covariates with a lower or higher degree of correlation.  The binomial response
is
\[
y_t \sim \text{Binom}(n, q_t)
\]
where $q_t = \exp({\psi_t}) / (1+\exp({\psi_t}))$, $n$ is either $1$ or $20$,
and $\alpha$ from (\ref{eqn:synth-psi}) set to 1.4.  The negative binomial
response is
\[
y_t \sim \text{NB}(d, \mu_t)
\]
where $d=4$, $\mu_t = \exp({\psi_t})$, and $\alpha$ from (\ref{eqn:synth-psi})
is either $\log(10)$ or $\log(100)$.  Thus, for dynamic binomial logistic
regression, we create four data sets by varying $n$ and $f$, and for dynamic
negative binomial regression, we create four data sets by varying $f$ and
$\alpha$.

For each data set and each method, we run the MCMC simulations twice, once
assuming that the parameters of the AR model are known and again assuming that
they are unknown.  When the parameters are unknown, both $\bPhi$ and $\bW$ are
assumed to be diagonal; the prior for $\bPhi$ is $\bPhi_{ii} \sim N(0.95, 0.1)
\bbI_{(0,1)}$ and the prior for $\bW$ is $\bW_{ii} \sim \Ga(300/2, 30/2)$.  For
all simulations, the prior for $\alpha$ is $N(0, 100)$ and the prior for
$\bbeta_0$ is $N(0, 100 \bI_2)$.  For the negative binomial benchmarks, $d$ is
taken to be known.
%% My note: $d$ unknown will ``hurt'' results in that it will induce extra
%% correlation, which presumably will hurt PG,mu=10 results.

Four data sets for each type of response, along with two prior structures yields
eight MCMC simulations for each method of posterior inference.  Tables
\ref{tab:dynlogit-detail} and \ref{tab:dynnb-detail} provide a complete summary
of the results.

\section{Simulation Results}

Tables \ref{tab:dynlogit-detail} and \ref{tab:dynnb-detail} provide detailed
benchmark results for all the synthetic data sets described above.  One may
rerun these tests on their own machine by downloading the supplementary file
\texttt{dynsup.tgz}, which contains a collection of R files along with a
customized version of the R package \texttt{BayesLogit}.  The README contained
therein provides detailed instructions for replicating the benchmarks.

% TABLES
% -------------------------------------------------------------------------------
% dyn logit

% More detailed tables
% -------------------------------------------------------------------------------
\begin{table}

  % \begin{center}
  %   \caption{\label{tab:dynlogit-detail} Dynamic binomial logistic benchmarks.}
  %   \hrule
  %   \vspace{2pt}
  %   \hrule
  % \end{center}

  \caption{\label{tab:dynlogit-detail} Dynamic binomial logistic benchmarks.}

  \small

  \begin{center}
    % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-wout.ar
    \begin{tabular}{l r r r r r r r r }
      Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &
      ESR.min  &  ESR.med  &  ESR.max  \\
      \hline

      \multicolumn{9}{c}{$n=1$, $f = (1,2)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      %PG     &    28.66 &     1.00 &   8333.94 &   9388.18 &   9904.42 &    290.79 &    327.55 &    345.57 \\
      PG     &    28.89 &     1.00 &   8318.79 &   9410.34 &   9900.54 &    287.97 &    325.75 &    342.72 \\ 
      dRUM   &    29.13 &     1.00 &   5344.83 &   7550.10 &   9757.91 &    183.48 &    259.19 &    334.98 \\
      CUBS   &   612.14 &     0.71 &   2799.89 &   4219.25 &   5845.51 &      4.57 &      6.89 &      9.55
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-1-with.ar
      % \begin{tabular}{l r r r r r r r r }

      \multicolumn{9}{c}{$n=1$, $f = (1,2)$, unknown AR parameters} \\

      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      %PG     &    31.33 &     1.00 &   1855.82 &   7109.57 &   9739.31 &     59.24 &    226.96 &    310.89 \\
      PG     &    31.74 &     1.00 &   1767.54 &   7260.85 &   9638.08 &     55.70 &    228.80 &    303.71 \\ 
      dRUM   &    31.95 &     1.00 &   1535.90 &   5632.82 &   8921.27 &     48.07 &    176.30 &    279.22 \\
      CUBS   &   612.37 &     0.56 &    242.71 &    487.21 &    775.24 &      0.39 &      0.79 &      1.26
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-wout.ar
      % \begin{tabular}{l r r r r r r r r }

      \multicolumn{9}{c}{$n=1$, $f = (1,1.1)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      %PG     &    28.60 &     1.00 &   8596.25 &   9402.62 &   9933.72 &  300.60 &    328.79 &    347.36 \\
      PG     &    28.92 &     1.00 &   8549.34 &   9398.49 &   9920.66 &    295.63 &    324.99 &    343.05 \\
      dRUM   &    29.13 &     1.00 &   6095.80 &   7675.76 &   9634.57 &    209.27 &    263.51 &    330.77 \\
      CUBS   &   620.95 &     0.72 &   4120.25 &   5531.41 &   6817.09 &      6.64 &      8.91 &     10.98
      \\ %\end{tabular}


      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-1-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=1$, $f = (1,1.1)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      %PG     &    31.37 &     1.00 &    765.97 &   4636.70 &   9415.57 &   24.42 &    147.81 &    300.15 \\
      PG     &    31.75 &     1.00 &    747.98 &   4541.20 &   9543.19 &     23.56 &    143.03 &    300.56 \\
      dRUM   &    31.96 &     1.00 &    656.58 &   3734.29 &   8079.58 &     20.55 &    116.85 &    252.82 \\
      CUBS   &   615.39 &     0.56 &    211.12 &    516.69 &    998.70 & 0.34 &      0.84 &      1.61
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=20$, $f = (1,2)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      %PG     &    73.54 &     1.00 &   7161.18 &   9238.58 &   9923.40 &     97.38 &    125.63 &    134.95 \\
      PG     &    42.09 &     1.00 &   7232.56 &   9231.66 &   9968.61 &    171.83 &    219.33 &    236.83 \\ 
      dRUM   &    28.87 &     1.00 &   3528.34 &   6959.36 &   9893.99 &    122.21 &    241.04 &    342.69 \\
      CUBS   &   650.67 &     0.51 &    727.34 &   2770.01 &   3699.75 &      1.12 &      4.26 &      5.69
      \\ %\end{tabular}


      % Bench-Dyn-06/Tables/table.bench-dynlogit-low-2-n-20-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=20$, $f = (1,2)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      %PG     &    76.26 &     1.00 &   1897.09 &   5482.92 &   9758.10 &     24.88 &     71.90 &    127.95 \\
      PG     &    44.97 &     1.00 &   1912.71 &   5555.84 &   9668.94 &     42.54 &    123.56 &    215.03 \\ 
      dRUM   &    31.69 &     1.00 &   1754.39 &   4003.81 &   9536.59 &     55.36 &    126.33 &    300.91 \\
      CUBS   &   662.42 &     0.47 &    498.89 &    953.03 &   1980.98 &      0.75 &      1.44 &      2.99
      \\ %\end{tabular}


      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=20$, $f = (1,1.1)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      %PG     &    73.16 &     1.00 &   7918.31 &   9416.01 &   9810.23 &    108.23 &    128.70 &    134.09 \\
      PG     &    42.01 &     1.00 &   7874.38 &   9424.07 &   9873.00 &    187.46 &    224.35 &    235.04 \\ 
      dRUM   &    28.88 &     1.00 &   4654.77 &   7771.95 &   9041.13 &    161.17 &    269.10 &    313.05 \\
      CUBS   &   659.75 &     0.63 &   3813.36 &   6375.18 &   6741.68 &      5.78 &      9.66 &     10.22
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynlogit-high-2-n-20-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$n=20$, $f = (1,1.1)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      %PG     &    75.98 &     1.00 &    408.79 &   5735.76 &   9484.30 &      5.38 &     75.49 &    124.83 \\
      PG     &    44.87 &     1.00 &    414.63 &   5713.07 &   9511.09 &      9.24 &    127.33 &    211.99 \\
      dRUM   &    31.71 &     1.00 &    418.48 &   4106.12 &   8269.33 &     13.20 &    129.48 &    260.76 \\
      CUBS   &   665.71 &     0.47 &    273.76 &   1290.78 &   1893.79 &      0.41 &      1.94 &      2.84
    \end{tabular}
  \end{center}

  Here $n$ corresponds to the number of trials for each response; $f$
  corresponds to the frequency used when constructing the covariates.  When $f =
  (1,2)$ the covariates are less correlated when $f=(1,1.1)$ the covariates are
  more correlated.  The PG method does well when $n=1$.  The FS method does well
  when $n=20$.  Time refers to the time taken to produce 10,000 post burn-in
  samples.  ARate refers to the Metroplis-Hastings acceptance rate.
\end{table}

% -------------------------------------------------------------------------------
% dyn-nb

\begin{table}

  %\begin{center}
    \caption{\label{tab:dynnb-detail} dynamic negative binomial benchmarks.}
    %\hrule
    %\vspace{2pt}
    %\hrule
  %\end{center}

  % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-wout.ar
  \begin{center}
    \small
    \begin{tabular}{l r r r r r r r r }
      Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  & ESR.med  &  ESR.max \\
      \hline

      \multicolumn{9}{c}{$\mu=10$, $f = (1,2)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    42.50 &     1.00 &   7972.97 &   9649.48 &   9947.01 &    187.60 &    227.04 &    234.04 \\
      FS   &    29.54 &     1.00 &   1861.10 &   6530.79 &   9831.67 &     62.99 &    221.05 &    332.77 \\
      CUBS   &   655.71 &     0.61 &    800.95 &   3517.15 &   5603.81 &      1.22 &      5.36 &      8.55
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-10-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=10$, $f = (1,2)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    45.29 &     1.00 &    156.63 &   4852.66 &   9812.26 &      3.46 &    107.14 &    216.67 \\
      FS   &    32.31 &     1.00 &    135.06 &   1856.03 &   7837.19 &      4.18 &     57.45 &    242.58 \\
      CUBS   &   674.23 &     0.55 &    116.36 &   1223.44 &   2700.52 &      0.17 &      1.81 &      4.01
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=10$, $f = (1,1.1)$, known AR parameters} \\
      % Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
      PG  &    42.39 &     1.00 &   9142.96 &   9692.30 &   9943.59 &    215.67 &    228.62 &    234.55 \\
      FS  &    29.55 &     1.00 &   2820.14 &   5898.99 &   9765.03 &     95.43 &    199.61 &    330.43 \\
      CUBS  &   656.58 &     0.73 &   4509.25 &   7276.52 &   7891.49 &      6.87 &     11.08 &     12.02

      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-10-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=10$, $f = (1,1.1)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    45.16 &     1.00 &    276.72 &   1725.30 &   9555.14 &      6.13 &     38.20 &    211.58 \\
      FS   &    32.28 &     1.00 &    263.56 &   1164.83 &   5458.53 &      8.16 &     36.08 &    169.10 \\
      CUBS   &   677.20 &     0.62 &    405.83 &   1438.07 &   3812.09 &      0.60 &      2.12 &      5.63
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=100$, $f = (1,2)$, known AR parameters} \\
      % Method  &   time &  ARate & ESS.min & ESS.med & ESS.max & ESR.min & ESR.med & ESR.max \\
      PG  &    38.56 &     1.00 &   1860.03 &   7287.72 &   9927.30 &     48.23 &    188.99 &    257.43 \\
      FS  &    29.17 &     1.00 &   2352.61 &   7301.91 &   9827.50 &     80.65 &    250.31 &    336.89 \\
      CUBS  &   669.34 &     0.38 &    228.97 &   1226.28 &   2833.25 &      0.34 &      1.83 &      4.23

      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-low-2-mu-100-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=100$, $f = (1,2)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    41.33 &     1.00 &   1798.58 &   4986.44 &   9178.54 &     43.51 &    120.63 &    222.06 \\
      FS   &    32.28 &     1.00 &   2098.04 &   4727.49 &   9132.24 &     65.00 &    146.45 &    282.90 \\
      CUBS   &   677.94 &     0.32 &    459.22 &    799.84 &   1194.99 &      0.68 &      1.18 &      1.76
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-wout.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=100$, $f = (1,1.1)$, known AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    40.73 &     1.00 &   4061.25 &   7265.75 &   9856.66 &     99.72 &    178.41 &    242.03 \\
      FS   &    29.44 &     1.00 &   3396.35 &   6938.80 &   9867.25 &    115.36 &    235.66 &    335.13 \\
      CUBS   &   668.03 &     0.42 &   1676.35 &   3429.40 &   4146.91 &      2.51 &      5.13 &      6.21
      \\ %\end{tabular}

      % Bench-Dyn-06/Tables/table.bench-dynnb-high-2-mu-100-with.ar
      % \begin{tabular}{l r r r r r r r r }
      \multicolumn{9}{c}{$\mu=100$, $f = (1,1.1)$, unknown AR parameters} \\
      % Method  &    time  &   ARate  &  ESS.min  &  ESS.med  &  ESS.max  &  ESR.min  &  ESR.med  &  ESR.max  \\
      PG   &    43.57 &     1.00 &   1186.07 &   3214.67 &   6880.24 &     27.22 &     73.79 &    157.93 \\
      FS   &    32.25 &     1.00 &   1115.12 &   3071.79 &   6879.66 &     34.58 &     95.25 &    213.31 \\
      CUBS   &   682.59 &     0.30 &    412.62 &    723.17 &   1028.33 &      0.60 &      1.06 &      1.51
    \end{tabular}
  \end{center}

  Here $\mu$ corresponds to the mean of the response; $f$ corresponds to the
  frequency used when constructing the covariates.  When $f = (1,2)$ the
  covariates are less correlated when $f=(1,1.1)$ the covariates are more
  correlated.  The PG method does well when $\mu=10$.  The FS method does well
  when $\mu=100$. Time refers to the time taken to produce 10,000 post burn-in
  samples.  ARate refers to the Metroplis-Hastings acceptance rate.

\end{table}

% -------------------------------------------------------------------------------
% \section{Sampling $d$ in dynamic negative binomial regression}
% \label{sec:sample-d}

% We may sample $d$ by an independent Metropolis-Hastings or a random-walk
% Metropolis-Hastings.  In either case, we need the log-likelihood of $d$.

% The conditional density for a single term $y_t \sim N(d, q_t)$ is
% \[
% \frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t} q_t^{y_t} (1-q_t)^{d} ,
% \]
% or
% \[
% \frac{\Gamma(y_t + d)}{\Gamma(d) \Gamma(y_t) y_t}
% \Big( \frac{\mu_t}{\mu_t + d} \Big)^{y_t} \Big( \frac{d}{\mu_t + d} \Big)^d
% \]
% in terms of the mean $\mu_t$.  Since we chose to model $\log \mu_t = \vx_t
% \beta_t$ it will be easier to work with the latter version.  In that case, the
% log-likelihood is
% \[
% \ell(d|\mu, y) = \sum_{t=1}^T [ \log \Gamma(y_t + d)  - \log \Gamma(d) ]  +
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]

% If we assume that $d$ is a natural number then we can rid ourselves of $T$
% evaluations of the gamma function.  We may expand $\Gamma(y_t + d)$ and
% $\Gamma(d)$ as products, in which case the log-likelihood is
% \[
% \ell(d|\mu, y) = \sum_{t=1}^T \sum_{j=0}^{y_t-1} \log(d + j) +
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]
% We can rewrite the first term of the sum as
% \[
% \sum_{t=1}^T \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{k-1} \log(d+j) \one \{y_t = k\}
% \]
% which becomes
% \[
% \sum_{k=1}^{\max(y_t)} n_k \sum_{j=0}^{k-1} \log(d+j)
% \]
% where $n_k = \{ \# y_t = k \}$.  We have thus
% \begin{align*}
%   \sum_{k=1}^{\max(y_t)} \sum_{j=0}^{\max(y_t)-1} n_k \log(d+j) \one\{j < k\}
%   & =
%   \sum_{j=0}^{\max(y_t)-1} \log(d+j) \sum_{k=j+1}^{\max(y_t)} n_k \\
%   & = \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j
% \end{align*}
% where $G_j = \{ \# y_t > j \}$.  Notice that $1-G_j = F(j) = \{\# y_t \leq j\}$
% and that $G$ may be pre-processed.  Hence the log-likelihood is
% \[
% \sum_{j=0}^{\max(y_t)-1} \log(d+j) G_j +
% \sum_{t=1}^T y_t \log \big( \frac{\mu_t}{\mu_t + d} \Big) +
% d \sum_{t=1}^T \log \Big( \frac{d}{\mu_t + d} \Big).
% \]
% Preprocessing $G$ saves us from having to compute a doubly indexed summation.

\end{document}