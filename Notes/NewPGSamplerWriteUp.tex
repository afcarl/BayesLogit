\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
% \usepackage{url}
\usepackage{graphicx}
\usepackage{natbib}
% \usepackage{outlines}
% \usepackage{parskip}
\input{commands2}

\newcommand{\Polya}{P\'{o}lya}
\newcommand{\JJ}{\text{J}^*}
\newcommand{\PG}{\mbox{PG}}
\newcommand{\Ga}{\mbox{Ga}}
\newcommand{\IGa}{\mbox{IGa}}
\newcommand{\simiid}{\stackrel{iid}{\sim}}
\newcommand{\bx}{{\bar x}}
\newcommand{\utanh}[1]{\frac{\tanh{\sqrt{#1}}}{\sqrt{#1}}}
\newcommand{\utan}[1]{\frac{\tan{\sqrt{#1}}}{\sqrt{#1}}}
\newcommand{\la}{\leftarrow}
\newcommand{\dd}[2]{\frac{d #1}{d #2}}

% Page Layout
\setlength{\evensidemargin}{0.0in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8in}

% Set Fancy Style
%\pagestyle{fancy}
%\lhead{Left Header}
%\rhead{Right Header}

\title{Improved \Polya-Gamma Sampling}

\author{
  Nicholas G. Polson \\ \\
  \textit{The University of Chicago} \\ \\
  James G. Scott \\ 
  Jesse Windle \\ \\
  \textit{The University of Texas at Austin}
}

\date{
  First Draft: March 10, 2013 \\
  This Draft: March 10, 2013
}

\begin{document}

\maketitle

\abstract{\citet{polson-etal-2013} devise a \Polya-Gamma sampling scheme that
  works well for a subset of the \Polya-Gamma family of distributions.  Herein,
  we improve sampling for members outside of that subset.}

\tableofcontents

\newpage

\section{Introduction}

The \Polya-Gamma family of distributions arises from a data augmentation scheme
introduced in \cite{polson-etal-2013} that is useful when working with logistic
likelihoods, which are likelihoods of the form
\[
\ell(\beta|y_i) = \frac{(e^{\psi_i})^{a_i}}{(1+e^{\psi_i})^{b_i}} \bbI \{\psi_i =
  x_i' \beta \},
\]
where $a_i$ and $b_i$ are functions of $y_i$ and the other parameters present in
the model.  Such likelihood arises when modeling proportions as the log-odds, as
can be done in binomial logistic regression and negative binomial regression for
count data.

The data augmentation scheme relies on generating \Polya-Gamma random variates
$\omega_i \sim \PG(n_i, \psi_i)$ for $i=1, \ldots, N$ where $n_i$ is on the
order of the response $y_i$ and $N$ is the number of observations.
\cite{polson-etal-2013} show how to efficiently generate $\PG(1, \psi_i)$ random
variates.  One may use this sampler to generate $\PG(n_i, \psi_i)$ random
variates for $n_i \in \bbN$ since the sum of $n_i$ independent $PG(1,\psi_i)$
random variates is distributed as $\PG(n_i,\psi_i)$. This is slow when $n_i$ is
large, as may be the case for count data.  Consequently, in terms of effective
sampling rate, \cite{polson-etal-2013} find that their method works well for
binomial logistic regression when the number of trials at each observation is
not too great, but that it does not work as well as the discrete mixture of
normals approach advocated by \cite{fruhwirth-schnatter-etal-2009} for negative
binomial regression.  However, \cite{polson-etal-2013} find that, in terms of
effective sample size, the \Polya-Gamma approach is superior to all methods for
both binomial logistic regression and negative binomial regression.  Thus, if
one can devise sufficiently efficient \Polya-Gamma sampler, then the
\Polya-Gamma data augmentation scheme may be become a superior approach for
generating posterior distributions within entire classes of models: those with
logistic likelihoods.

Herein, we attempt to make incremental improvements upon the \Polya-Gamma
sampler presented in \cite{polson-etal-2013}.  Our goal is two fold: first,
improve \Polya-Gamma sampling for large shape parameters; second, find a way to
sample \Polya-Gamma random variates for non-integral shape parameters.

\section{Preliminaries}

The \Polya-Gamma family of distributions $\{PG(b,z)\}_{b>0, z\geq 0}$ is closely
related to a subset of laws surveyed by \cite{biane-etal-2001} whose Laplace
transforms are 
\[
\bbE[ e^{-t \JJ(b)} ] = \cosh^{-b} (\sqrt{2 t}).
\]
\cite{devroye-2009} showed how to efficiently sample the $\JJ(1)$ distribution.
The \Polya-Gamma family is, in essence, identical to an exponential tilted
version of the $\JJ(b)$ density.  To see this, define the $\JJ(b,z)$
distribution as the random variable with density
\[
\cosh^b(z) e^{- x z^2 / 2} f(x)
\]
where $f(x)$ is the density of a $\JJ(b)$ random variable.  Then $X \sim
\PG(b,z) \iff 4 X \sim \JJ(b,z/2)$.  As shown by \cite{biane-etal-2001}, the
density of the $\JJ(b)$ distribution is an infinite, alternating sum
\begin{equation}
\label{eqn:igamma-rep}
\frac{2^b}{\Gamma(b)} \sum_{n=0}^{\infty} (-1)^n \frac{\Gamma(n+b)}{\Gamma(n+1)} 
\frac{(2n+b)}{\sqrt{2 \pi x^3}} \exp \Big( - \frac{(2n+b)^2}{2 x} \Big).
\end{equation}
It will be convient to work directly with the $\JJ(b,z)$ distribution and hence
we focus on this distribution throughout, though as noted above, once can always
return to the \PG family by a simple transformation.

One may appeal to the properties of Laplace transforms to show that
\[
\bbE[ e^{-t \JJ(b,z)} ] = \frac{\cosh^b(z)}{\cosh^b(\sqrt{2t + z^2})}.
\]
Thus, the moment generating function of $\JJ(b,z)$ is
\[
M(t; b,z) = \frac{\cosh^b(z)}{\cos^{b}(\sqrt{2t - z^2})}
\]
as $\cosh(\sqrt{-u}) = \cos(\sqrt{u})$.  We denote the cumulant generating
function as $K(t; b,z) = \log M(t; b,z)$.  A key step in what follows relies on
the fact that one can represent the cosine function as an infinite product of
monomials.  In particular, the Weierstrass factorization theorem says that
\[
\cos(\sqrt{2t}) = \prod_{n=0}^\infty \Big( 1 - \frac{t}{c_n} \Big), \; 
  c_n = \frac{\pi^2}{2} \Big( n + \frac{1}{2} \Big)^2.
\]
Employing this for both the numerator and denominator of the moment generating
function shows that
\[
M(t; b, z) = \prod_{n=0}^\infty \Big( 1 - \frac{t}{d_n} \Big)^{-b}, \; 
  d_n = \frac{\pi^2}{2} \Big( n + \frac{1}{2} \Big)^2 + \frac{z^2}{2}.
\]
Thus, each member of $\JJ(b,z)$ is an infinite convolution of gamma
distributions, and can be expressed as
\[
\JJ(b,z) \sim \sum_{n=0}^\infty \frac{g_n}{d_n} \, , \; g_n \simiid Ga(b,1).
\]
One may truncate this sum at, for instance, $n=200$, to generate approximate
draws from $\JJ(b,z)$ when $b$ is not an integer; however, this is extremely
slow, and, as shown below, one can devise much better approximations.  The
moment generating function also shows that if $X_1 \sim \JJ(b_1,z)$ and $X_2
\sim \JJ(b_2, z)$ that $X_1 + X_2 \sim \JJ(b_1 + b_2, z)$.

\section{An Alternate Density}
\label{sec:alternate-density}

The $\JJ(1,z)$ sampler in \cite{polson-etal-2013}, which is motivated by
\cite{devroye-2009}, relies on a reciprocal relationship noticed by
\cite{ciesielski-taylor-1962}, which shows that in addition to
(\ref{eqn:igamma-rep}) one may represent the density of a $\JJ(1)$ random
variable as
\[
\sum_{n=0}^\infty (-1)^n \pi \Big(n+\frac{1}{2}\Big) e^{(n+1/2)^2 \pi^2 x / 2}.
\]
By pasting these two densities together, one arrives at an extremely efficient
sampler.  Unfortunately, there is no known general reciprocal relationship;
however, \cite{biane-etal-2001} provide an alternate density for the $\JJ(2)$
distribution based upon a reciprocal relationship with another random variable.

While there may not be an obvious reciprocal relationship to use, one may find
other alternate representations for the density of $\JJ(h)$ random variables
when $h$ is a positive integer.  Exploiting an idea from \cite{kent-1980} for
infinite convolutions of exponential random variables, one may invert the moment
generating function using partial fractions.  Assume $z=0$, so that we are
working with an untitled $\JJ(h)$ distribution.  Consider its moment generating
function,
\begin{equation}
\label{eqn:mgf-product}
M(t) = \prod_{n=0}^\infty \Big(1 - \frac{t}{c_n}\Big)^{-h}.
\end{equation}
This can be expanded by partial fractions so that
\begin{equation}
\label{eqn:mgf-partial-fraction}
M(t) = \sum_{n=0}^\infty \sum_{m=1}^h \frac{A_{nm}}{(t - c_n)^m}.
\end{equation}
Inverting this sum term by term we find that one can represent the density as
\[
f(x|h) = \sum_{n=0}^\infty \sum_{m=1}^h A_{nm} \frac{x^{m-1} e^{-c_i x}}{(m-1)!}.
\]

To find formulas for the $\{A_{nm}\}_{nm}$ coefficients, consider the Laurent
series expansion of $M(t)$ about $c_i$.
\begin{equation}
\label{eqn:mgf-laurent}
M(t) = \sum_{n=0}^\infty a_{n}^{(i)} (t - c_i)^n + \sum_{m=1}^h
\frac{b_m^{(i)}}{(t - c_i)^m}.
\end{equation}
Such an expansions is valid since $c_n$ is an isolated singular point.  Since
the coefficients at the pole are unique, comparing coefficients in
(\ref{eqn:mgf-partial-fraction}) and (\ref{eqn:mgf-laurent}) shows that $A_{im}
= b_m^{(i)}$.  Further, one may calculate $b_m^{(i)}$ by considering the
function
\[
\nu_h(t) = (t - c_i)^h M(t)
\]
and the computing
\[
b_m^{(i)} = \frac{\nu_h^{(h-m)}(c_i)}{(h-m)!}.
%\lim_{t \ra c_i} \frac{\nu^{(h-m)}(t)}{(h-m)!}.
\]
Writing the MGF in product form, as in (\ref{eqn:mgf-product}), we see that
\[
\nu_h(t) = (-c_i)^h \prod_{n \neq i} \Big(1 - \frac{t}{c_n} \Big)^h.
\]
Define
\[
\psi_h(t) = h \log (-c_i) - h \sum_{n \neq i} \Big(1 - \frac{t}{c_n}\Big).
\]
Then $\log \nu_h(t) = \exp \psi_h(t)$.  The
derivatives of $\nu$ can then be expressed as
\begin{align*}
\nu_h' & = e^{\psi_h} \psi_h'; \\
\nu_h'' & = e^{\psi_h} (\psi_h')^2 + e^{\psi_h} \psi_h''; \\
\nu_h''' & = e^{\psi_h} (\psi_h')^3 + 3 e^{\psi_h} \psi_h' \psi_h'' + e^{\psi_h}
\psi_h''' \\
\ldots & = \ldots \; \; 
\end{align*}
where
\[
\psi_1^{(k)}(t) = (k-1)! \sum_{n \neq i} ( c_n - t )^{-k}.
\]
Thus, one may calculate $b_{m}^{(i)}$ numerically using $\psi_h^{(k)}$, though
the convergence may be slow.

However, the most important coefficient, $b_h^{(i)}$, is already known.  Make
the dependence of $b_m^{(i)}$ on $h$ explicit by writing $b_m^{(i)}(h)$.  From
the formulas above we know that \( b_h^{(i)}(h) = \nu_h(c_i) \) and that \(
\nu_h(c_i) = \exp(\psi_1(c_i))^h.  \) But \( \exp(\psi_1(c_i)) = \nu_1(c_i) =
b_1^{(i)}(1).  \) From the reciprocal relationship provided at the start of the
section, we know that \( b_1^{(i)}(1) = (-1)^i \sqrt{2 c_i}.  \) Thus,
\[
A_{ih} = b_h^{(i)}(h) = (-1)^{ih} (2 c_i)^{h/2}.
\]
For integral $h$, the density for $\JJ(h)$ then takes the form
\[
f(x|h) = \sum_{n=0}^\infty
\Big[ \sum_{m=1}^h \frac{A_{nm} (h-1)! }{A_{nh} (m-1)!} \frac{1}{x^{h-m}} \Big]
\frac{A_{nh} x^{h-1} e^{-c_i x}}{(h-1)!}.
\]
Thus, for sufficiently large $x$, the term 
\[
\frac{A_{0h} x^{h-1} e^{-c_0 x}}{(h-1)!} = \frac{(\pi/2)^{h/2} x^{h-1} e^{-c_0 x}}{(h-1)!}
\]
should dominate.  

\begin{remark}
\label{remark:tails}
This provides insight into the tail behavior of the $\JJ(h)$ distribution.  For
the right tail, we expect the density to decay as a $\Ga(h,c_0)$ distribution.
Examining the representation (\ref{eqn:igamma-rep}), we expect the left tail to
decay like $\IGa(1/2, h^2/2)$.  These two observations will prove useful when
finding an approximation of the $\JJ(h)$ density.
\end{remark}

%Note that $\nu$ and $b_m^{(i)}$ all depend upon and $h$, $c_i$.

\section{Saddle-point Approximation}
\label{sec:largeb}

\cite{daniels-1954} provides a framework for constructing approximations to the
density of sums of random variables.  More generally, one can view his work as
creating approximations of the density of $X(n) / n$ where $X(h)$ is an
infinitely divisible family.  The following summarizes his approach.

Let $X(h)$ be an infinitely divisible family.  Let $M(t)$ denote the moment
generating function of $X(1)$, and let $K(t)$ denote its cumulant generating
function.  Then $M(it)$ is the characteristic function.  (ASIDE: I think the
conditions that are put upon $K$ are ultimately related to it being analytic is
a certain domain.)  The MGF is defined by
\[
M(t) = e^{K(t)} = \int_{-\infty}^\infty e^{tx} f(x) dx.
\]
where $f(x)$ is the density of the random variable $X(1)$.  Let $\bar x$ denote
$X(n)/n$, which can be thought of as the sample mean of $n$ independent $X(1)$
random variables when $n$ is an integer.  Let $\tilde x$ denote $X(n)$, which
can be thought of as the sum of $n$ independent $X(1)$ random variables when $n$
is an integer.  Then the MGF of $\tilde x$ is $M^n(t)$ and the $MGF$ of $\bar x$
is $M^n(t/n)$.  Thus the corresponding Fourier inversions are
\[
\tilde f(\tilde x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} M^n(i t) e^{-i t
  \tilde x} dt.
\]
and
\[
f_n(\bar x) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} M^n(i t/n) e^{-i t \bar x}
d t 
=  \frac{n}{2 \pi} \int_{-\infty}^{\infty} M^n(i t) e^{-i n t \bar x}
d t 
\]
where $\tilde f$ is the density of $X(n)$ and $f_n$ is the density of $X(n)/n$.
One can do a change of variables $T = it$ so that
\[
f_n(\bar x) = \frac{n}{2 \pi i} \int_{-\infty i}^{\infty i} M^n(T) e^{-n T \bar x} dT.
\]
Assume that $M(t)$ is analytic on some strip that includes the imaginary axis.
Then this integral is equal to (use the cumulant generating function now)
\[
f_n(\bar x) = \frac{n}{2 \pi i} \int_{\tau -\infty i}^{\tau + \infty i} e^{n (
  K(T) - T \bar x)} dT.
\]
where $\tau$ is a real number within our strip.  
% We could be integrating over
% any path in the strip, but let's just assume it is a straight line. 
The key observation by Daniels is that we want to focus the mass of our
integration on as small a region as possible and that we can do so by picking
the path of the integral to go through a saddle point, in which case the
integrand is small outside the neighborhood of the saddle point.  In particular,
we want to minimize
\[
K(T) - T \bar x
\]
since this will be where the integrand is greatest.  This occurs when
\begin{equation}
\label{eqn:t0}
K'(T) - \bar x = 0.
\end{equation}
Daniels goes on to provide conditions upon the distribution $F_n$ for which this
holds.  Given the root $T_0(x)$ that solves this equation the saddle point
approximation is
\[
g_n(\bar x) = \Big( \frac{n}{2 \pi} \Big)^{1/2} K''(T_0)^{-1/2} e^{n [K(T_0) - T_0
  \bar x]}
\]
where $T_0 = T_0(\bar x)$ is defined by (\ref{eqn:t0}).  We want to sample from
the density proportional to $g_n(\bar x)$.

\section{Sampling the saddle point approximation}
\label{sec:largeb}

Concave functions can be bounded from above by continuous, piecewise linear
functions.  Devroye provides several examples of how this may be used in
practice, even for the case of general log-concave densities
\citep{devroye-1986, devroye-2012}.  The basic idea is that if one can bound the
density by piecewise linear functions, then one can sample a proposal
distribution from a mixture of truncated exponential distributions.  We follow
this approach, though with a few modifications.  In particular, we will bound
the exponent of $g_n(x)$ rather than the kernel itself employing functions more
complex than polynomials.  (We have replaced $\bar x$ with $x$ for notational
ease.)  One may phrase the minimization problem above in terms of convex
duality, in which the exponent of $g_n(x)$ is
\[
\phi(x) = \min_{s \in \bbR} \Big\{ K(s) - s x \Big\}
\]
where $K(t)$ is the cumulant generating function: $K(t)$ is indeed convex for
$\JJ(1,z)$.  When needed, we will write $K_z(t)$ to denote the explicitly
dependence upon $z$.  The connection to duality will help us find a good bound
for $\phi(x)$ and the following facts will be useful.

\begin{fact}
\label{fact:dual}
Let $\phi(x)$ be the concave dual of $K$.  Let
\[
t(x) = \argmin{s \in \bbR} \Big\{ K(s) - s x \Big\}.
\]
Assume that when we write $t$ we are implicitly evaluating it at $x$.  Then
\begin{enumerate}
\item $\displaystyle K'(t) = x$;
\item $\displaystyle \phi(x) = K(t) - t x$;
\item $\displaystyle \phi'(x) = -t$;
% \[
% \phi'(x) = K'(t) \dd{t}{x} - \dd{t}{x} x - t = -t.
% \]
\item $\displaystyle \dd{t}{x}(x) = [K''(t)]^{-1}$;
% Using (1)
% \[
% K''(t) \dd{t}{x}(x) = 1 \implies  \dd{t}{x}(x) = [K''(t)]^{-1}.
% \]
\item $\displaystyle \dd{^2 t}{x^2}(x) = - \frac{K'''(t)}{(K''(t))^3}$.
% Differentiate $\dd{t}{x}$ above and apply (4).
\item As seen by item (3), $\phi'(x)$ is maximized when $t(x)=0$.  Thus, 
\[
m = \argmax{x} \phi(x) \; \text{ is attained when } m = \utan{z}.
\]
\end{enumerate}

\end{fact}

Sometimes it will be helpful to work with a shifted version of $t$: \( u = t -
z^2/2.  \) To reiterate, we will go between three different variables: $x, t, u$
characterized by the transformations
\begin{enumerate}
\item $x = K'(t)$ and
\item $u = t - z^2 / 2$.
\end{enumerate}
It will also be helpful to have the derivatives of $K$ on hand and a few facts
about $x$ and $u$.
\begin{fact}
\label{fact:cgf-derivatives}
Recall that $K(t) = \log \cosh(z) - \log \cos \sqrt{2u}$ is the cumulant
generating function of $\JJ(1,z)$.  Its derivatives, with respect to $t$, are:
\begin{enumerate}
\item $\displaystyle K'(t) = \utan{2u}$;
\item $\displaystyle K''(t) = \frac{\tan^2(\sqrt{2u})}{2u} + \frac{1}{2u} \Big(1
  - \utan{2u} \Big)$.
\end{enumerate}
As shown above, $K'(t) = x$.  When we evaluate $K''$ at $t(x)$ we have
\[
K''(t) = x^2 + \frac{1}{2u} (1 - x).
\]
We may write $\utan{s}$ piecewise as
\[
\utan{s} = 
\begin{cases}
\utan{s}, & s > 0 \\
\utanh{|s|},  & s < 0 \\
1, & s = 0.
\end{cases}
\]
The last component can be seen by taking the Taylor expansion around $s=0$.
Thus, $u < 0 \iff x < 1$, $u> 0 \iff x > 1$, and $u=0 \iff x = 1$.
\end{fact}

This leads to the following two claims, which will help us bound the
saddle-point approximation.  Notice that in each case, we are adjusting the
exponent by the amount needed to match what we expect to be the shape of the
tails found in Remark \ref{remark:tails}.

\begin{lemma}
  The function $\eta_r(x) = \phi(x) - (\log(x) - \log(m))$ is concave
  for all $x > 0$ and strictly concave when $x \neq 1$.
\end{lemma}

\begin{proof}
Taking derivatives:
\[
\eta_r'(x) = \phi'(x) - \frac{1}{x}
\]
and
\[
\eta_r''(x) = - \dd{t}{x}(x) + \frac{1}{x^2}.
\]
Using Fact \ref{fact:dual}, this is negative if and only if
\[
[K''(t)]^{-1} \geq \frac{1}{x^2} \iff x^2 \geq K''(t) \iff 0 \geq \frac{(1-x)}{2u}.
\]
When $x > 1$, $u(x) > 0$, and $\eta_r''(x) < 0$.  When $x < 1$, $u(x) < 0$, and
$\eta_r''(x) < 0$.  Continuity of $K''$ ensures that $\eta_r''(0) \leq 0$.
\end{proof}

\begin{lemma}
  The function $\eta_l(x) = \phi(x) - \Big[\frac{1}{2} \Big(1- \frac{1}{x}\Big)
  - \frac{1}{2} \Big(1- \frac{1}{m}\Big)\Big]$ is concave for all $x > 0$ and
  strictly concave for $x \neq 1$.
\end{lemma}

\begin{proof}
Taking derivatives:
\[
\eta_\ell'(x) = \phi'(x) - \frac{1}{2x^2}
\]
and
\[
\eta_\ell''(x) = - \dd{t}{x}(x) + \frac{1}{x^3}.
\]
Using Fact \ref{fact:dual}, this is negative if and only if
\[
[K''(t)]^{-1} \geq \frac{1}{x^3} \iff x^3 \geq K''(t) \iff (x^2 +
\frac{1}{2u})(x-1) \geq 0.
\]
Again, we know that when $x > 1$, $u > 0$, and hence $\eta_l(x) < 0$.  When $x <
1$ we need to show that $x^2 + 1/(2u) < 0$.  This is equivalent to showing that
\[
x^2 < -\frac{1}{2u} \iff 2u x^2 > -1, \, u < 0.
\]
That is
\[
\tan^2{\sqrt{2u}} > -1 \iff \tanh{\sqrt{|2u|}} > -1, \, \text{ for } u < 0,
\]
which indeed holds.  Thus, when $x < 1$, $\eta_l(x) < 0$.  Again, continuity of
$K$ then ensures that $\eta_l''(0) \leq 0$.
\end{proof}

These two lemmas ensure the following claim.
\begin{claim}
The function
\[
\eta(x) =
\begin{cases}
\eta_l(x), & x \leq 1 \\
\eta_r(x), & x \geq 1
\end{cases}
\]
is concave.  Let $\delta(x) = \phi(x) - \eta(x)$ denote the difference between
$\phi(x)$ and $\eta(x)$.  Since $\phi(x) - \phi(m) \leq 0$, $\eta(x) - \phi(m)
\leq - \delta(x)$.
\end{claim}

We may create an enclosing envelope in the following way.  See figure
\ref{fig:envelope} for a graphical interpretation.

\begin{enumerate}
\item Pick a point $q = (1, \phi(1)+\alpha)$, $\alpha>0$.  (Or pick $x_l$ and
  $x_r$ arbitrarily.)
\item Find the tangent line $L_\ell(x)$ that touches $\eta(x)$ at some $x_l < 1$
  and that passes through $q$.  Use this line as the leftmost envelope of
  $\eta(x)$ on $(0,1]$
\item Find the tangent line $L_r(x)$ that touches $\eta(x)$ at some $x_r > 1$
  and that passes through $q$.  Use this line as the rightmost envelope of
  $\eta(x)$ on $[1,\infty)$.
\item That is, let
\[
e(x) = 
\begin{cases}
L_\ell(x), & x < 1, \\
L_r(x), & x \geq 1.
\end{cases}
\]
\end{enumerate}
Then the envelope for $\phi(x)$ is
\[
\phi(x) \leq e(x) + \delta(x).
\]

\begin{lemma}
  There are constants $1> \alpha_\ell, \alpha_r > 0$ such that the function
  $K''(t)$ satisfies
\[
K''(t) \geq  \alpha_\ell(z) x^3 \; \text{ for } x < 1
\]
and
\[
K''(t) \geq  \alpha_r(z) x^2 \; \text{ for } x > 1.
\]
\end{lemma}

Thus, we may dominate the left most portion of $g_n$ with a inverse gaussian kernel:
\[
x^{-3/2} \exp \Big( - \frac{n}{2x} + n e_\ell(x) \Big);
\]
and we may dominate the right most portion of $g_n$ with a gamma kernel:
\[
x^{n-1} \exp \Big( n e_r(x) \Big).
\]

\begin{figure}
\centering
\includegraphics[scale=0.5]{eta-phi-envelope.png}
\caption{\label{fig:envelope} The saddle point approximation is proportional to
  $[K''(t(x))]^{-0.5} \exp( n \phi(x) )$.  On the left, the function $\eta(x)$
  is a solid black curve, which is bounded from above by the solid green curve
  $e(x)$ (for envelope).  On the right, the function $\phi(x)$ is plotted in red
  while the envelope $e(x) + \delta(x)$ is plotted in green.}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.5]{sp-jj0.png}
\caption{\label{fig:saddlepoint} The black line in $\JJ(1)$.  The blue, dotted
  line is the saddle point approximation.}
\end{figure}

% \section{To do:}

% To do:
% \begin{enumerate}
% \item Need to pick $x_l$, $x_r$ more methodically.
% \item Normalize densities above (already have functions).
% \item Calculate weights.
% \end{enumerate}

% \section{Sampling $\JJ(b,z)$ for small $b$.}
% \label{sec:smallb}

% \subsection{$\JJ(b)$}

% \begin{claim}
% The density is bounded:
% \[
% f(x|b) \leq \frac{(\pi/2)^b x^{b-1}}{\Gamma(b)} e^{-\frac{\pi^2}{8} x}
% \]
% for sufficiently large $x$.
% \end{claim}

% If you have a bibliography.
% The file withe bibliography is name.bib.
\bibliographystyle{abbrvnat}
\bibliography{bayeslogit}{}

\end{document}
